
# Awesome Video Generation [![Awesome](https://awesome.re/badge-flat.svg)](https://awesome.re)

A comprehensive collection of works on video generation/synthesis/prediction.

<br>
<br>

<p align="center">
<img src="https://magvit.cs.cmu.edu/img/fp/00083_gen_sr.gif" width="240px"/>  
<img src="https://magvit.cs.cmu.edu/img/mt_ssv2/full_generation_0143SqueezingSomething_000000782_sr.gif" width="240px"/>
</p>

<p align="center">
<img src="https://modelscope.cn/api/v1/models/damo/text-to-video-synthesis/repo?Revision=master&FilePath=./samples/006_A_cat_eating_food_out_of_a_bowl,_in_style_of_van_Gogh._003.gif&View=true" width="240px"/>  
<img src="https://modelscope.cn/api/v1/models/damo/text-to-video-synthesis/repo?Revision=master&FilePath=./samples/040_Incredibly_detailed_science_fiction_scene_set_on_an_alien_planet,_view_of_a_marketplace._Pixel_art._003.gif&View=true" width="240px"/>
</p>

<p align="center">
<img src="https://kfmei.page/vidm/results/sky/vidm.gif" width="480px"/>  
</p>

<p align="center">
(Source: <a href="https://mask-cond-video-diffusion.github.io">MCVD</a>, <a href="https://modelscope.cn/models/damo/text-to-video-synthesis/summary">VideoFusion</a>, and <a href="https://kfmei.page/vidm/">VIDM</a>)
</p>



## Survey Papers



+ [Video Frame Interpolation: A Comprehensive Survey](https://dl.acm.org/doi/10.1145/3556544)  

+ [Diffusion Models: A Comprehensive Survey of Methods and Applications](https://arxiv.org/abs/2209.00796)  
  [![Star](https://img.shields.io/github/stars/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.svg?style=social&label=Star)](https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2209.00796)

+ [Diffusion Models in Vision: A Survey](https://arxiv.org/pdf/2209.04747) (IEEE TPAMI 2023)  
  [![Star](https://img.shields.io/github/stars/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey.svg?style=social&label=Star)](https://github.com/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2209.04747)

+ [What comprises a good talking-head video generation?: A Survey and Benchmark](https://arxiv.org/pdf/2005.03201)  
  [![Star](https://img.shields.io/github/stars/lelechen63/talking-head-generation-survey.svg?style=social&label=Star)](https://github.com/lelechen63/talking-head-generation-survey)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2005.03201)

+ [A Review on Deep Learning Techniques for Video Prediction](https://arxiv.org/abs/2004.05214) (2020)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2004.05214)



## Datasets



+ [CelebV-Text: A Large-Scale Facial Text-Video Dataset](https://arxiv.org/abs/2303.14717)  
  [![Star](https://img.shields.io/github/stars/CelebV-Text/CelebV-Text.svg?style=social&label=Star)](https://github.com/CelebV-Text/CelebV-Text)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.14717)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://celebv-text.github.io/)

+ [CelebV-HQ: A Large-Scale Video Facial Attributes Dataset](https://arxiv.org/abs/2207.12393)  
  [![Star](https://img.shields.io/github/stars/celebv-hq/celebv-hq.svg?style=social&label=Star)](https://github.com/celebv-hq/celebv-hq)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2207.12393)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://celebv-hq.github.io/)

+ [UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild](https://arxiv.org/pdf/1212.0402)  
  [![Star](https://img.shields.io/github/stars/wushidonguc/two-stream-action-recognition-keras.svg?style=social&label=Star)](https://github.com/wushidonguc/two-stream-action-recognition-keras)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1212.0402)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://www.crcv.ucf.edu/data/UCF101.php)

+ [The Kinetics Human Action Video Dataset](https://arxiv.org/pdf/1705.06950)  
  [![Star](https://img.shields.io/github/stars/deepmind/kinetics-i3d.svg?style=social&label=Star)](https://github.com/deepmind/kinetics-i3d)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1705.06950)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://www.deepmind.com/open-source/kinetics)

## 2023

+ [Probabilistic Adaptation of Text-to-Video Models](https://arxiv.org/abs/2306.01872)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.01872)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://video-adapter.github.io/video-adapter/)

+ [VIDM: Video Implicit Diffusion Models](https://arxiv.org/abs/2212.00235) (AAAI 2023)  
  [![Star](https://img.shields.io/github/stars/MKFMIKU/VIDM.svg?style=social&label=Star)](https://github.com/MKFMIKU/VIDM)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.00235)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://kfmei.page/vidm/)

+ [Mm-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation](https://arxiv.org/abs/2212.09478) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/researchmm/MM-Diffusion.svg?style=social&label=Star)](https://github.com/researchmm/MM-Diffusion)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.09478)

+ [Video Probabilistic Diffusion Models in Projected Latent Space](https://arxiv.org/abs/2302.07685) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/sihyun-yu/PVDM.svg?style=social&label=Star)](https://github.com/sihyun-yu/PVDM)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.07685)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sihyun.me/PVDM/)

+ [VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation](https://arxiv.org/abs/2303.08320) (CVPR 2023)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.08320)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)

+ [Conditional Image-to-Video Generation with Latent Flow Diffusion Models](https://arxiv.org/abs/2303.13744) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/nihaomiao/CVPR23_LFDM.svg?style=social&label=Star)](https://github.com/nihaomiao/CVPR23_LFDM)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.13744)

+ [Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos](https://arxiv.org/abs/2303.16897) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/sukun1045/video-physics-sound-diffusion.svg?style=social&label=Star)](https://github.com/sukun1045/video-physics-sound-diffusion)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.16897)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sukun1045.github.io/video-physics-sound-diffusion/)

+ [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2304.08818) (CVPR 2023)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.08818)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)

+ [3D Cinemagraphy from a Single Image](https://arxiv.org/abs/2303.05724) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/xingyi-li/3d-cinemagraphy.svg?style=social&label=Star)](https://github.com/xingyi-li/3d-cinemagraphy)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.05724)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://xingyi-li.github.io/3d-cinemagraphy/)

+ [MOSO: Decomposing MOtion, Scene and Object for Video Prediction](https://arxiv.org/abs/2303.03684) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/iva-mzsun/MOSO.svg?style=social&label=Star)](https://github.com/iva-mzsun/MOSO)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.03684)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://iva-mzsun.github.io/MOSO)

+ [SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation](https://arxiv.org/abs/2211.12194) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/OpenTalker/SadTalker.svg?style=social&label=Star)](https://github.com/OpenTalker/SadTalker)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2211.12194)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sadtalker.github.io/)

+ [Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers](https://arxiv.org/abs/2303.11251) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/Ugness/MeBT.svg?style=social&label=Star)](https://github.com/Ugness/MeBT)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.11251)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sites.google.com/view/mebt-cvpr2023)

+ [Learning Universal Policies via Text-Guided Video Generation](https://arxiv.org/abs/2302.00111)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.00111)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://universal-policy.github.io/unipi/)

+ [Tell me what happened: Unifying text-guided video completion via multimodal masked video generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Tell_Me_What_Happened_Unifying_Text-Guided_Video_Completion_via_Multimodal_CVPR_2023_paper.pdf) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/tsujuifu/pytorch_tvc.svg?style=social&label=Star)](https://github.com/tsujuifu/pytorch_tvc)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2211.12824)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://tvc-mmvg.github.io/)

+ [Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation](https://arxiv.org/pdf/2304.08477.pdf)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.08477)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://latent-shift.github.io/)

+ [Video-p2p: Video editing with cross-attention control](https://arxiv.org/abs/2303.04761)  
  [![Star](https://img.shields.io/github/stars/ShaoTengLiu/Video-P2P.svg?style=social&label=Star)](https://github.com/ShaoTengLiu/Video-P2P)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.04761)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://video-p2p.github.io/)

+ [MAGVIT: Masked Generative Video Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.pdf) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/google-research/magvit.svg?style=social&label=Star)](https://github.com/google-research/magvit)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.05199)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://magvit.cs.cmu.edu/)

+ [Text2video-zero: Text-to-image diffusion models are zero-shot video generators](https://arxiv.org/abs/2303.13439)  
  [![Star](https://img.shields.io/github/stars/Picsart-AI-Research/Text2Video-Zero.svg?style=social&label=Star)](https://github.com/Picsart-AI-Research/Text2Video-Zero)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.13439)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://text2video-zero.github.io/)

+ [Pix2video: Video editing using image diffusion](https://arxiv.org/pdf/2303.12688.pdf)  
  [![Star](https://img.shields.io/github/stars/G-U-N/Pix2Video.pytorch.svg?style=social&label=Star)](https://github.com/G-U-N/Pix2Video.pytorch)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](G-U-N/Pix2Video.pytorch)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://duyguceylan.github.io/pix2video.github.io/)

+ [Dreamix: Video diffusion models are general video editors](https://arxiv.org/abs/2302.01329)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.01329)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://dreamix-video-editing.github.io/)

+ [VarietySound: Timbre-controllable video to sound generation via unsupervised information disentanglement](https://arxiv.org/abs/2211.10666)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2211.10666)

+ [Text-to-4d dynamic scene generation](https://arxiv.org/abs/2301.11280)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2301.11280)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://make-a-video3d.github.io/)

+ [Fatezero: Fusing attentions for zero-shot text-based video editing](https://arxiv.org/abs/2303.09535)  
  [![Star](https://img.shields.io/github/stars/ChenyangQiQi/FateZero.svg?style=social&label=Star)](https://github.com/ChenyangQiQi/FateZero)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.09535)

+ [Structure and content-guided video synthesis with diffusion models](https://arxiv.org/abs/2302.03011)  
  [![Star](https://img.shields.io/github/stars/kyegomez/StarlightVision.svg?style=social&label=Star)](https://github.com/kyegomez/StarlightVision)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.03011)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://taldatech.github.io/ddlp-web)

+ [GD-VDM: Generated Depth for better Diffusion-based Video Generation](https://arxiv.org/abs/2306.11173)  
  [![Star](https://img.shields.io/github/stars/lapid92/gd-vdm.svg?style=social&label=Star)](https://github.com/lapid92/gd-vdm)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.11173)

+ [DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles](https://arxiv.org/abs/2306.05957)  
  [![Star](https://img.shields.io/github/stars/taldatech/ddlp.svg?style=social&label=Star)](https://github.com/taldatech/ddlp)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.05957)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://taldatech.github.io/ddlp-web)

+ [Video Diffusion Models with Local-Global Context Guidance](https://arxiv.org/abs/2306.02562)  
  [![Star](https://img.shields.io/github/stars/exisas/lgc-vd.svg?style=social&label=Star)](https://github.com/exisas/lgc-vd)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.02562)

+ [Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising](https://arxiv.org/abs/2305.18264)  
  [![Star](https://img.shields.io/github/stars/G-U-N/Gen-L-Video.svg?style=social&label=Star)](https://github.com/G-U-N/Gen-L-Video)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.18264)

+ [Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation](https://arxiv.org/abs/2305.14330)  
  [![Star](https://img.shields.io/github/stars/ku-cvlab/direct2v.svg?style=social&label=Star)](https://github.com/ku-cvlab/direct2v)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.14330)

+ [ControlVideo: Training-free Controllable Text-to-Video Generation](https://arxiv.org/abs/2305.13077)  
  [![Star](https://img.shields.io/github/stars/ybybzhang/controlvideo.svg?style=social&label=Star)](https://github.com/ybybzhang/controlvideo)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.13077)

+ [Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models](https://arxiv.org/pdf/2305.13840)  
  [![Star](https://img.shields.io/github/stars/Weifeng-Chen/control-a-video.svg?style=social&label=Star)](https://github.com/Weifeng-Chen/control-a-video)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.13840)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://controlavideo.github.io/)

+ [VDT: An Empirical Study on Video Diffusion with Transformers](https://arxiv.org/abs/2305.13311)  
  [![Star](https://img.shields.io/github/stars/rerv/vdt.svg?style=social&label=Star)](https://github.com/rerv/vdt)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.13311)

+ [Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models](https://arxiv.org/abs/2305.05845)  
  [![Star](https://img.shields.io/github/stars/rohandkn/skribble2vid.svg?style=social&label=Star)](https://github.com/rohandkn/skribble2vid)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.05845)

+ [Text2Performer: Text-Driven Human Video Generation](https://arxiv.org/abs/2304.08483)  
  [![Star](https://img.shields.io/github/stars/yumingj/text2performer.svg?style=social&label=Star)](https://github.com/yumingj/text2performer)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.08483)

+ [Generative Disco: Text-to-Video Generation for Music Visualization](https://arxiv.org/abs/2304.08551)  
  [![Star](https://img.shields.io/github/stars/hellovivian/generative-disco.svg?style=social&label=Star)](https://github.com/hellovivian/generative-disco)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.08551)

+ [MoStGAN-V: Video Generation with Temporal Motion Styles](https://arxiv.org/abs/2304.02777)  
  [![Star](https://img.shields.io/github/stars/xiaoqian-shen/mostgan-v.svg?style=social&label=Star)](https://github.com/xiaoqian-shen/mostgan-v)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.02777)

+ [Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos](https://arxiv.org/abs/2304.01186)  
  [![Star](https://img.shields.io/github/stars/mayuelala/followyourpose.svg?style=social&label=Star)](https://github.com/mayuelala/followyourpose)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.01186)

+ [Sounding Video Generator: A Unified Framework for Text-guided Sounding Video Generation](https://arxiv.org/abs/2303.16541)  
  [![Star](https://img.shields.io/github/stars/jwliu-cc/svg.svg?style=social&label=Star)](https://github.com/jwliu-cc/svg)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.16541)

+ [MotionVideoGAN: A Novel Video Generator Based on the Motion Space Learned from Image Pairs](https://arxiv.org/abs/2303.02906)  
  [![Star](https://img.shields.io/github/stars/bbzhu-jy16/motionvideogan.svg?style=social&label=Star)](https://github.com/bbzhu-jy16/motionvideogan)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.02906)

+ [Consistency Models](https://arxiv.org/abs/2303.01469)  
  [![Star](https://img.shields.io/github/stars/openai/consistency_models.svg?style=social&label=Star)](https://github.com/openai/consistency_models)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.01469)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://jepsen.io/consistency)

+ [MotionVideoGAN: A Novel Video Generator Based on the Motion Space Learned from Image Pairs](https://arxiv.org/abs/2303.02906)  
  [![Star](https://img.shields.io/github/stars/bbzhu-jy16/motionvideogan.svg?style=social&label=Star)](https://github.com/bbzhu-jy16/motionvideogan)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.02906)

+ [Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction](https://arxiv.org/abs/2301.08951)  
  [![Star](https://img.shields.io/github/stars/FudanVI/compositional-scene-representation-toolbox.svg?style=social&label=Star)](https://github.com/FudanVI/compositional-scene-representation-toolbox)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2301.08951)

+ [VideoComposer: Compositional Video Synthesis with Motion Controllability](https://arxiv.org/pdf/2306.02018)  
  [![Star](https://img.shields.io/github/stars/damo-vilab/videocomposer.svg?style=social&label=Star)](https://github.com/damo-vilab/videocomposer)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.02018)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://videocomposer.github.io/)

+ [Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity](https://arxiv.org/abs/2305.11675) (May, 2023)  
  [![Star](https://img.shields.io/github/stars/jqin4749/MindVideo.svg?style=social&label=Star)](https://github.com/jqin4749/MindVideo)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.11675)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://mind-video.com/)

+ [Any-to-Any Generation via Composable Diffusion](https://arxiv.org/abs/2305.11846)  
  [![Star](https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&label=Star)](https://github.com/microsoft/i-Code/tree/main/i-Code-V3)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.11846)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://codi-gen.github.io/)

+ [VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation](https://arxiv.org/abs/2305.10874)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.10874)

+ [Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models](https://arxiv.org/abs/2305.10474)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.10474)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://research.nvidia.com/labs/dir/pyoco/)

+ [Motion-Conditioned Diffusion Model for Controllable Video Synthesis](https://arxiv.org/abs/2304.14404)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.14404)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://tsaishien-chen.github.io/MCDiff/)

+ [LaMD: Latent Motion Diffusion for Video Generation](https://arxiv.org/abs/2304.11603)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.11603)  

+ [DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion](https://arxiv.org/abs/2304.06025)  
  [![Star](https://img.shields.io/github/stars/johannakarras/DreamPose.svg?style=social&label=Star)](https://github.com/johannakarras/DreamPose)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.06025)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://grail.cs.washington.edu/projects/dreampose/)

+ [Seer: Language Instructed Video Prediction with Latent Diffusion Models](https://arxiv.org/abs/2303.14897)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.14897)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://seervideodiffusion.github.io/)

+ [Learning 3D Photography Videos via Self-supervised Diffusion on Single Images](https://arxiv.org/abs/2302.10781)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.10781)

+ [InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions](https://arxiv.org/abs/2304.05684)  
  [![Star](https://img.shields.io/github/stars/tr3e/InterGen.svg?style=social&label=Star)](https://github.com/tr3e/InterGen)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.05684)

+ [ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model](https://arxiv.org/abs/2304.01116)  
  [![Star](https://img.shields.io/github/stars/mingyuan-zhang/ReMoDiffuse.svg?style=social&label=Star)](https://github.com/mingyuan-zhang/ReMoDiffuse)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.01116)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html)

+ [Human Motion Diffusion as a Generative Prior](https://arxiv.org/abs/2303.01418)  
  [![Star](https://img.shields.io/github/stars/priorMDM/priorMDM.svg?style=social&label=Star)](https://github.com/priorMDM/priorMDM)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.01418)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://priormdm.github.io/priorMDM-page/)

+ [Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions](https://arxiv.org/abs/2306.03988)  
  [![Star](https://img.shields.io/github/stars/araachie/yoda.svg?style=social&label=Star)](https://github.com/araachie/yoda)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.03988)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://araachie.github.io/yoda/)

+ [Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance](https://arxiv.org/abs/2306.00943)  
  [![Star](https://img.shields.io/github/stars/VideoCrafter/Make-Your-Video.svg?style=social&label=Star)](https://github.com/VideoCrafter/Make-Your-Video)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.00943)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://doubiiu.github.io/projects/Make-Your-Video/)

+ [Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts](https://arxiv.org/abs/2305.08850)  
  [![Star](https://img.shields.io/github/stars/HeliosZhao/Make-A-Protagonist.svg?style=social&label=Star)](https://github.com/HeliosZhao/Make-A-Protagonist)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.08850)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://make-a-protagonist.github.io/)

+ [DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation](https://arxiv.org/abs/2305.06225)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.06225)

+ [LEO: Generative Latent Image Animator for Human Video Synthesis](https://arxiv.org/abs/2305.03989)  
  [![Star](https://img.shields.io/github/stars/wyhsirius/LEO.svg?style=social&label=Star)](https://github.com/wyhsirius/LEO)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.03989)

+ [Multi-object Video Generation from Single Frame Layouts](https://arxiv.org/abs/2305.03983)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.03983)

+ [StyleLipSync: Style-based Personalized Lip-sync Video Generation](https://arxiv.org/abs/2305.00521)  
  [![Star](https://img.shields.io/github/stars/TaekyungKi/StyleLipSync.svg?style=social&label=Star)](https://github.com/TaekyungKi/StyleLipSync)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.00521)

+ [High-Fidelity and Freely Controllable Talking Head Video Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_High-Fidelity_and_Freely_Controllable_Talking_Head_Video_Generation_CVPR_2023_paper.pdf) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/hologerry/PECHead.svg?style=social&label=Star)](https://github.com/hologerry/PECHead)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.10168)

+ [Video Generation Beyond a Single Clip](https://arxiv.org/abs/2304.07483)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.07483)

+ [DisCoHead: Audio-and-Video-Driven Talking Head Generation by Disentangled Control of Head Pose and Facial Expressions](https://arxiv.org/abs/2303.07697) (ICASSP 2023)  
  [![Star](https://img.shields.io/github/stars/deepbrainai-research/discohead.svg?style=social&label=Star)](https://github.com/deepbrainai-research/discohead)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.07697)

+ [Controllable Video Generation by Learning the Underlying Dynamical System with Neural ODE](https://arxiv.org/abs/2303.05323)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.05323)

+ [DPE: Disentanglement of Pose and Expression for General Video Portrait Editing](https://openaccess.thecvf.com/content/CVPR2023/papers/Pang_DPE_Disentanglement_of_Pose_and_Expression_for_General_Video_Portrait_CVPR_2023_paper.pdf) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/OpenTalker/DPE.svg?style=social&label=Star)](https://github.com/OpenTalker/DPE)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2301.06281)

+ [PV3D: A 3D Generative Model for Portrait Video Generation](https://arxiv.org/abs/2212.06384) (ICLR 2023)  
  [![Star](https://img.shields.io/github/stars/bytedance/pv3d.svg?style=social&label=Star)](https://github.com/bytedance/pv3d)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.06384)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://showlab.github.io/pv3d/)

+ [AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion](https://arxiv.org/abs/2305.04001) (CVPR 2023 Workshop)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.04001)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://lifrary.github.io/AADiff/)

+ [Controllable One-Shot Face Video Synthesis With Semantic Aware Prior](https://arxiv.org/abs/2304.14471)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.14471)

+ [Decoupling Dynamic Monocular Videos for Dynamic View Synthesis](https://arxiv.org/abs/2304.01716)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.01716)

+ [Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis](https://arxiv.org/abs/2303.12644) (MICCAI 2023)  
  [![Star](https://img.shields.io/github/stars/HReynaud/EchoDiffusion.svg?style=social&label=Star)](https://github.com/HReynaud/EchoDiffusion)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.12644)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://hreynaud.github.io/EchoDiffusion/)

+ [WALDO: Future Video Synthesis using Object Layer Decomposition and Parametric Flow Prediction](https://arxiv.org/abs/2211.14308)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2211.14308)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://16lemoing.github.io/waldo/)

+ [Fast Fourier Inception Networks for Occluded Video Prediction](https://arxiv.org/abs/2306.10346)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2306.10346)

+ [Let's Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction](https://arxiv.org/abs/2305.13903)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.13903)

+ [PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video Prediction](https://arxiv.org/abs/2305.11421)  
  [![Star](https://img.shields.io/github/stars/easylearningscores/PastNet.svg?style=social&label=Star)](https://github.com/easylearningscores/PastNet)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2305.11421)

+ [A Control-Centric Benchmark for Video Prediction](https://arxiv.org/abs/2304.13723) (ICLR 2023)  
  [![Star](https://img.shields.io/github/stars/s-tian/vp2.svg?style=social&label=Star)](https://github.com/s-tian/vp2)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.13723)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://s-tian.github.io/projects/vp2/)

+ [Combining Vision and Tactile Sensation for Video Prediction](https://arxiv.org/abs/2304.11193)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.11193)

+ [MS-LSTM: Exploring Spatiotemporal Multiscale Representations in Video Prediction Domain](https://arxiv.org/abs/2304.07724)  
  [![Star](https://img.shields.io/github/stars/mazhf/MS-RNN.svg?style=social&label=Star)](https://github.com/mazhf/MS-RNN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.07724)

+ [Forecasting localized weather impacts on vegetation as seen from space with meteo-guided video prediction](https://arxiv.org/abs/2303.16198)  
  [![Star](https://img.shields.io/github/stars/earthnet2021/earthnet-models-pytorch.svg?style=social&label=Star)](https://github.com/XX/YY)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.16198)

+ [A Dynamic Multi-Scale Voxel Flow Network for Video Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_A_Dynamic_Multi-Scale_Voxel_Flow_Network_for_Video_Prediction_CVPR_2023_paper.pdf) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/megvii-research/CVPR2023-DMVFN.svg?style=social&label=Star)](https://github.com/megvii-research/CVPR2023-DMVFN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.09875)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://huxiaotaostasy.github.io/DMVFN/)

+ [TKN: Transformer-based Keypoint Prediction Network For Real-time Video Prediction](https://arxiv.org/abs/2303.09807)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.09807)

+ [Implicit Stacked Autoregressive Model for Video Prediction](https://arxiv.org/abs/2303.07849)  
  [![Star](https://img.shields.io/github/stars/seominseok0429/Implicit-Stacked-Autoregressive-Model-for-Video-Prediction.svg?style=social&label=Star)](https://github.com/seominseok0429/Implicit-Stacked-Autoregressive-Model-for-Video-Prediction)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.07849)

+ [MOSO: Decomposing MOtion, Scene and Object for Video Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_MOSO_Decomposing_MOtion_Scene_and_Object_for_Video_Prediction_CVPR_2023_paper.pdf) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/iva-mzsun/MOSO.svg?style=social&label=Star)](https://github.com/iva-mzsun/MOSO)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.03684)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://iva-mzsun.github.io/MOSO)

+ [Polar Prediction of Natural Videos](https://arxiv.org/abs/2303.03432)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.03432)

+ [STDepthFormer: Predicting Spatio-temporal Depth from Video with a Self-supervised Transformer Model](https://arxiv.org/abs/2303.01196) (IROS 2023)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.01196)

+ [Object-Centric Video Prediction via Decoupling of Object Dynamics and Interactions](https://arxiv.org/abs/2302.11850)  
  [![Star](https://img.shields.io/github/stars/AIS-Bonn/OCVP-object-centric-video-prediction.svg?style=social&label=Star)](https://github.com/AIS-Bonn/OCVP-object-centric-video-prediction)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.11850)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sites.google.com/view/ocvp-vp)

+ [Anti-aliasing Predictive Coding Network for Future Video Frame Prediction](https://arxiv.org/abs/2301.05421)  
  [![Star](https://img.shields.io/github/stars/ling-cf/ppnv2.svg?style=social&label=Star)](https://github.com/ling-cf/ppnv2)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2301.05421)

+ [Long-horizon video prediction using a dynamic latent hierarchy](https://arxiv.org/abs/2212.14376)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.14376)

+ [Motion and Context-Aware Audio-Visual Conditioned Video Prediction](https://arxiv.org/abs/2212.04679)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.04679)

+ [MIMO Is All You Need : A Strong Multi-In-Multi-Out Baseline for Video Prediction](https://ojs.aaai.org/index.php/AAAI/article/view/25289) (AAAI 2023)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.04655)

+ [A unified model for continuous conditional video prediction](https://arxiv.org/abs/2210.05810) (CVPR 2023 Workshop)  
  [![Star](https://img.shields.io/github/stars/XiYe20/NPVP.svg?style=social&label=Star)](https://github.com/XiYe20/NPVP)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2210.05810)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://npvp.github.io/)

+ [PreCNet: Next-Frame Video Prediction Based on Predictive Coding](https://arxiv.org/abs/2004.14878) (IEEE TNNLS 2023)  
  [![Star](https://img.shields.io/github/stars/ctu-vras/precnet.svg?style=social&label=Star)](https://github.com/ctu-vras/precnet)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2004.14878)

+ [NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation](https://arxiv.org/abs/2303.12346)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.12346)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://msra-nuwa.azurewebsites.net/#/)

<br>

__Needs verification & further processing:__

+ Synthesizing Artistic Cinemagraphs from Text
+ VINECS: Video-based Neural Character Skinning
+ Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation
+ DisCo: Disentangled Control for Referring Human Dance Generation in Real World
+ PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN
+ BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion
+ Envisioning a Next Generation Extended Reality Conferencing System with Efficient Photorealistic Human Rendering
+ Reprogramming Audio-driven Talking Face Synthesis into Text-driven
+ Action-conditioned Deep Visual Prediction with RoAM, a new Indoor Human Motion Dataset for Autonomous Robots
+ Self-supervised Learning of Event-guided Video Frame Interpolation for Rolling Shutter Frames
+ Boost Video Frame Interpolation via Motion Adaptation
+ Facial Expression Re-targeting from a Single Character
+ VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing
+ DORSal: Diffusion for Object-centric Representations of Scenes
+ Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation
+ A Weakly Supervised Approach to Emotion-change Prediction and Improved Mood Inference
+ Unmasking Deepfakes: Masked Autoencoding Spatiotemporal Transformers for Enhanced Video Forgery Detection
+ Generative Semantic Communication: Diffusion Models Beyond Bit Recovery
+ Color-aware Deep Temporal Backdrop Duplex Matting System
+ Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions
+ MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning
+ Video Colorization with Pre-trained Text-to-Image Diffusion Models
+ Temporal-controlled Frame Swap for Generating High-Fidelity Stereo Driving Data for Autonomy Analysis
+ Quantifying Sample Anonymity in Score-Based Generative Models with Adversarial Fingerprinting
+ Adjustable Visual Appearance for Generalizable Novel View Synthesis
+ 4DSR-GCN: 4D Video Point Cloud Upsampling using Graph Convolutional Networks
+ Intelligent Grimm -- Open-ended Visual Storytelling via Latent Diffusion Models
+ MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding
+ Exploring Phonetic Context in Lip Movement for Authentic Talking Face Generation
+ Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models
+ Context-Preserving Two-Stage Video Domain Translation for Portrait Stylization
+ OD-NeRF: Efficient Training of On-the-Fly Dynamic Neural Radiance Fields
+ EgoVSR: Towards High-Quality Egocentric Video Super-Resolution
+ NegVSR: Augmenting Negatives for Generalized Noise Modeling in Real-World Video Super-Resolution
+ Video Prediction Models as Rewards for Reinforcement Learning
+ Reparo: Loss-Resilient Generative Codec for Video Conferencing
+ CPNet: Exploiting CLIP-based Attention Condenser and Probability Map Guidance for High-fidelity Talking Face Generation
+ Synthesizing Diverse Human Motions in 3D Indoor Scenes
+ InstructVid2Vid: Controllable Video Editing with Natural Language Instructions
+ SurgMAE: Masked Autoencoders for Long Surgical Video Analysis
+ SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models
+ Learning Pose Image Manifolds Using Geometry-Preserving GANs and Elasticae
+ IDO-VFI: Identifying Dynamics via Optical Flow Guidance for Video Frame Interpolation with Events
+ Light-VQA: A Multi-Dimensional Quality Assessment Model for Low-Light Video Enhancement
+ Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models
+ Identity-Preserving Talking Face Generation with Landmark and Appearance Priors
+ HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion
+ Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer
+ NeuralEditor: Editing Neural Radiance Fields via Manipulating Point Clouds
+ DSEC-MOS: Segment Any Moving Object with Moving Ego Vehicle
+ Can deepfakes be created by novice users?
+ Video Frame Interpolation with Densely Queried Bilateral Correlation
+ Dynamic Video Frame Interpolation with integrated Difficulty Pre-Assessment
+ Recurrent Transformer Encoders for Vision-based Estimation of Fatigue and Engagement in Cognitive Training Sessions
+ AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation
+ Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation
+ CAT-NeRF: Constancy-Aware Tx$^2$Former for Dynamic Body Modeling
+ Soundini: Sound-Guided Diffusion for Natural Video Editing
+ Boosting Video Object Segmentation via Space-time Correspondence Learning
+ VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs
+ MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation
+ Neural Image-based Avatars: Generalizable Radiance Fields for Human Avatar Modeling
+ Generative AI for learning: Investigating the potential of synthetic learning videos
+ That's What I Said: Fully-Controllable Talking Face Generation
+ Boundary-Denoising for Video Activity Localization
+ HNeRV: A Hybrid Neural Representation for Videos
+ BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation
+ NPC: Neural Point Characters from Video
+ TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles
+ Long-Short Temporal Co-Teaching for Weakly Supervised Video Anomaly Detection
+ FONT: Flow-guided One-shot Talking Head Generation with Natural Head Motions
+ Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models
+ Consistent View Synthesis with Pose-Guided Diffusion Models
+ DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder
+ The secret of immersion: actor driven camera movement generation for auto-cinematography
+ Novel View Synthesis of Humans using Differentiable Rendering
+ CelebV-Text: A Large-Scale Facial Text-Video Dataset
+ GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents
+ SUDS: Scalable Urban Dynamic Scenes
+ NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects
+ Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods for Front-Facing Views
+ HandNeRF: Neural Radiance Fields for Animatable Interacting Hands
+ Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators
+ SHERF: Generalizable Human NeRF from a Single Image
+ Pix2Video: Video Editing using Image Diffusion
+ Music-Driven Group Choreography
+ Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields
+ Efficient Decision-based Black-box Patch Attacks on Video Recognition
+ Emotionally Enhanced Talking Face Generation
+ Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization
+ Confidence Attention and Generalization Enhanced Distillation for Continuous Video Domain Adaptation
+ MoRF: Mobile Realistic Fullbody Avatars from a Monocular Video
+ Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation
+ Leaping Into Memories: Space-Time Deep Feature Synthesis
+ Learning Data-Driven Vector-Quantized Degradation Model for Animation Video Super-Resolution
+ FateZero: Fusing Attentions for Zero-shot Text-based Video Editing
+ LDMVFI: Video Frame Interpolation with Latent Diffusion Models
+ Learning Physical-Spatio-Temporal Features for Video Shadow Removal
+ NLUT: Neural-based 3D Lookup Tables for Video Photorealistic Style Transfer
+ Automatic Geo-alignment of Artwork in Children's Story Books
+ Blowing in the Wind: CycleNet for Human Cinemagraphs from Still Images
+ Blind Video Deflickering by Neural Filtering with a Flawed Atlas
+ GRADE: Generating Realistic Animated Dynamic Environments for Robotics Research
+ Butterfly: Multiple Reference Frames Feature Propagation Mechanism for Neural Video Compression
+ Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training
+ One-Shot Video Inpainting
+ Continuous Space-Time Video Super-Resolution Utilizing Long-Range Temporal Information
+ Learning Neural Volumetric Representations of Dynamic Humans in Minutes
+ STB-VMM: Swin Transformer Based Video Motion Magnification
+ OPT: One-shot Pose-Controllable Talking Head Generation
+ One-Shot Face Video Re-enactment using Hybrid Latent Spaces of StyleGAN2
+ Video Waterdrop Removal via Spatio-Temporal Fusion in Driving Scenes
+ Structure and Content-Guided Video Synthesis with Diffusion Models
+ AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis
+ Dreamix: Video Diffusion Models are General Video Editors
+ SceneScape: Text-Driven Consistent Scene Generation
+ Dynamic Storyboard Generation in an Engine-based Virtual Environment for Video Production
+ Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation
+ Optical Flow Estimation in 360$^\circ$ Videos: Dataset, Model and Application
+ Parkinson gait modelling from an anomaly deep representation
+ Unsupervised Volumetric Animation
+ Text-To-4D Dynamic Scene Generation
+ Regeneration Learning: A Learning Paradigm for Data Generation
+ Building Scalable Video Understanding Benchmarks through Sports
+ Event-Based Frame Interpolation with Ad-hoc Deblurring
+ DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation
+ Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation
+ HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling
+ StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles
+ Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields
+ SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
+ SkyGPT: Probabilistic Short-term Solar Forecasting Using Synthetic Sky Videos from Physics-constrained VideoGPT
+ MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images
+ Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks
+ Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis
+ Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes
+ Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos
+ DynamicStereo: Consistent Dynamic Depth from Stereo Videos
+ ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs
+ Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis
+ 3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes
+ Learning Temporal Distribution and Spatial Correlation for Universal Moving Object Segmentation
+ Leveraging triplet loss for unsupervised action segmentation
+ MonoHuman: Animatable Human Neural Field from Monocular Video
+ Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion
+ Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert
+ VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs
+ RobustSwap: A Simple yet Robust Face Swapping Model against Attribute Leakage
+ CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis
+ Prediction of the morphological evolution of a splashing drop using an encoder-decoder
+ TriPlaneNet: An Encoder for EG3D Inversion
+ Dual-path Adaptation from Image to Video Transformers
+ Predictive Experience Replay for Continual Visual Control and Forecasting
+ Video-P2P: Video Editing with Cross-attention Control
+ IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF
+ Robust Dynamic Radiance Fields



## 2022



+ [Video Diffusion Models](https://arxiv.org/abs/2204.03458) (NeurIPS 2022)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2204.03458)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://video-diffusion.github.io/)

+ [McVd: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation](https://arxiv.org/abs/2205.09853) (NeurIPS 2022)  
  [![Star](https://img.shields.io/github/stars/Tobi-r9/RaMViD.svg?style=social&label=Star)](https://github.com/voletiv/mcvd-pytorch)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2205.09853)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://mask-cond-video-diffusion.github.io)

+ [Diffusion Models for Video Prediction and Infilling](https://arxiv.org/abs/2206.07696) (TMLR 2022)  
  [![Star](https://img.shields.io/github/stars/Tobi-r9/RaMViD.svg?style=social&label=Star)](https://github.com/Tobi-r9/RaMViD)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2206.07696)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sites.google.com/view/video-diffusion-prediction)

+ [Make-A-Video: Text-to-Video Generation without Text-Video Data](https://openreview.net/forum?id=nJfylDvgzlq) (ICLR 2023)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://openreview.net/forum?id=nJfylDvgzlq)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://makeavideo.studio)

+ [DaGAN: Depth-Aware Generative Adversarial Network for Talking Head Video Generation](https://arxiv.org/abs/2203.06605) (CVPR 2022)  
  [![Star](https://img.shields.io/github/stars/harlanhong/CVPR2022-DaGAN.svg?style=social&label=Star)](https://github.com/harlanhong/CVPR2022-DaGAN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2203.06605)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://harlanhong.github.io/publications/dagan.html)

+ [Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning](https://arxiv.org/abs/2203.02573) (CVPR 2022)  
  [![Star](https://img.shields.io/github/stars/snap-research/MMVID.svg?style=social&label=Star)](https://github.com/snap-research/MMVID)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2203.02573)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/MMVID/)

+ [Playable Environments: Video Manipulation in Space and Time](https://arxiv.org/abs/2203.01914) (CVPR 2022)  
  [![Star](https://img.shields.io/github/stars/willi-menapace/PlayableEnvironments.svg?style=social&label=Star)](https://github.com/willi-menapace/PlayableEnvironments)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2203.01914)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://willi-menapace.github.io/playable-environments-website/)

+ [Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis](https://arxiv.org/abs/2207.05049) (ECCV 2022)  
  [![Star](https://img.shields.io/github/stars/fast-vid2vid/fast-vid2vid.svg?style=social&label=Star)](https://github.com/fast-vid2vid/fast-vid2vid)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2207.05049)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://fast-vid2vid.github.io/)

+ [TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts](https://arxiv.org/abs/2207.01696) (ECCV 2022)  
  [![Star](https://img.shields.io/github/stars/EricGuo5513/TM2T.svg?style=social&label=Star)](https://github.com/EricGuo5513/TM2T)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2207.01696)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://ericguo5513.github.io/TM2T/)

+ [Imagen Video: High Definition Video Generation with Diffusion Models](https://arxiv.org/abs/2210.02303)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2210.02303)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://video-diffusion.github.io/)


+ [Phenaki: Variable length video generation from open domain textual description](https://arxiv.org/abs/2210.02399)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2210.02399)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sites.research.google/phenaki/)
  [alt-website](https://phenaki.video/)

  Code (unofficial?): [![Star](https://img.shields.io/github/stars/lucidrains/phenaki-pytorch.svg?style=social&label=Star)](https://github.com/XX/YY)

+ [Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation](https://arxiv.org/abs/2212.11565)  
  [![Star](https://img.shields.io/github/stars/bbzhu-jy16/motionvideogan.svg?style=social&label=Star)](https://github.com/showlab/Tune-A-Video)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.11565)

+ [Towards Smooth Video Composition](https://arxiv.org/abs/2212.07413)  
  [![Star](https://img.shields.io/github/stars/genforce/StyleSV.svg?style=social&label=Star)](https://github.com/genforce/StyleSV)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.07413)

+ [Latent Video Diffusion Models for High-Fidelity Long Video Generation](https://arxiv.org/abs/2211.13221)  
  [![Star](https://img.shields.io/github/stars/yingqinghe/lvdm.svg?style=social&label=Star)](https://github.com/yingqinghe/lvdm)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2211.13221)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://yingqinghe.github.io/LVDM/)

+ [SinFusion: Training Diffusion Models on a Single Image or Video](https://arxiv.org/abs/2211.11743)  
  [![Star](https://img.shields.io/github/stars/yanivnik/sinfusion-code.svg?style=social&label=Star)](https://github.com/yanivnik/sinfusion-code)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2211.11743)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://yanivnik.github.io/sinfusion/static/video_comparisons.html)

+ [INR-V: A Continuous Representation Space for Video-based Generative Tasks](https://arxiv.org/abs/2210.16579)  
  [![Star](https://img.shields.io/github/stars/bipashasen/INRV.svg?style=social&label=Star)](https://github.com/bipashasen/INRV)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2210.16579)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://skymanaditya1.github.io/INRV/)

+ [Computational Choreography using Human Motion Synthesis](https://arxiv.org/abs/2210.04366)  
  [![Star](https://img.shields.io/github/stars/patrickrperrine/comp-choreo.svg?style=social&label=Star)](https://github.com/patrickrperrine/comp-choreo)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2210.04366)

+ [Phenaki: Variable Length Video Generation From Open Domain Textual Description](https://arxiv.org/abs/2210.02399)  
  [![Star](https://img.shields.io/github/stars/lucidrains/phenaki-pytorch.svg?style=social&label=Star)](https://github.com/lucidrains/phenaki-pytorch)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2210.02399)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://phenaki.video/)

+ [Temporally Consistent Transformers for Video Generation](https://arxiv.org/abs/2210.02396)  
  [![Star](https://img.shields.io/github/stars/wilson1yan/teco.svg?style=social&label=Star)](https://github.com/wilson1yan/teco)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2210.02396)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://wilson1yan.github.io/teco/)

+ [StyleFaceV: Face Video Generation via Decomposing and Recomposing Pretrained StyleGAN3](https://arxiv.org/abs/2208.07862)  
  [![Star](https://img.shields.io/github/stars/arthur-qiu/stylefacev.svg?style=social&label=Star)](https://github.com/arthur-qiu/stylefacev)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2208.07862)
  [![Website](https://img.shields.io/badge/Website-9cf)](http://haonanqiu.com/projects/StyleFaceV.html)

+ [NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis](https://arxiv.org/abs/2207.09814)  
  [![Star](https://img.shields.io/github/stars/arthur-qiu/stylefacev.svg?style=social&label=Star)](https://github.com/microsoft/nuwa)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2207.09814)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://nuwa-infinity.microsoft.com/)

+ [3D-Aware Video Generation](https://arxiv.org/abs/2206.14797)  
  [![Star](https://img.shields.io/github/stars/sherwinbahmani/3dvideogeneration.svg?style=social&label=Star)](https://github.com/sherwinbahmani/3dvideogeneration/)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2206.14797)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sherwinbahmani.github.io/3dvidgen/)

+ [Patch-based Object-centric Transformers for Efficient Video Generation](https://arxiv.org/abs/2206.04003)  
  [![Star](https://img.shields.io/github/stars/wilson1yan/povt.svg?style=social&label=Star)](https://github.com/wilson1yan/povt)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2206.04003)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sites.google.com/view/povt-public)

+ [Generating Long Videos of Dynamic Scenes](https://arxiv.org/abs/2206.03429)  
  [![Star](https://img.shields.io/github/stars/nvlabs/long-video-gan.svg?style=social&label=Star)](https://github.com/nvlabs/long-video-gan)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2206.03429)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://www.timothybrooks.com/tech/long-video-gan/)

+ [D'ARTAGNAN: Counterfactual Video Generation](https://arxiv.org/abs/2206.01651)  
  [![Star](https://img.shields.io/github/stars/nvlabs/long-video-gan.svg?style=social&label=Star)](https://github.com/hreynaud/dartagnan)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2206.01651)

+ [CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers](https://arxiv.org/abs/2205.15868)  
  [![Star](https://img.shields.io/github/stars/nvlabs/long-video-gan.svg?style=social&label=Star)](https://github.com/thudm/cogvideo)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2205.15868)

+ [Latent Video Diffusion Models for High-Fidelity Video Generation With Arbitrary Lengths](https://arxiv.org/abs/2211.13221)  
  [![Star](https://img.shields.io/github/stars/YingqingHe/LVDM.svg?style=social&label=Star)](https://github.com/YingqingHe/LVDM)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2211.13221)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://yingqinghe.github.io/LVDM/)

+ [MagicVideo: Efficient Video Generation With Latent Diffusion Models](https://arxiv.org/abs/2211.11018)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2211.11018)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://magicvideo.github.io/#)

+ [Diffusion Probabilistic Modeling for Video Generation](https://arxiv.org/abs/2203.09481)  
  [![Star](https://img.shields.io/github/stars/buggyyang/RVD.svg?style=social&label=Star)](https://github.com/buggyyang/RVD)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2203.09481)

+ [Flexible Diffusion Modeling of Long Videos](https://arxiv.org/abs/2205.11495)  
  [![Star](https://img.shields.io/github/stars/plai-group/flexible-video-diffusion-modeling.svg?style=social&label=Star)](https://github.com/plai-group/flexible-video-diffusion-modeling)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2205.11495)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://fdmolv.github.io/)

+ [Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer](https://arxiv.org/pdf/2204.03638) (ECCV 2022)  
  [![Star](https://img.shields.io/github/stars/songweige/tats.svg?style=social&label=Star)](https://github.com/songweige/tats)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2204.03638)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://songweige.github.io/projects/tats/index.html)

+ [Diffusion Probabilistic Modeling for Video Generation](https://arxiv.org/pdf/2203.09481)  
  [![Star](https://img.shields.io/github/stars/buggyyang/rvd.svg?style=social&label=Star)](https://github.com/buggyyang/rvd)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2203.09481)

+ [StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN](https://arxiv.org/pdf/2203.04036)  
  [![Star](https://img.shields.io/github/stars/OpenTalker/StyleHEAT.svg?style=social&label=Star)](https://github.com/OpenTalker/StyleHEAT)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2203.04036)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://feiiyin.github.io/StyleHEAT/)

+ [Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks](https://arxiv.org/pdf/2202.10571) (ICLR 2022)  
  [![Star](https://img.shields.io/github/stars/sihyun-yu/digan.svg?style=social&label=Star)](https://github.com/sihyun-yu/digan)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2202.10571)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sihyun.me/digan/)

+ [StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2](https://arxiv.org/pdf/2112.14683) (CVPR 2022)  
  [![Star](https://img.shields.io/github/stars/universome/stylegan-v.svg?style=social&label=Star)](https://github.com/universome/stylegan-v)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2112.14683)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://universome.github.io/stylegan-v.html)

+ [Make It Move: Controllable Image-to-Video Generation with Text Descriptions](https://arxiv.org/pdf/2112.02815) (CVPR 2022)  
  [![Star](https://img.shields.io/github/stars/youncy-hu/mage.svg?style=social&label=Star)](https://github.com/youncy-hu/mage)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2112.02815)

<br>

__Needs verification & further processing:__

+ NIRVANA: Neural Implicit Representations of Videos with Adaptive Networks and Autoregressive Patch-wise Modeling
+ How Do Deepfakes Move? Motion Magnification for Deepfake Source Detection
+ NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action
+ Latent Discretization for Continuous-time Sequence Compression
+ Cross-Resolution Flow Propagation for Foveated Video Super-Resolution
+ MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular Videos
+ DDH-QA: A Dynamic Digital Humans Quality Assessment Database
+ Scalable Adaptive Computation for Iterative Generation
+ Predictive Coding Based Multiscale Network with Encoder-Decoder LSTM for Video Prediction
+ InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds
+ MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation
+ PointAvatar: Deformable Point-based Head Avatars from Videos
+ EVAL: Explainable Video Anomaly Localization
+ PV3D: A 3D Generative Model for Portrait Video Generation
+ Video Prediction by Efficient Transformers
+ Fighting Malicious Media Data: A Survey on Tampering Detection and Deepfake Detection
+ MAGVIT: Masked Generative Video Transformer
+ Physically Plausible Animation of Human Upper Body from a Single Image
+ Reinforcement Learning for Predicting Traffic Accidents
+ MIMO Is All You Need : A Strong Multi-In-Multi-Out Baseline for Video Prediction
+ Neural Cell Video Synthesis via Optical-Flow Diffusion
+ Video Object of Interest Segmentation
+ Muscles in Action
+ Audio-Driven Co-Speech Gesture Video Generation
+ FakeOut: Leveraging Out-of-domain Self-supervision for Multi-modal Video Deepfake Detection
+ VIDM: Video Implicit Diffusion Models
+ Mixed Neural Voxels for Fast Multi-view Video Synthesis
+ Spatio-Temporal Crop Aggregation for Video Representation Learning
+ VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild
+ Randomized Conditional Flow Matching for Video Prediction
+ Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis
+ WALDO: Future Video Synthesis using Object Layer Decomposition and Parametric Flow Prediction
+ Efficient Feature Extraction for High-resolution Video Frame Interpolation
+ Dynamic Neural Portraits
+ TPA-Net: Generate A Dataset for Text to Physics-based Animation
+ Make-A-Story: Visual Memory Conditioned Consistent Story Generation
+ Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation
+ Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video
+ SuperTran: Reference Based Video Transformer for Enhancing Low Bitrate Streams in Real Time
+ Touch and Go: Learning from Human-Collected Vision and Touch
+ Depth-Supervised NeRF for Multi-View RGB-D Operating Room Images
+ Real-time Neural Radiance Talking Portrait Synthesis via Audio-spatial Decomposition
+ SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation
+ FLEX: Full-Body Grasping Without Full-Body Grasps
+ Blur Interpolation Transformer for Real-World Motion from Blur
+ DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata
+ H-VFI: Hierarchical Frame Interpolation for Videos with Large Motions
+ AdaFNIO: Adaptive Fourier Neural Interpolation Operator for video frame interpolation
+ VarietySound: Timbre-Controllable Video to Sound Generation via Unsupervised Information Disentanglement
+ Pedestrian Spatio-Temporal Information Fusion For Video Anomaly Detection
+ SPACE: Speech-driven Portrait Animation with Controllable Expression
+ UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer
+ Creative divergent synthesis with generative models
+ CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming
+ Advancing Learned Video Compression with In-loop Frame Prediction
+ SSGVS: Semantic Scene Graph-to-Video Synthesis
+ Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories
+ Temporal Consistency Learning of inter-frames for Video Super-Resolution
+ SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory
+ CPG-RL: Learning Central Pattern Generators for Quadruped Locomotion
+ Learning Variational Motion Prior for Video-based Motion Capture
+ Streaming Radiance Fields for 3D Video Synthesis
+ Learning to forecast vegetation greenness at fine resolution over Africa with ConvLSTMs
+ EpipolarNVS: leveraging on Epipolar geometry for single-image Novel View Synthesis
+ Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization
+ Facial Expression Video Generation Based-On Spatio-temporal Convolutional GAN: FEV-GAN
+ Transfer-learning for video classification: Video Swin Transformer on multiple domains
+ Temporal and Contextual Transformer for Multi-Camera Editing of TV Shows
+ Stochastic Occupancy Grid Map Prediction in Dynamic Scenes
+ MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Pose
+ Pre-Avatar: An Automatic Presentation Generation Framework Leveraging Talking Avatar
+ AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars
+ A Generalist Framework for Panoptic Segmentation of Images and Videos
+ Masked Motion Encoding for Self-Supervised Video Representation Learning
+ SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models
+ Controllable Radiance Fields for Dynamic Face Synthesis
+ A unified model for continuous conditional video prediction
+ Contrastive Video-Language Learning with Fine-grained Frame Sampling
+ DeepHS-HDRVideo: Deep High Speed High Dynamic Range Video Reconstruction
+ Self-supervised Video Representation Learning with Motion-Aware Masked Autoencoders
+ Compressing Video Calls using Synthetic Talking Heads
+ Text-driven Video Prediction
+ Audio-Visual Face Reenactment
+ Geometry Driven Progressive Warping for One-Shot Face Animation
+ BVI-VFI: A Video Quality Database for Video Frame Interpolation
+ Cross-identity Video Motion Retargeting with Joint Transformation and Synthesis
+ Spatio-Temporal Relation Learning for Video Anomaly Detection
+ Multi-modal Video Chapter Generation
+ Real-RawVSR: Real-World Raw Video Super-Resolution with a Benchmark Dataset
+ Anomaly Detection in Aerial Videos with Transformers
+ VToonify: Controllable High-Resolution Portrait Video Style Transfer
+ T3VIP: Transformation-based 3D Video Prediction
+ NeuralMarker: A Framework for Learning General Marker Correspondence
+ AutoLV: Automatic Lecture Video Generator
+ Continuously Controllable Facial Expression Editing in Talking Face Videos
+ A Deep Moving-camera Background Model
+ HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator
+ Talking Head from Speech Audio using a Pre-trained Image Generator
+ Treating Motion as Option to Reduce Motion Dependency in Unsupervised Video Object Segmentation
+ Neural Sign Reenactor: Deep Photorealistic Sign Language Retargeting
+ Diffusion Models: A Comprehensive Survey of Methods and Applications
+ REMOT: A Region-to-Whole Framework for Realistic Human Motion Transfer
+ SketchBetween: Video-to-Video Synthesis for Sprite Animation via Sketches
+ StableFace: Analyzing and Improving Motion Stability for Talking Face Generation
+ VMFormer: End-to-End Video Matting with Transformer
+ Neural Novel Actor: Learning a Generalized Animatable Neural Representation for Human Actors
+ StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation
+ Towards MOOCs for Lipreading: Using Synthetic Talking Heads to Train Humans in Lipreading at Scale
+ Temporal View Synthesis of Dynamic Scenes through 3D Object Motion Estimation with Multi-Plane Images
+ Wildfire Forecasting with Satellite Images and Deep Generative Model
+ Video Interpolation by Event-driven Anisotropic Adjustment of Optical Flow
+ Extreme-scale Talking-Face Video Upsampling with Audio-Visual Priors
+ Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective
+ Semi-Supervised Video Inpainting with Cycle Consistency Constraints
+ UAV-CROWD: Violent and non-violent crowd activity simulator from the perspective of UAV
+ Motion Sensitive Contrastive Learning for Self-supervised Video Representation
+ Cine-AI: Generating Video Game Cutscenes in the Style of Human Directors
+ Language-Guided Face Animation by Recurrent StyleGAN-based Generator
+ Leveraging Endo- and Exo-Temporal Regularization for Black-box Video Domain Adaptation
+ Boosting neural video codecs by exploiting hierarchical redundancy
+ PS-NeRV: Patch-wise Stylized Neural Representations for Videos
+ Real-time Gesture Animation Generation from Speech for Virtual Human Interaction
+ Multimodal Generation of Novel Action Appearances for Synthetic-to-Real Recognition of Activities of Daily Living
+ Motion-aware Memory Network for Fast Video Salient Object Detection
+ Meta-Interpolation: Time-Arbitrary Frame Interpolation via Dual Meta-Learning
+ Efficient Video Deblurring Guided by Motion Magnitude
+ Error-Aware Spatial Ensembles for Video Frame Interpolation
+ Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis
+ InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images
+ RealFlow: EM-based Realistic Optical Flow Dataset Generation from Videos
+ Towards Interpretable Video Super-Resolution via Alternating Optimization
+ Error Compensation Framework for Flow-Guided Video Inpainting
+ Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance
+ TTVFI: Learning Trajectory-Aware Transformer for Video Frame Interpolation
+ Audio Input Generates Continuous Frames to Synthesize Facial Video Using Generative Adiversarial Networks
+ Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision and Language Models
+ Neighbor Correspondence Matching for Flow-based Video Frame Synthesis
+ Deepfake Video Detection with Spatiotemporal Dropout Transformer
+ You Only Align Once: Bidirectional Interaction for Spatial-Temporal Video Super-Resolution
+ Long-term Leap Attention, Short-term Periodic Shift for Video Classification
+ CANF-VC: Conditional Augmented Normalizing Flows for Video Compression
+ A Probabilistic Model Of Interaction Dynamics for Dyadic Face-to-Face Settings
+ Cross-Attention Transformer for Video Interpolation
+ Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation
+ Video Dialog as Conversation about Objects Living in Space-Time
+ Segmenting Moving Objects via an Object-Centric Layered Representation
+ Exploring Temporally Dynamic Data Augmentation for Video Recognition
+ Programmatic Concept Learning for Human Motion Description and Synthesis
+ Optimizing Video Prediction via Video Frame Interpolation
+ Video Anomaly Detection via Prediction Network with Enhanced Spatio-Temporal Memory Exchange
+ Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer
+ MaskViT: Masked Visual Pre-Training for Video Prediction
+ REVECA -- Rich Encoder-decoder framework for Video Event CAptioner
+ Enhanced Bi-directional Motion Estimation for Video Frame Interpolation
+ Multimodal Dialogue State Tracking
+ E2V-SDE: From Asynchronous Events to Fast and Continuous Video Reconstruction via Neural Stochastic Differential Equations
+ Stand-Alone Inter-Frame Attention in Video Models
+ Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos
+ STIP: A SpatioTemporal Information-Preserving and Perception-Augmented Model for High-Resolution Video Prediction
+ JNMR: Joint Non-linear Motion Regression for Video Frame Interpolation
+ SimVP: Simpler yet Better Video Prediction
+ Learning Task Agnostic Temporal Consistency Correction
+ Recurrent Video Restoration Transformer with Guided Deformable Attention
+ Cascaded Video Generation for Videos In-the-Wild
+ D$^2$NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video
+ A Survey of Deep Fake Detection for Trial Courts
+ TubeFormer-DeepLab: Video Mask Transformer
+ IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation
+ Feature-Aligned Video Raindrop Removal with Temporal Constraints
+ Future Transformer for Long-term Action Anticipation
+ Video2StyleGAN: Disentangling Local and Global Variations in a Video
+ Merkel Podcast Corpus: A Multimodal Dataset Compiled from 16 Years of Angela Merkel's Weekly Video Podcasts
+ Automatic Generation of Synthetic Colonoscopy Videos for Domain Randomization
+ Latent-space disentanglement with untrained generator networks for the isolation of different motion types in video data
+ Video Frame Interpolation with Transformer
+ Multi-encoder Network for Parameter Reduction of a Kernel-based Interpolation Architecture
+ The Effectiveness of Temporal Dependency in Deepfake Video Detection
+ Diverse Video Generation from a Single Video
+ Video-ReTime: Learning Temporally Varying Speediness for Time Remapping
+ Spatial-Temporal Space Hand-in-Hand: Spatial-Temporal Video Super-Resolution via Cycle-Projected Mutual Learning
+ Image2Gif: Generating Continuous Realistic Animations with Warping NODEs
+ GAN-Based Multi-View Video Coding with Spatio-Temporal EPI Reconstruction
+ Dual-Level Decoupled Transformer for Video Captioning
+ Parametric Reshaping of Portraits in Videos
+ Video Extrapolation in Space and Time
+ Zero-Episode Few-Shot Contrastive Predictive Coding: Solving intelligence tests without prior training
+ Copy Motion From One to Another: Fake Motion Video Generation
+ Neural Implicit Representations for Physical Parameter Inference from a Single Video
+ Talking Head Generation Driven by Speech-Related Facial Action Units and Audio- Based on Multimodal Representation Fusion
+ Restricted Black-box Adversarial Attack Against DeepFake Face Swapping
+ ClothFormer:Taming Video Virtual Try-on in All Module
+ Learning Dynamic View Synthesis With Few RGBD Cameras
+ Future Object Detection with Spatiotemporal Transformers
+ STAU: A SpatioTemporal-Aware Unit for Video Prediction and Beyond
+ Sound-Guided Semantic Video Generation
+ Less than Few: Self-Shot Video Instance Segmentation
+ Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion
+ End-to-end Dense Video Captioning as Sequence Generation
+ MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration
+ Controllable Video Generation through Global and Local Motion Dynamics
+ Dynamic Neural Textures: Generating Talking-Face Videos with Continuously Controllable Expressions
+ Self-Supervised Traffic Advisors: Distributed, Multi-view Traffic Prediction for Smart Cities
+ Structure-Aware Motion Transfer with Deformable Anchor Model
+ Self-Supervised Video Representation Learning with Motion-Contrastive Perception
+ HSTR-Net: High Spatio-Temporal Resolution Video Generation For Wide Area Surveillance
+ SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage
+ Many-to-many Splatting for Efficient Video Frame Interpolation
+ Video Demoireing with Relation-Based Temporal Consistency
+ Learning Video Salient Object Detection Progressively from Unlabeled Videos
+ Neural Rendering of Humans in Novel View and Pose from Monocular Video
+ MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images
+ Foveation-based Deep Video Compression without Motion Search
+ STRPM: A Spatiotemporal Residual Predictive Model for High-Resolution Video Prediction
+ High-resolution Face Swapping via Latent Semantics Disentanglement
+ VPTR: Efficient Transformers for Video Prediction
+ Long-term Video Frame Interpolation via Feature Propagation
+ Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production
+ Dressing in the Wild by Watching Dance Videos
+ Structured Local Radiance Fields for Human Avatar Modeling
+ V3GAN: Decomposing Background, Foreground and Motion for Video Generation
+ Reinforcement Learning with Action-Free Pre-Training from Videos
+ Keypoints Tracking via Transformer Networks
+ VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training
+ Unifying Motion Deblurring and Frame Interpolation with Events
+ QS-Craft: Learning to Quantize, Scrabble and Craft for Conditional Human Motion Animation
+ Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields
+ Stochastic Video Prediction with Structure and Motion
+ Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation
+ Beyond a Video Frame Interpolator: A Space Decoupled Learning Approach to Continuous Image Transition
+ Transframer: Arbitrary Frame Prediction with Generative Models
+ Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image
+ MSPred: Video Prediction at Multiple Spatio-Temporal Scales with Hierarchical Recurrent Networks
+ Latent Image Animator: Learning to Animate Images via Latent Space Navigation
+ DialogueNeRF: Towards Realistic Avatar Face-to-face Conversation Video Generation
+ Implicit Motion Handling for Video Camouflaged Object Detection
+ One-stage Video Instance Segmentation: From Frame-in Frame-out to Clip-in Clip-out
+ NeRFocus: Neural Radiance Field for 3D Synthetic Defocus
+ Source-free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition
+ Generative Cooperative Learning for Unsupervised Video Anomaly Detection
+ Visually Supervised Speaker Detection and Localization via Microphone Array
+ A Novel Dual Dense Connection Network for Video Super-resolution
+ Region-of-Interest Based Neural Video Compression
+ PanoFlow: Learning 360° Optical Flow for Surrounding Temporal Understanding
+ Model Attribution of Face-swap Deepfake Videos
+ Instantaneous Physiological Estimation using Video Transformers
+ Thinking the Fusion Strategy of Multi-reference Face Reenactment
+ Cut and Continuous Paste towards Real-time Deep Fall Detection
+ Neural Marionette: Unsupervised Learning of Motion Skeleton and Latent Dynamics from Volumetric Video
+ Enhancing Deformable Convolution based Video Frame Interpolation with Coarse-to-fine 3D CNN
+ Exploring Discontinuity for Video Frame Interpolation
+ Robust Deepfake On Unrestricted Media: Generation And Detection
+ A new face swap method for image and video domains: a technical report
+ Feature-Style Encoder for Style-Based GAN Inversion
+ Third Time's the Charm? Image and Video Editing with StyleGAN3
+ Deep Video Prior for Video Consistency and Propagation
+ Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions
+ Semantically Video Coding: Instill Static-Dynamic Clues into Structured Bitstream for AI Tasks
+ Splatting-based Synthesis for Video Frame Interpolation
+ Stitch it in Time: GAN-Based Facial Editing of Real Videos
+ End-to-end Generative Pretraining for Multimodal Video Captioning
+ Self-Supervised Deep Blind Video Super-Resolution
+ Autoencoding Video Latents for Adversarial Video Generation
+ AugLy: Data Augmentations for Robustness
+ Towards Realistic Visual Dubbing with Heterogeneous Sources
+ Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels
+ Learning Temporally and Semantically Consistent Unpaired Video-to-video Translation Through Pseudo-Supervision From Synthetic Optical Flow
+ MetaDance: Few-shot Dancing Video Retargeting via Temporal-aware Meta-learning
+ Music2Video: Automatic Generation of Music Video with fusion of audio and text
+ MobileFaceSwap: A Lightweight Framework for Video Face Swapping
+ Structured 3D Features for Reconstructing Controllable Avatars
+ MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis
+ High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors
+ 3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models
+ Audio-visual video face hallucination with frequency supervision and cross modality support by speech based lip reading loss
+ It Takes Two: Masked Appearance-Motion Modeling for Self-supervised Video Transformer Pre-training
+ See, Plan, Predict: Language-guided Cognitive Planning with Video Prediction
+ Motion Transformer for Unsupervised Image Animation
+ Low-Light Video Enhancement with Synthetic Event Guidance
+ Neural Capture of Animatable 3D Human from Monocular Video
+ NDF: Neural Deformable Fields for Dynamic Human Modelling
+ Diverse Dance Synthesis via Keyframes with Transformer Controllers
+ CTrGAN: Cycle Transformers GAN for Gait Transfer
+ Enhanced Deep Animation Video Interpolation
+ An Identity-Preserved Framework for Human Motion Transfer
+ Unsupervised Coherent Video Cartoonization with Perceptual Motion Consistency
+ Learning Multi-Object Dynamics with Compositional Neural Radiance Fields
+ Transporters with Visual Foresight for Solving Unseen Rearrangement Tasks
+ VRT: A Video Restoration Transformer


## 2021

+ [NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion](https://arxiv.org/pdf/2111.12417)  
  [![Star](https://img.shields.io/github/stars/lucidrains/nuwa-pytorch.svg?style=social&label=Star)](https://github.com/lucidrains/nuwa-pytorch)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2111.12417)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://www.microsoft.com/en-us/research/project/nuwa-infinity/)

+ [Generative Adversarial Graph Convolutional Networks for Human Action Synthesis](https://arxiv.org/pdf/2110.11191) (WACV 2022)  
  [![Star](https://img.shields.io/github/stars/degardinbruno/kinetic-gan.svg?style=social&label=Star)](https://github.com/degardinbruno/kinetic-gan)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2110.11191)

+ [Towards Using Clothes Style Transfer for Scenario-aware Person Video Generation](https://arxiv.org/pdf/2110.11894)  
  [![Star](https://img.shields.io/github/stars/xsimba123/demos-of-csf-sa.svg?style=social&label=Star)](https://github.com/xsimba123/demos-of-csf-sa)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/pdf/2110.11894)

+ [Latent Image Animator: Learning to animate image via latent space navigation](https://arxiv.org/pdf/2203.09043) (ICLR 2022)  
  [![Star](https://img.shields.io/github/stars/wyhsirius/LIA.svg?style=social&label=Star)](https://github.com/wyhsirius/LIA)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2203.09043)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://wyhsirius.github.io/LIA-project/)

+ [SLAMP: Stochastic Latent Appearance and Motion Prediction](https://arxiv.org/pdf/2108.02760) (ICCV 2021)  
  [![Star](https://img.shields.io/github/stars/wyhsirius/LIA.svg?style=social&label=Star)](https://github.com/kaanakan/slamp)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2108.02760)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://kuis-ai.github.io/slamp/)

+ [VirtualConductor: Music-driven Conducting Video Generation System](https://arxiv.org/pdf/2108.04350) (ICME 2021)  
  [![Star](https://img.shields.io/github/stars/ChenDelong1999/VirtualConductor.svg?style=social&label=Star)](https://github.com/ChenDelong1999/VirtualConductor)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2108.04350)

+ [Click to Move: Controlling Video Generation with Sparse Motion](https://arxiv.org/abs/2108.08815) (ICCV 2021)  
  [![Star](https://img.shields.io/github/stars/PierfrancescoArdino/C2M.svg?style=social&label=Star)](https://github.com/PierfrancescoArdino/C2M)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2108.08815)

+ [VideoGPT: Video Generation using VQ-VAE and Transformers](https://arxiv.org/pdf/2104.10157v2.pdf)  
  [![Star](https://img.shields.io/github/stars/wilson1yan/VideoGPT.svg?style=social&label=Star)](https://github.com/wilson1yan/VideoGPT)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/pdf/2104.10157v2.pdf)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://wilson1yan.github.io/videogpt/index.html)


+ [Latent Neural Differential Equations for Video Generation](https://arxiv.org/pdf/2011.03864v3.pdf)  
  [![Star](https://img.shields.io/github/stars/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation.svg?style=social&label=Star)](https://github.com/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/pdf/2011.03864v3.pdf)

+ [Stochastic Image-to-Video Synthesis Using cINNs](https://openaccess.thecvf.com/content/CVPR2021/papers/Dorkenwald_Stochastic_Image-to-Video_Synthesis_Using_cINNs_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![Star](https://img.shields.io/github/stars/CompVis/image2video-synthesis-using-cINNs.svg?style=social&label=Star)](https://github.com/CompVis/image2video-synthesis-using-cINNs)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2105.04551)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://compvis.github.io/image2video-synthesis-using-cINNs/)


+ [Understanding Object Dynamics for Interactive Image-to-Video Synthesis](https://openaccess.thecvf.com/content/CVPR2021/papers/Blattmann_Understanding_Object_Dynamics_for_Interactive_Image-to-Video_Synthesis_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![Star](https://img.shields.io/github/stars/CompVis/interactive-image2video-synthesis.svg?style=social&label=Star)](https://github.com/CompVis/interactive-image2video-synthesis)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2106.11303)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://compvis.github.io/interactive-image2video-synthesis/)

+ [One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_One-Shot_Free-View_Neural_Talking-Head_Synthesis_for_Video_Conferencing_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![Star](https://img.shields.io/github/stars/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis.svg?style=social&label=Star)](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2011.15126)

+ [Flow Guided Transformable Bottleneck Networks for Motion Retargeting](https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Flow_Guided_Transformable_Bottleneck_Networks_for_Motion_Retargeting_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2106.07771)

+ [Stable View Synthesis](https://openaccess.thecvf.com/content/CVPR2021/papers/Riegler_Stable_View_Synthesis_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![Star](https://img.shields.io/github/stars/isl-org/StableViewSynthesis.svg?style=social&label=Star)](https://github.com/isl-org/StableViewSynthesis)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2011.07233)

+ [Scene-Aware Generative Network for Human Motion Synthesis](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Scene-Aware_Generative_Network_for_Human_Motion_Synthesis_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2105.14804)

+ [Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Neural_Scene_Flow_Fields_for_Space-Time_View_Synthesis_of_Dynamic_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![Star](https://img.shields.io/github/stars/zhengqili/Neural-Scene-Flow-Fields.svg?style=social&label=Star)](https://github.com/zhengqili/Neural-Scene-Flow-Fields)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2011.13084)

+ [Deep Animation Video Interpolation in the Wild](https://openaccess.thecvf.com/content/CVPR2021/papers/Siyao_Deep_Animation_Video_Interpolation_in_the_Wild_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![Star](https://img.shields.io/github/stars/lisiyao21/AnimeInterp.svg?style=social&label=Star)](https://github.com/lisiyao21/AnimeInterp)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2104.02495)

+ [High-Fidelity Neural Human Motion Transfer from Monocular Video](https://openaccess.thecvf.com/content/CVPR2021/papers/Kappel_High-Fidelity_Neural_Human_Motion_Transfer_From_Monocular_Video_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![Star](https://img.shields.io/github/stars/MoritzKappel/HF-NHMT.svg?style=social&label=Star)](https://github.com/MoritzKappel/HF-NHMT)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2012.10974)

+ [Learning Semantic-Aware Dynamics for Video Prediction](https://openaccess.thecvf.com/content/CVPR2021/papers/Bei_Learning_Semantic-Aware_Dynamics_for_Video_Prediction_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2104.09762)

+ [Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Flow-Guided_One-Shot_Talking_Face_Generation_With_a_High-Resolution_Audio-Visual_Dataset_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![Star](https://img.shields.io/github/stars/MRzzm/HDTF.svg?style=social&label=Star)](https://github.com/MRzzm/HDTF)

+ [Layout-Guided Novel View Synthesis From a Single Indoor Panorama](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Layout-Guided_Novel_View_Synthesis_From_a_Single_Indoor_Panorama_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![Star](https://img.shields.io/github/stars/bluestyle97/PNVS.svg?style=social&label=Star)](https://github.com/bluestyle97/PNVS)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2103.17022)

+ [Space-Time Neural Irradiance Fields for Free-Viewpoint Video](https://openaccess.thecvf.com/content/CVPR2021/papers/Xian_Space-Time_Neural_Irradiance_Fields_for_Free-Viewpoint_Video_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2011.12950)

+ [GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_GeoSim_Realistic_Video_Simulation_via_Geometry-Aware_Composition_for_Self-Driving_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2101.06543)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://tmux.top/publication/geosim/)

+ [Animating Pictures With Eulerian Motion Fields](https://openaccess.thecvf.com/content/CVPR2021/papers/Holynski_Animating_Pictures_With_Eulerian_Motion_Fields_CVPR_2021_paper.pdf) (CVPR 2021)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2011.15128)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://eulerian.cs.washington.edu/)

+ [SLAMP: Stochastic Latent Appearance and Motion Prediction](https://arxiv.org/pdf/2108.02760) (ICCV 2021)  
  [![Star](https://img.shields.io/github/stars/wyhsirius/LIA.svg?style=social&label=Star)](https://github.com/kaanakan/slamp)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2108.02760)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://kuis-ai.github.io/slamp/)

+ [CCVS: Context-aware Controllable Video Synthesis](https://arxiv.org/pdf/2107.08037v2) (NeurIPS 2021)  
  [![Star](https://img.shields.io/github/stars/16lemoing/ccvs.svg?style=social&label=Star)](https://github.com/16lemoing/ccvs)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2107.08037v2)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://16lemoing.github.io/ccvs/)

+ [Diverse Video Generation using a Gaussian Process Trigger](https://arxiv.org/pdf/2107.04619) (ICLR 2021)  
  [![Star](https://img.shields.io/github/stars/shgaurav1/DVG.svg?style=social&label=Star)](https://github.com/shgaurav1/DVG)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2107.04619)

+ [FitVid: Overfitting in Pixel-Level Video Prediction](https://arxiv.org/pdf/2106.13195)  
  [![Star](https://img.shields.io/github/stars/google-research/fitvid.svg?style=social&label=Star)](https://github.com/google-research/fitvid)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2106.13195)

+ [NWT: Towards natural audio-to-video generation with representation learning](https://arxiv.org/pdf/2106.04283)  
  [![Star](https://img.shields.io/github/stars/lucidrains/NWT-pytorch.svg?style=social&label=Star)](https://github.com/lucidrains/NWT-pytorch)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2106.04283)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://next-week-tonight.github.io/NWT_blog/)

+ [Editable Free-viewpoint Video Using a Layered Neural Representation](https://arxiv.org/pdf/2104.14786)  
  [![Star](https://img.shields.io/github/stars/darlinghang/st-nerf.svg?style=social&label=Star)](https://github.com/darlinghang/st-nerf)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2104.14786)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://jiakai-zhang.github.io/st-nerf/)

+ [A Good Image Generator Is What You Need for High-Resolution Video Synthesis](https://arxiv.org/pdf/2104.15069)  
  [![Star](https://img.shields.io/github/stars/snap-research/MoCoGAN-HD.svg?style=social&label=Star)](https://github.com/snap-research/MoCoGAN-HD)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2104.15069)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://bluer555.github.io/MoCoGAN-HD/)

+ [GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions](https://arxiv.org/pdf/2104.14806)  
  [![Star](https://img.shields.io/github/stars/mehdidc/DALLE_clip_score.svg?style=social&label=Star)](https://github.com/mehdidc/DALLE_clip_score)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2104.14806)

+ [Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme-Pose Dictionary](https://arxiv.org/pdf/2104.14631)  
  [![Star](https://img.shields.io/github/stars/sibozhang/Text2Video.svg?style=social&label=Star)](https://github.com/sibozhang/Text2Video)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/pdf/2104.14631)

+ [Adaptive Appearance Rendering](https://arxiv.org/pdf/2104.11931)  
  [![Star](https://img.shields.io/github/stars/wisdomdeng/AdaptiveRendering.svg?style=social&label=Star)](https://github.com/wisdomdeng/AdaptiveRendering)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2104.11931)

+ [Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation](https://arxiv.org/pdf/2104.07995)  
  [![Star](https://img.shields.io/github/stars/FuxiVirtualHuman/Write-a-Speaker.svg?style=social&label=Star)](https://github.com/FuxiVirtualHuman/Write-a-Speaker)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2104.07995)

+ [Predicting Video with VQVAE](https://arxiv.org/pdf/2103.01950)  
  [![Star](https://img.shields.io/github/stars/FuxiVirtualHuman/Write-a-Speaker.svg?style=social&label=Star)](https://github.com/mattiasxu/Video-VQVAE)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2103.01950)

+ [Playable Video Generation](https://arxiv.org/pdf/2101.12195) (CVPR 2021)  
  [![Star](https://img.shields.io/github/stars/FuxiVirtualHuman/Write-a-Speaker.svg?style=social&label=Star)](https://github.com/willi-menapace/PlayableVideoGeneration)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2101.12195)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://willi-menapace.github.io/playable-video-generation-website/)

+ [Infinite Nature: Perpetual View Generation of Natural Scenes from a Single Image](https://arxiv.org/pdf/2012.09855) (ICCV 2021)  
  [![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star)](https://github.com/google-research/google-research/tree/master/infinite_nature)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2012.09855)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://infinite-nature.github.io/)

+ [Vid-ODE: Continuous-Time Video Generation with Neural Ordinary Differential Equation](https://arxiv.org/pdf/2010.08188) (AAAI 2021)  
  [![Star](https://img.shields.io/github/stars/psh01087/Vid-ODE.svg?style=social&label=Star)](https://github.com/psh01087/Vid-ODE)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2010.08188)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://psh01087.github.io/Vid-ODE/)

+ [Compositional Video Synthesis with Action Graphs](https://arxiv.org/pdf/2006.15327) (ICML 2021)  
  [![Star](https://img.shields.io/github/stars/roeiherz/AG2Video.svg?style=social&label=Star)](https://github.com/roeiherz/AG2Video)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2006.15327)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://research.nvidia.com/labs/par/publication/sg2vid.html)

+ [Temporal Shift GAN for Large Scale Video Generation](https://arxiv.org/pdf/2004.01823) (WACV 2021)

  [![Star](https://img.shields.io/github/stars/amunozgarza/tsb-gan.svg?style=social&label=Star)](https://github.com/amunozgarza/tsb-gan)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2004.01823)



__temporary__:

Dynamic View Synthesis from Dynamic Monocular Video (ICCV 2021)
https://free-view-video.github.io/


Spatio-temporal Predictive Network For Videos With Physical Properties (CVPR 2021)


## 2020

+ [Stochastic Talking Face Generation Using Latent Distribution Matching](https://arxiv.org/pdf/2011.10727)  
  [![Star](https://img.shields.io/github/stars/ry85/Stochastic-Talking-Face-Generation-Using-Latent-Distribution-Matching.svg?style=social&label=Star)](https://github.com/ry85/Stochastic-Talking-Face-Generation-Using-Latent-Distribution-Matching)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2011.10727)

+ [Latent Neural Differential Equations for Video Generation](https://arxiv.org/pdf/2011.03864)  
  [![Star](https://img.shields.io/github/stars/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation.svg?style=social&label=Star)](https://github.com/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2011.03864)

+ [LIFI: Towards Linguistically Informed Frame Interpolation](https://arxiv.org/pdf/2010.16078)  
  [![Star](https://img.shields.io/github/stars/midas-research/linguistically-informed-frame-interpolation.svg?style=social&label=Star)](https://github.com/midas-research/linguistically-informed-frame-interpolation)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2010.16078)

+ [DTVNet: Dynamic Time-lapse Video Generation via Single Still Image](https://arxiv.org/pdf/2008.04776) (ECCV 2020)  
  [![Star](https://img.shields.io/github/stars/zhangzjn/DTVNet.svg?style=social&label=Star)](https://github.com/zhangzjn/DTVNet)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2008.04776)

+ [Unsupervised object-centric video generation and decomposition in 3D](https://arxiv.org/pdf/2007.06705) (NeurIPS 2020)  
  [![Star](https://img.shields.io/github/stars/pmh47/o3v.svg?style=social&label=Star)](https://github.com/pmh47/o3v)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2007.06705)

+ [Novel-View Human Action Synthesis](https://arxiv.org/pdf/2007.02808) (ACCV 2020)  
  [![Star](https://img.shields.io/github/stars/mlakhal/gtnet.svg?style=social&label=Star)](https://github.com/mlakhal/gtnet)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2007.02808)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://mlakhal.github.io/novel-view_action_synthesis.html)

+ [Structure-Aware Human-Action Generation](https://arxiv.org/pdf/2007.01971) (ECCV 2020)  
  [![Star](https://img.shields.io/github/stars/PingYu-iris/SA-GCN.svg?style=social&label=Star)](https://github.com/PingYu-iris/SA-GCN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2007.01971)

+ [Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single Sample](https://arxiv.org/pdf/2006.12226) (NeurIPS 2020)  
  [![Star](https://img.shields.io/github/stars/shirgur/hp-vae-gan.svg?style=social&label=Star)](https://github.com/shirgur/hp-vae-gan)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2006.12226)

+ [Latent Video Transformer](https://arxiv.org/pdf/2006.10704)  
  [![Star](https://img.shields.io/github/stars/rakhimovv/lvt.svg?style=social&label=Star)](https://github.com/rakhimovv/lvt)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2006.10704)

+ [Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose](https://arxiv.org/pdf/2002.10137)  
  [![Star](https://img.shields.io/github/stars/yiranran/Audio-driven-TalkingFace-HeadPose.svg?style=social&label=Star)](https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2002.10137)

+ [Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and Temporal-Consistency Video Prediction](https://arxiv.org/pdf/2002.09905) (CVPR 2020)  
  [![Star](https://img.shields.io/github/stars/Bei-Jin/STMFANet.svg?style=social&label=Star)](https://github.com/Bei-Jin/STMFANet)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2002.09905)

+ [Stochastic Latent Residual Video Prediction](https://arxiv.org/pdf/2002.09219) (ICML 2020)  
  [![Star](https://img.shields.io/github/stars/edouardelasalles/srvp.svg?style=social&label=Star)](https://github.com/edouardelasalles/srvp)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2002.09219)

+ [G3AN: Disentangling Appearance and Motion for Video Generation](https://arxiv.org/pdf/1912.05523) (CVPR 2020)  
  [![Star](https://img.shields.io/github/stars/wyhsirius/g3an-project.svg?style=social&label=Star)](https://github.com/wyhsirius/g3an-project)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1912.05523)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://wyhsirius.github.io/G3AN/)

+ [Scaling Autoregressive Video Models](https://arxiv.org/pdf/1906.02634) (ICLR 2020)  
  [![Star](https://img.shields.io/github/stars/rakhimovv/lvt.svg?style=social&label=Star)](https://github.com/rakhimovv/lvt)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1906.02634)

+ [VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation](https://arxiv.org/pdf/1903.04480) (ICLR 2020)
  [![Star](https://img.shields.io/github/stars/tensorflow/tensor2tensor.svg?style=social&label=Star)](https://github.com/tensorflow/tensor2tensor)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1903.04480)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://lucassheng.github.io/publication/pan-video-2019/)

## 2019

+ [Music-oriented Dance Video Synthesis with Pose Perceptual Loss](https://arxiv.org/pdf/2002.09219)  
  [![Star](https://img.shields.io/github/stars/xrenaa/Music-Dance-Video-Synthesis.svg?style=social&label=Star)](https://github.com/xrenaa/Music-Dance-Video-Synthesis)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2002.09219)

+ [DwNet: Dense warp-based network for pose-guided human video generation](https://arxiv.org/pdf/1910.09139)  
  [![Star](https://img.shields.io/github/stars/ubc-vision/DwNet.svg?style=social&label=Star)](https://github.com/ubc-vision/DwNet)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1910.09139)

+ [Order Matters: Shuffling Sequence Generation for Video Prediction](https://arxiv.org/pdf/1907.08845)  
  [![Star](https://img.shields.io/github/stars/andrewjywang/SEENet.svg?style=social&label=Star)](https://github.com/andrewjywang/SEENet)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1907.08845)

+ [Adversarial Video Generation on Complex Datasets](https://arxiv.org/pdf/1907.06571)  
  [![Star](https://img.shields.io/github/stars/Harrypotterrrr/DVD-GAN.svg?style=social&label=Star)](https://github.com/Harrypotterrrr/DVD-GAN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1907.06571)

+ [From Here to There: Video Inbetweening Using Direct 3D Convolutions](https://arxiv.org/pdf/1905.10240)  
  [![Star](https://img.shields.io/github/stars/xih108/Video_Completion.svg?style=social&label=Star)](https://github.com/xih108/Video_Completion)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1905.10240)

+ [Improved Conditional VRNNs for Video Prediction](https://arxiv.org/pdf/1904.12165) (ICCV 2019)  
  [![Star](https://img.shields.io/github/stars/facebookresearch/improved_vrnn.svg?style=social&label=Star)](https://github.com/facebookresearch/improved_vrnn)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1904.12165)

+ [Sliced Wasserstein Generative Models](https://arxiv.org/pdf/1706.02631) (CVPR 2019)  
  [![Star](https://img.shields.io/github/stars/musikisomorphie/swd.svg?style=social&label=Star)](https://github.com/musikisomorphie/swd)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1706.02631)

+ [Point-to-Point Video Generation](https://arxiv.org/pdf/1904.02912) (ICCV 2019)  
  [![Star](https://img.shields.io/github/stars/yccyenchicheng/p2pvg.svg?style=social&label=Star)](https://github.com/yccyenchicheng/p2pvg)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1904.02912)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://zswang666.github.io/P2PVG-Project-Page/)

+ [High Frame Rate Video Reconstruction based on an Event Camera](https://arxiv.org/pdf/1903.06531)  
  [![Star](https://img.shields.io/github/stars/panpanfei/Bringing-a-Blurry-Frame-Alive-at-High-Frame-Rate-with-an-Event-Camera.svg?style=social&label=Star)](https://github.com/panpanfei/Bringing-a-Blurry-Frame-Alive-at-High-Frame-Rate-with-an-Event-Camera)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1903.06531)

+ [Video Generation from Single Semantic Label Map](https://arxiv.org/pdf/1903.04480) (CVPR 2019)  
  [![Star](https://img.shields.io/github/stars/junting/seg2vid.svg?style=social&label=Star)](https://github.com/junting/seg2vid)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1903.04480)

+ [Learning to navigate image manifolds induced by generative adversarial networks for unsupervised video generation](https://arxiv.org/pdf/1901.11384)  
  [![Star](https://img.shields.io/github/stars/junting/seg2vid.svg?style=social&label=Star)](https://github.com/belaalb/frameGAN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1901.11384)

+ [Animating Arbitrary Objects via Deep Motion Transfer](https://arxiv.org/pdf/1812.08861) (CVPR 2019)  
  [![Star](https://img.shields.io/github/stars/AliaksandrSiarohin/monkey-net.svg?style=social&label=Star)](https://github.com/AliaksandrSiarohin/monkey-net)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1812.08861)

+ [StoryGAN: A Sequential Conditional GAN for Story Visualization](https://arxiv.org/pdf/1812.02784) (CVPR 2019)  
  [![Star](https://img.shields.io/github/stars/yitong91/StoryGAN.svg?style=social&label=Star)](https://github.com/yitong91/StoryGAN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1812.02784)

+ [Stochastic Adversarial Video Prediction](https://arxiv.org/pdf/1804.01523) (ICLR 2019)  
  [![Star](https://img.shields.io/github/stars/alexlee-gk/video_prediction.svg?style=social&label=Star)](https://github.com/alexlee-gk/video_prediction)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1804.01523)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://video-prediction.github.io/video_prediction/)

## 2018

+ [TwoStreamVAN: Improving Motion Modeling in Video Generation](https://arxiv.org/pdf/1812.01037)  
  [![Star](https://img.shields.io/github/stars/sunxm2357/TwoStreamVAN.svg?style=social&label=Star)](https://github.com/sunxm2357/TwoStreamVAN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1812.01037)

+ [Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation](https://arxiv.org/pdf/1811.09393)  
  [![Star](https://img.shields.io/github/stars/thunil/TecoGAN.svg?style=social&label=Star)](https://github.com/thunil/TecoGAN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1811.09393)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://ge.in.tum.de/wp-content/uploads/2020/05/ClickMe.html)

+ [Towards High Resolution Video Generation with Progressive Growing of Sliced Wasserstein GANs](https://arxiv.org/pdf/1810.02419)  
  [![Star](https://img.shields.io/github/stars/musikisomorphie/swd.svg?style=social&label=Star)](https://github.com/musikisomorphie/swd)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1810.02419)

+ [Everybody Dance Now](https://arxiv.org/pdf/1808.07371) (ICCV 2019)  
  [![Star](https://img.shields.io/github/stars/carolineec/EverybodyDanceNow.svg?style=social&label=Star)](https://github.com/carolineec/EverybodyDanceNow)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1808.07371)

+ [Learning to Forecast and Refine Residual Motion for Image-to-Video Generation](https://arxiv.org/pdf/1807.09951) (ECCV 2018)  
  [![Star](https://img.shields.io/github/stars/garyzhao/FRGAN.svg?style=social&label=Star)](https://github.com/garyzhao/FRGAN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1807.09951)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://garyzhao.github.io/archives/eccv18_frgan_poster.pdf)

+ [Talking Face Generation by Conditional Recurrent Adversarial Network](https://arxiv.org/pdf/1804.04786)  
  [![Star](https://img.shields.io/github/stars/susanqq/Talking_Face_Generation.svg?style=social&label=Star)](https://github.com/susanqq/Talking_Face_Generation)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1804.04786)

+ [Probabilistic Video Generation using Holistic Attribute Control](https://arxiv.org/pdf/1803.08085) (ECCV 2018)  
  [![Star](https://img.shields.io/github/stars/yccyenchicheng/pytorch-VideoVAE.svg?style=social&label=Star)](https://github.com/yccyenchicheng/pytorch-VideoVAE)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1803.08085)

+ [Stochastic Video Generation with a Learned Prior](https://arxiv.org/pdf/1802.07687) (ICML 2018)  
  [![Star](https://img.shields.io/github/stars/edenton/svg.svg?style=social&label=Star)](https://github.com/edenton/svg)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1802.07687)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://holmdk.github.io/2020/01/22/stochastic_vid.html)

+ [Stochastic Video Generation with a Learned Prior](https://arxiv.org/pdf/1802.07687) (ICML 2018)  
  [![Star](https://img.shields.io/github/stars/edenton/svg.svg?style=social&label=Star)](https://github.com/edenton/svg)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1802.07687)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://holmdk.github.io/2020/01/22/stochastic_vid.html)

+ [Stochastic Variational Video Prediction](https://arxiv.org/pdf/1710.11252) (ICLR 2018)  
  [![Star](https://img.shields.io/github/stars/RoboTurk-Platform/roboturk_real_dataset.svg?style=social&label=Star)](https://github.com/RoboTurk-Platform/roboturk_real_dataset)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1710.11252)

+ [Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture](https://arxiv.org/pdf/1711.09618) (AAAI 2018)  
  [![Star](https://img.shields.io/github/stars/mil-tokyo/FTGAN.svg?style=social&label=Star)](https://github.com/mil-tokyo/FTGAN)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1711.09618)

+ [MoCoGAN: Decomposing Motion and Content for Video Generation](https://arxiv.org/pdf/1707.04993) (CVPR 2018)  
  [![Star](https://img.shields.io/github/stars/sergeytulyakov/mocogan.svg?style=social&label=Star)](https://github.com/sergeytulyakov/mocogan)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1707.04993)

## 2017

+ [Improving Video Generation for Multi-functional Applications](https://arxiv.org/pdf/1711.11453)  
  [![Star](https://img.shields.io/github/stars/bernhard2202/improved-video-gan.svg?style=social&label=Star)](https://github.com/bernhard2202/improved-video-gan)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1711.11453)

+ [Attentive Semantic Video Generation using Captions](https://arxiv.org/pdf/1708.05980) (ICCV 2017)  
  [![Star](https://img.shields.io/github/stars/Singularity42/cap2vid.svg?style=social&label=Star)](https://github.com/Singularity42/cap2vid)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1708.05980)

+ [Temporal Generative Adversarial Nets with Singular Value Clipping](https://arxiv.org/pdf/1611.06624) (ICCV 2017)  
  [![Star](https://img.shields.io/github/stars/universome/stylegan-v.svg?style=social&label=Star)](https://github.com/universome/stylegan-v)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1611.06624)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://pfnet-research.github.io/tgan/)


## 2016

+ [Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive Architectures](https://arxiv.org/pdf/1611.10314)  
  [![Star](https://img.shields.io/github/stars/Singularity42/Sync-DRAW.svg?style=social&label=Star)](https://github.com/Singularity42/Sync-DRAW)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1611.10314)

+ [Unsupervised Learning for Physical Interaction through Video Prediction](https://arxiv.org/pdf/1605.07157)  
  [![Star](https://img.shields.io/github/stars/Xiaohui9607/physical_interaction_video_prediction_pytorch.svg?style=social&label=Star)](https://github.com/Xiaohui9607/physical_interaction_video_prediction_pytorch)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/1605.07157)

<!--
  // Entry template

+ [TITLE](LINK) (CONFERENCE)  
  [![Star](https://img.shields.io/github/stars/XXX/YYY.svg?style=social&label=Star)](GITHUB)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](ARXIV)
  [![Website](https://img.shields.io/badge/Website-9cf)](WEBSITE)

-->
