
# video-generation-survey

## Survey Papers

+ [ Video Frame Interpolation: A Comprehensive Survey ]( https://dl.acm.org/doi/10.1145/3556544 )
  
+ [Diffusion Models: A Comprehensive Survey of Methods and Applications](https://arxiv.org/abs/2209.00796)  
  [![Star](https://img.shields.io/github/stars/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.svg?style=social&label=Star)](https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2209.00796)

+ [Diffusion Models in Vision: A Survey](https://arxiv.org/pdf/2209.04747)  (IEEE TPAMI 2023)  
  [![Star](https://img.shields.io/github/stars/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey.svg?style=social&label=Star)](https://github.com/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2209.04747)

## Datasets

+ [ CelebV-Text: A Large-Scale Facial Text-Video Dataset ]( https://arxiv.org/abs/2303.14717 )   
  [![Star](https://img.shields.io/github/stars/CelebV-Text/CelebV-Text.svg?style=social&label=Star)]( https://github.com/CelebV-Text/CelebV-Text ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2303.14717 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://celebv-text.github.io/ )

+ [CelebV-HQ: A Large-Scale Video Facial Attributes Dataset](https://arxiv.org/abs/2207.12393)  
  [![Star](https://img.shields.io/github/stars/celebv-hq/celebv-hq.svg?style=social&label=Star)](https://github.com/celebv-hq/celebv-hq) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2207.12393) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://celebv-hq.github.io/)

## 2023


+ [VIDM: Video Implicit Diffusion Models](https://arxiv.org/abs/2212.00235) (AAAI 2023)  
  [![Star](https://img.shields.io/github/stars/MKFMIKU/VIDM.svg?style=social&label=Star)](https://github.com/MKFMIKU/VIDM) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.00235) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://kfmei.page/vidm/)

+ [Mm-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation](https://arxiv.org/abs/2212.09478) (CVPR 2023)   
  [![Star](https://img.shields.io/github/stars/researchmm/MM-Diffusion.svg?style=social&label=Star)](https://github.com/researchmm/MM-Diffusion) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2212.09478)

+ [Video Probabilistic Diffusion Models in Projected Latent Space](https://arxiv.org/abs/2302.07685) (CVPR 2023)   
  [![Star](https://img.shields.io/github/stars/sihyun-yu/PVDM.svg?style=social&label=Star)](https://github.com/sihyun-yu/PVDM) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2302.07685) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sihyun.me/PVDM/)

+ [VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation](https://arxiv.org/abs/2303.08320) (CVPR 2023)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.08320) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)

+ [Conditional Image-to-Video Generation with Latent Flow Diffusion Models](https://arxiv.org/abs/2303.13744) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/nihaomiao/CVPR23_LFDM.svg?style=social&label=Star)](https://github.com/nihaomiao/CVPR23_LFDM) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.13744)

+ [Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos](https://arxiv.org/abs/2303.16897) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/sukun1045/video-physics-sound-diffusion.svg?style=social&label=Star)](https://github.com/sukun1045/video-physics-sound-diffusion) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.16897) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sukun1045.github.io/video-physics-sound-diffusion/)

+ [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2304.08818) (CVPR 2023)  
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2304.08818) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)

+ [3D Cinemagraphy from a Single Image](https://arxiv.org/abs/2303.05724) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/xingyi-li/3d-cinemagraphy.svg?style=social&label=Star)](https://github.com/xingyi-li/3d-cinemagraphy)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.05724) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://xingyi-li.github.io/3d-cinemagraphy/)

+ [MOSO: Decomposing MOtion, Scene and Object for Video Prediction](https://arxiv.org/abs/2303.03684) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/iva-mzsun/MOSO.svg?style=social&label=Star)](https://github.com/iva-mzsun/MOSO)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.03684) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://iva-mzsun.github.io/MOSO)

+ [SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation](https://arxiv.org/abs/2211.12194) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/OpenTalker/SadTalker.svg?style=social&label=Star)](https://github.com/OpenTalker/SadTalker)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2211.12194)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sadtalker.github.io/)

+ [Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers](https://arxiv.org/abs/2303.11251) (CVPR 2023)  
  [![Star](https://img.shields.io/github/stars/Ugness/MeBT.svg?style=social&label=Star)](https://github.com/Ugness/MeBT)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2303.11251)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sites.google.com/view/mebt-cvpr2023)

+ [ Learning Universal Policies via Text-Guided Video Generation ]( https://arxiv.org/abs/2302.00111 )   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2302.00111 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://universal-policy.github.io/unipi/ )

+ [ Tell me what happened: Unifying text-guided video completion via multimodal masked video generation ]( https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Tell_Me_What_Happened_Unifying_Text-Guided_Video_Completion_via_Multimodal_CVPR_2023_paper.pdf ) (CVPR 2023)   
  [![Star](https://img.shields.io/github/stars/tsujuifu/pytorch_tvc.svg?style=social&label=Star)]( https://github.com/tsujuifu/pytorch_tvc ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2211.12824 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://tvc-mmvg.github.io/ )

+ [ Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation ]( https://arxiv.org/pdf/2304.08477.pdf )    
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2304.08477 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://latent-shift.github.io/ )

+ [ Video-p2p: Video editing with cross-attention control ]( https://arxiv.org/abs/2303.04761 )    
  [![Star](https://img.shields.io/github/stars/ShaoTengLiu/Video-P2P.svg?style=social&label=Star)]( https://github.com/ShaoTengLiu/Video-P2P ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2303.04761 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://video-p2p.github.io/ )

+ [ MAGVIT: Masked Generative Video Transformer ]( https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.pdf ) (CVPR 2023)   
  [![Star](https://img.shields.io/github/stars/google-research/magvit.svg?style=social&label=Star)]( https://github.com/google-research/magvit ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2212.05199 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://magvit.cs.cmu.edu/ )

+ [ Text2video-zero: Text-to-image diffusion models are zero-shot video generators ]( https://arxiv.org/abs/2303.13439 )    
  [![Star](https://img.shields.io/github/stars/Picsart-AI-Research/Text2Video-Zero.svg?style=social&label=Star)]( https://github.com/Picsart-AI-Research/Text2Video-Zero ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2303.13439 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://text2video-zero.github.io/ )

+ [ Pix2video: Video editing using image diffusion ]( https://arxiv.org/pdf/2303.12688.pdf )    
  [![Star](https://img.shields.io/github/stars/G-U-N/Pix2Video.pytorch.svg?style=social&label=Star)]( https://github.com/G-U-N/Pix2Video.pytorch ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( G-U-N/Pix2Video.pytorch ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://duyguceylan.github.io/pix2video.github.io/ )

+ [ Dreamix: Video diffusion models are general video editors ]( https://arxiv.org/abs/2302.01329 )   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2302.01329 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://dreamix-video-editing.github.io/ )

+ [ VarietySound: Timbre-controllable video to sound generation via unsupervised information disentanglement ]( https://arxiv.org/abs/2211.10666 )   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2211.10666 ) 

+ [ Text-to-4d dynamic scene generation ]( https://arxiv.org/abs/2301.11280 )   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2301.11280 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://make-a-video3d.github.io/ )

+ [ Fatezero: Fusing attentions for zero-shot text-based video editing ]( https://arxiv.org/abs/2303.09535 )   
  [![Star](https://img.shields.io/github/stars/ChenyangQiQi/FateZero.svg?style=social&label=Star)]( https://github.com/ChenyangQiQi/FateZero ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2303.09535 ) 

+ [ Structure and content-guided video synthesis with diffusion models ]( https://arxiv.org/abs/2302.03011 )   
  [![Star](https://img.shields.io/github/stars/kyegomez/StarlightVision.svg?style=social&label=Star)]( https://github.com/kyegomez/StarlightVision ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2302.03011 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://taldatech.github.io/ddlp-web )

+ [ GD-VDM: Generated Depth for better Diffusion-based Video Generation ]( https://arxiv.org/abs/2306.11173 )   
  [![Star](https://img.shields.io/github/stars/lapid92/gd-vdm.svg?style=social&label=Star)]( https://github.com/lapid92/gd-vdm ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2306.11173 ) 

+ [ DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles ]( https://arxiv.org/abs/2306.05957 )   
  [![Star](https://img.shields.io/github/stars/taldatech/ddlp.svg?style=social&label=Star)]( https://github.com/taldatech/ddlp ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2306.05957 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://taldatech.github.io/ddlp-web )

+ [ Video Diffusion Models with Local-Global Context Guidance ]( https://arxiv.org/abs/2306.02562 )   
  [![Star](https://img.shields.io/github/stars/exisas/lgc-vd.svg?style=social&label=Star)]( https://github.com/exisas/lgc-vd ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2306.02562 ) 

+ [ Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising ]( https://arxiv.org/abs/2305.18264 )   
  [![Star](https://img.shields.io/github/stars/G-U-N/Gen-L-Video.svg?style=social&label=Star)]( https://github.com/G-U-N/Gen-L-Video ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2305.18264 ) 

+ [ Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation ]( https://arxiv.org/abs/2305.14330 )   
  [![Star](https://img.shields.io/github/stars/ku-cvlab/direct2v.svg?style=social&label=Star)]( https://github.com/ku-cvlab/direct2v ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2305.14330 )

+ [ ControlVideo: Training-free Controllable Text-to-Video Generation ]( https://arxiv.org/abs/2305.13077 )   
  [![Star](https://img.shields.io/github/stars/ybybzhang/controlvideo.svg?style=social&label=Star)]( https://github.com/ybybzhang/controlvideo ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2305.13077 )

+ [ Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models ]( https://arxiv.org/pdf/2305.13840 )   
  [![Star](https://img.shields.io/github/stars/Weifeng-Chen/control-a-video.svg?style=social&label=Star)]( https://github.com/Weifeng-Chen/control-a-video ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2305.13840 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://controlavideo.github.io/ )

+ [ VDT: An Empirical Study on Video Diffusion with Transformers ]( https://arxiv.org/abs/2305.13311 )   
  [![Star](https://img.shields.io/github/stars/rerv/vdt.svg?style=social&label=Star)]( https://github.com/rerv/vdt ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2305.13311 )

+ [ Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models ]( https://arxiv.org/abs/2305.05845 )   
  [![Star](https://img.shields.io/github/stars/rohandkn/skribble2vid.svg?style=social&label=Star)]( https://github.com/rohandkn/skribble2vid ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2305.05845 ) 

+ [ Text2Performer: Text-Driven Human Video Generation ]( https://arxiv.org/abs/2304.08483 )   
  [![Star](https://img.shields.io/github/stars/yumingj/text2performer.svg?style=social&label=Star)]( https://github.com/yumingj/text2performer ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2304.08483 ) 

+ [ Generative Disco: Text-to-Video Generation for Music Visualization ]( https://arxiv.org/abs/2304.08551 )   
  [![Star](https://img.shields.io/github/stars/hellovivian/generative-disco.svg?style=social&label=Star)]( https://github.com/hellovivian/generative-disco ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2304.08551 )

+ [ MoStGAN-V: Video Generation with Temporal Motion Styles ]( https://arxiv.org/abs/2304.02777 )   
  [![Star](https://img.shields.io/github/stars/xiaoqian-shen/mostgan-v.svg?style=social&label=Star)]( https://github.com/xiaoqian-shen/mostgan-v ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2304.02777 )

+ [ Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos ]( https://arxiv.org/abs/2304.01186 )   
  [![Star](https://img.shields.io/github/stars/mayuelala/followyourpose.svg?style=social&label=Star)]( https://github.com/mayuelala/followyourpose ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2304.01186 )

+ [ Sounding Video Generator: A Unified Framework for Text-guided Sounding Video Generation ]( https://arxiv.org/abs/2303.16541 )   
  [![Star](https://img.shields.io/github/stars/jwliu-cc/svg.svg?style=social&label=Star)]( https://github.com/jwliu-cc/svg ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2303.16541 )

+ [ MotionVideoGAN: A Novel Video Generator Based on the Motion Space Learned from Image Pairs ]( https://arxiv.org/abs/2303.02906 )   
  [![Star](https://img.shields.io/github/stars/bbzhu-jy16/motionvideogan.svg?style=social&label=Star)]( https://github.com/bbzhu-jy16/motionvideogan ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2303.02906 )

+ [ Consistency Models ]( https://arxiv.org/abs/2303.01469 )   
  [![Star](https://img.shields.io/github/stars/openai/consistency_models.svg?style=social&label=Star)]( https://github.com/openai/consistency_models ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2303.01469 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://jepsen.io/consistency )

+ [ MotionVideoGAN: A Novel Video Generator Based on the Motion Space Learned from Image Pairs ]( https://arxiv.org/abs/2303.02906 )   
  [![Star](https://img.shields.io/github/stars/bbzhu-jy16/motionvideogan.svg?style=social&label=Star)]( https://github.com/bbzhu-jy16/motionvideogan ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2303.02906 )

+ [ Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction ]( https://arxiv.org/abs/2301.08951 )   
  [![Star](https://img.shields.io/github/stars/FudanVI/compositional-scene-representation-toolbox.svg?style=social&label=Star)]( https://github.com/FudanVI/compositional-scene-representation-toolbox ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2301.08951 )

+ [ VideoComposer: Compositional Video Synthesis with Motion Controllability ]( https://arxiv.org/pdf/2306.02018 )   
  [![Star](https://img.shields.io/github/stars/damo-vilab/videocomposer.svg?style=social&label=Star)]( https://github.com/damo-vilab/videocomposer ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2306.02018 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://videocomposer.github.io/ )

## 2022


+ [Video Diffusion Models](https://arxiv.org/abs/2204.03458) (NeurIPS 2022)   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2204.03458) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://video-diffusion.github.io/)

+ [McVd: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation](https://arxiv.org/abs/2205.09853) (NeurIPS 2022)   
  [![Star](https://img.shields.io/github/stars/Tobi-r9/RaMViD.svg?style=social&label=Star)](https://github.com/voletiv/mcvd-pytorch)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2205.09853) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://mask-cond-video-diffusion.github.io)

+ [Diffusion Models for Video Prediction and Infilling](https://arxiv.org/abs/2206.07696) (TMLR 2022)   
  [![Star](https://img.shields.io/github/stars/Tobi-r9/RaMViD.svg?style=social&label=Star)](https://github.com/Tobi-r9/RaMViD) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2206.07696) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://sites.google.com/view/video-diffusion-prediction)

+ [Make-A-Video: Text-to-Video Generation without Text-Video Data](https://openreview.net/forum?id=nJfylDvgzlq) (ICLR 2023)   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://openreview.net/forum?id=nJfylDvgzlq) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://makeavideo.studio)

+ [Depth-Aware Generative Adversarial Network for Talking Head Video Generation](https://arxiv.org/abs/2203.06605) (CVPR 2022)   
  [![Star](https://img.shields.io/github/stars/harlanhong/CVPR2022-DaGAN.svg?style=social&label=Star)](https://github.com/harlanhong/CVPR2022-DaGAN) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2203.06605)

+ [Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning](https://arxiv.org/abs/2203.02573) (CVPR 2022)   
  [![Star](https://img.shields.io/github/stars/snap-research/MMVID.svg?style=social&label=Star)](https://github.com/snap-research/MMVID) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2203.02573) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://snap-research.github.io/MMVID/)

+ [Playable Environments: Video Manipulation in Space and Time](https://arxiv.org/abs/2203.01914) (CVPR 2022)   
  [![Star](https://img.shields.io/github/stars/willi-menapace/PlayableEnvironments.svg?style=social&label=Star)](https://github.com/willi-menapace/PlayableEnvironments) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2203.01914) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://willi-menapace.github.io/playable-environments-website/)

+ [Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis](https://arxiv.org/abs/2207.05049) (ECCV 2022)   
  [![Star](https://img.shields.io/github/stars/fast-vid2vid/fast-vid2vid.svg?style=social&label=Star)](https://github.com/fast-vid2vid/fast-vid2vid) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2207.05049) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://fast-vid2vid.github.io/)

+ [TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts](https://arxiv.org/abs/2207.01696) (ECCV 2022)   
  [![Star](https://img.shields.io/github/stars/EricGuo5513/TM2T.svg?style=social&label=Star)](https://github.com/EricGuo5513/TM2T) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2207.01696) 
  [![Website](https://img.shields.io/badge/Website-9cf)](https://ericguo5513.github.io/TM2T/)

+ [ Imagen Video: High Definition Video Generation with Diffusion Models ]( https://arxiv.org/abs/2210.02303 )   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2210.02303 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://video-diffusion.github.io/ )


+ [ Phenaki: Variable length video generation from open domain textual description ]( https://arxiv.org/abs/2210.02399 )   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2210.02399 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://sites.research.google/phenaki/ )
  [alt-website](https://phenaki.video/)

  Code (unofficial?): [![Star](https://img.shields.io/github/stars/lucidrains/phenaki-pytorch.svg?style=social&label=Star)]( https://github.com/XX/YY ) 


+ [ Cogvideo: Large-scale pretraining for text-to-video generation via transformers ]( https://arxiv.org/abs/2205.15868 )   
  [![Star](https://img.shields.io/github/stars/THUDM/CogVideo.svg?style=social&label=Star)]( https://github.com/THUDM/CogVideo ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( (Conference) ) 

+ [ Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation ]( https://arxiv.org/abs/2212.11565 )   
  [![Star](https://img.shields.io/github/stars/bbzhu-jy16/motionvideogan.svg?style=social&label=Star)]( https://github.com/showlab/Tune-A-Video ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2212.11565 )

+ [ Towards Smooth Video Composition ]( https://arxiv.org/abs/2212.07413 )   
  [![Star](https://img.shields.io/github/stars/genforce/StyleSV.svg?style=social&label=Star)]( https://github.com/genforce/StyleSV ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2212.07413 )

+ [ Latent Video Diffusion Models for High-Fidelity Long Video Generation ]( https://arxiv.org/abs/2211.13221 )   
  [![Star](https://img.shields.io/github/stars/yingqinghe/lvdm.svg?style=social&label=Star)]( https://github.com/yingqinghe/lvdm ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2211.13221 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://yingqinghe.github.io/LVDM/ )

+ [ SinFusion: Training Diffusion Models on a Single Image or Video ]( https://arxiv.org/abs/2211.11743 )   
  [![Star](https://img.shields.io/github/stars/yanivnik/sinfusion-code.svg?style=social&label=Star)]( https://github.com/yanivnik/sinfusion-code ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2211.11743 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://yanivnik.github.io/sinfusion/static/video_comparisons.html )

+ [ INR-V: A Continuous Representation Space for Video-based Generative Tasks ]( https://arxiv.org/abs/2210.16579 )   
  [![Star](https://img.shields.io/github/stars/bipashasen/INRV.svg?style=social&label=Star)]( https://github.com/bipashasen/INRV ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2210.16579 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://skymanaditya1.github.io/INRV/ )

+ [ Computational Choreography using Human Motion Synthesis ]( https://arxiv.org/abs/2210.04366 )   
  [![Star](https://img.shields.io/github/stars/patrickrperrine/comp-choreo.svg?style=social&label=Star)]( https://github.com/patrickrperrine/comp-choreo ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2210.04366 )

+ [ Phenaki: Variable Length Video Generation From Open Domain Textual Description ]( https://arxiv.org/abs/2210.02399 )   
  [![Star](https://img.shields.io/github/stars/lucidrains/phenaki-pytorch.svg?style=social&label=Star)]( https://github.com/lucidrains/phenaki-pytorch ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2210.02399 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://phenaki.video/ )

+ [ Temporally Consistent Transformers for Video Generation ]( https://arxiv.org/abs/2210.02396 )   
  [![Star](https://img.shields.io/github/stars/wilson1yan/teco.svg?style=social&label=Star)]( https://github.com/wilson1yan/teco ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2210.02396 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://wilson1yan.github.io/teco/ )

+ [ StyleFaceV: Face Video Generation via Decomposing and Recomposing Pretrained StyleGAN3 ]( https://arxiv.org/abs/2208.07862 )   
  [![Star](https://img.shields.io/github/stars/arthur-qiu/stylefacev.svg?style=social&label=Star)]( https://github.com/arthur-qiu/stylefacev ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2208.07862 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( http://haonanqiu.com/projects/StyleFaceV.html )

+ [ NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis ]( https://arxiv.org/abs/2207.09814 )   
  [![Star](https://img.shields.io/github/stars/arthur-qiu/stylefacev.svg?style=social&label=Star)]( https://github.com/microsoft/nuwa ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2207.09814 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://nuwa-infinity.microsoft.com/ )

+ [ 3D-Aware Video Generation ]( https://arxiv.org/abs/2206.14797 )   
  [![Star](https://img.shields.io/github/stars/sherwinbahmani/3dvideogenerati.svg?style=social&label=Star)]( https://github.com/sherwinbahmani/3dvideogenerati ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2206.14797 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://sherwinbahmani.github.io/3dvidgen/ )

+ [ Patch-based Object-centric Transformers for Efficient Video Generation ]( https://arxiv.org/abs/2206.04003 )   
  [![Star](https://img.shields.io/github/stars/wilson1yan/povt.svg?style=social&label=Star)]( https://github.com/wilson1yan/povt ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2206.04003 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://sites.google.com/view/povt-public )

+ [ Generating Long Videos of Dynamic Scenes ]( https://arxiv.org/abs/2206.03429 )   
  [![Star](https://img.shields.io/github/stars/nvlabs/long-video-gan.svg?style=social&label=Star)]( https://github.com/nvlabs/long-video-gan ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2206.03429 )
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://www.timothybrooks.com/tech/long-video-gan/ )

+ [ D'ARTAGNAN: Counterfactual Video Generation ]( https://arxiv.org/abs/2206.01651 )   
  [![Star](https://img.shields.io/github/stars/nvlabs/long-video-gan.svg?style=social&label=Star)]( https://github.com/hreynaud/dartagnan ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2206.01651 )

+ [ CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers ]( https://arxiv.org/abs/2205.15868 )   
  [![Star](https://img.shields.io/github/stars/nvlabs/long-video-gan.svg?style=social&label=Star)]( https://github.com/thudm/cogvideo ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2205.15868 )

## 2021



+ [Click to Move: Controlling Video Generation with Sparse Motion](https://arxiv.org/abs/2108.08815) (ICCV 2021)   
  [![Star](https://img.shields.io/github/stars/PierfrancescoArdino/C2M.svg?style=social&label=Star)](https://github.com/PierfrancescoArdino/C2M) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2108.08815) 


+ [VideoGPT: Video Generation using VQ-VAE and Transformers](https://arxiv.org/pdf/2104.10157v2.pdf)   
  [![Star](https://img.shields.io/github/stars/wilson1yan/VideoGPT.svg?style=social&label=Star)](https://github.com/wilson1yan/VideoGPT) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/pdf/2104.10157v2.pdf)
  [![Website](https://img.shields.io/badge/Website-9cf)](https://wilson1yan.github.io/videogpt/index.html)


+ [Latent Neural Differential Equations for Video Generation](https://arxiv.org/pdf/2011.03864v3.pdf)   
  [![Star](https://img.shields.io/github/stars/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation.svg?style=social&label=Star)](https://github.com/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/pdf/2011.03864v3.pdf)

+ [ Stochastic Image-to-Video Synthesis Using cINNs ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Dorkenwald_Stochastic_Image-to-Video_Synthesis_Using_cINNs_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![Star](https://img.shields.io/github/stars/CompVis/image2video-synthesis-using-cINNs.svg?style=social&label=Star)]( https://github.com/CompVis/image2video-synthesis-using-cINNs ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2105.04551 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://compvis.github.io/image2video-synthesis-using-cINNs/ )


+ [ Understanding Object Dynamics for Interactive Image-to-Video Synthesis ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Blattmann_Understanding_Object_Dynamics_for_Interactive_Image-to-Video_Synthesis_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![Star](https://img.shields.io/github/stars/CompVis/interactive-image2video-synthesis.svg?style=social&label=Star)]( https://github.com/CompVis/interactive-image2video-synthesis ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2106.11303 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://compvis.github.io/interactive-image2video-synthesis/ )

+ [ One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_One-Shot_Free-View_Neural_Talking-Head_Synthesis_for_Video_Conferencing_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![Star](https://img.shields.io/github/stars/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis.svg?style=social&label=Star)]( https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2011.15126 ) 

+ [ Flow Guided Transformable Bottleneck Networks for Motion Retargeting ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Flow_Guided_Transformable_Bottleneck_Networks_for_Motion_Retargeting_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2106.07771 ) 

+ [ Stable View Synthesis ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Riegler_Stable_View_Synthesis_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![Star](https://img.shields.io/github/stars/isl-org/StableViewSynthesis.svg?style=social&label=Star)]( https://github.com/isl-org/StableViewSynthesis ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2011.07233 ) 

+ [ Scene-Aware Generative Network for Human Motion Synthesis ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Scene-Aware_Generative_Network_for_Human_Motion_Synthesis_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2105.14804 ) 
  
+ [ Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Neural_Scene_Flow_Fields_for_Space-Time_View_Synthesis_of_Dynamic_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![Star](https://img.shields.io/github/stars/zhengqili/Neural-Scene-Flow-Fields.svg?style=social&label=Star)]( https://github.com/zhengqili/Neural-Scene-Flow-Fields ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2011.13084 ) 
  
+ [ Deep Animation Video Interpolation in the Wild ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Siyao_Deep_Animation_Video_Interpolation_in_the_Wild_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![Star](https://img.shields.io/github/stars/lisiyao21/AnimeInterp.svg?style=social&label=Star)]( https://github.com/lisiyao21/AnimeInterp ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2104.02495 ) 

+ [ High-Fidelity Neural Human Motion Transfer from Monocular Video ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Kappel_High-Fidelity_Neural_Human_Motion_Transfer_From_Monocular_Video_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![Star](https://img.shields.io/github/stars/MoritzKappel/HF-NHMT.svg?style=social&label=Star)]( https://github.com/MoritzKappel/HF-NHMT ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2012.10974 ) 

+ [ Learning Semantic-Aware Dynamics for Video Prediction ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Bei_Learning_Semantic-Aware_Dynamics_for_Video_Prediction_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2104.09762 ) 

+ [ Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Flow-Guided_One-Shot_Talking_Face_Generation_With_a_High-Resolution_Audio-Visual_Dataset_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![Star](https://img.shields.io/github/stars/MRzzm/HDTF.svg?style=social&label=Star)]( https://github.com/MRzzm/HDTF ) 

+ [ Layout-Guided Novel View Synthesis From a Single Indoor Panorama ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Layout-Guided_Novel_View_Synthesis_From_a_Single_Indoor_Panorama_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![Star](https://img.shields.io/github/stars/bluestyle97/PNVS.svg?style=social&label=Star)]( https://github.com/bluestyle97/PNVS ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2103.17022 ) 

+ [ Space-Time Neural Irradiance Fields for Free-Viewpoint Video ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Xian_Space-Time_Neural_Irradiance_Fields_for_Free-Viewpoint_Video_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2011.12950 ) 

+ [ GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_GeoSim_Realistic_Video_Simulation_via_Geometry-Aware_Composition_for_Self-Driving_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2101.06543 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://tmux.top/publication/geosim/ )

+ [ Animating Pictures With Eulerian Motion Fields ]( https://openaccess.thecvf.com/content/CVPR2021/papers/Holynski_Animating_Pictures_With_Eulerian_Motion_Fields_CVPR_2021_paper.pdf ) (CVPR 2021)   
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( https://arxiv.org/abs/2011.15128 ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( https://eulerian.cs.washington.edu/ )


<!--
  // Entry template
  
+ [ TITLE ]( LINK ) (CONFERENCE)   
  [![Star](https://img.shields.io/github/stars/XXX/YYY.svg?style=social&label=Star)]( GITHUB ) 
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)]( ARXIV ) 
  [![Website](https://img.shields.io/badge/Website-9cf)]( WEBSITE )
  
-->
