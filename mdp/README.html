<h1>Awesome Video Generation <a href="https://awesome.re"><img src="https://awesome.re/badge-flat.svg" alt="Awesome"></a></h1>
<h2>üìù Introduction</h2>
<p>A comprehensive collection of works on video generation/synthesis/prediction.</p>
<br>
<br>

<p align="center">
<img src="https://magvit.cs.cmu.edu/img/fp/00083_gen_sr.gif" width="240px"/>  
<img src="https://magvit.cs.cmu.edu/img/mt_ssv2/full_generation_0143SqueezingSomething_000000782_sr.gif" width="240px"/>
</p>

<p align="center">
<img src="https://modelscope.cn/api/v1/models/damo/text-to-video-synthesis/repo?Revision=master&FilePath=./samples/006_A_cat_eating_food_out_of_a_bowl,_in_style_of_van_Gogh._003.gif&View=true" width="240px"/>  
<img src="https://modelscope.cn/api/v1/models/damo/text-to-video-synthesis/repo?Revision=master&FilePath=./samples/040_Incredibly_detailed_science_fiction_scene_set_on_an_alien_planet,_view_of_a_marketplace._Pixel_art._003.gif&View=true" width="240px"/>
</p>

<p align="center">
<img src="https://kfmei.page/vidm/results/sky/vidm.gif" width="480px"/>  
</p>

<p align="center">
(Source: <a href="https://mask-cond-video-diffusion.github.io">MCVD</a>, <a href="https://modelscope.cn/models/damo/text-to-video-synthesis/summary">VideoFusion</a>, and <a href="https://kfmei.page/vidm/">VIDM</a>)
</p>

<h2>Contents</h2>
<ul>
<li><a href="#awesome-video-generation-">Awesome Video Generation </a><ul>
<li><a href="#-introduction">üìù Introduction</a></li>
<li><a href="#contents">Contents</a></li>
<li><a href="#survey_papers">‚ú®Survey_Papers</a></li>
<li><a href="#datasets">üåüDatasets</a></li>
<li><a href="#video-generation_subtopics">üöÄVideo-generation_subtopics</a></li>
<li><a href="#2023">2023</a></li>
<li><a href="#2022">2022</a></li>
<li><a href="#2021">2021</a></li>
<li><a href="#2020">2020</a></li>
<li><a href="#2019">2019</a></li>
<li><a href="#2018">2018</a></li>
<li><a href="#2017">2017</a></li>
<li><a href="#2016">2016</a></li>
</ul>
</li>
</ul>
<h2>‚ú®Survey_Papers</h2>
<ul>
<li><p><a href="https://dl.acm.org/doi/10.1145/3556544">Video Frame Interpolation: A Comprehensive Survey</a>  </p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.00796">Diffusion Models: A Comprehensive Survey of Methods and Applications</a><br><a href="https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy"><img src="https://img.shields.io/github/stars/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.00796"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2209.04747">Diffusion Models in Vision: A Survey</a> (IEEE TPAMI 2023)<br><a href="https://github.com/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey"><img src="https://img.shields.io/github/stars/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.04747"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2005.03201">What comprises a good talking-head video generation?: A Survey and Benchmark</a><br><a href="https://github.com/lelechen63/talking-head-generation-survey"><img src="https://img.shields.io/github/stars/lelechen63/talking-head-generation-survey.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2005.03201"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2004.05214">A Review on Deep Learning Techniques for Video Prediction</a> (2020)<br><a href="https://arxiv.org/abs/2004.05214"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
</ul>
<h2>üåüDatasets</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2307.06942">InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</a><br><a href="https://github.com/opengvlab/internvideo"><img src="https://img.shields.io/github/stars/opengvlab/internvideo.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2307.06942"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.14717">CelebV-Text: A Large-Scale Facial Text-Video Dataset</a><br><a href="https://github.com/CelebV-Text/CelebV-Text"><img src="https://img.shields.io/github/stars/CelebV-Text/CelebV-Text.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2303.14717"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://celebv-text.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.12393">CelebV-HQ: A Large-Scale Video Facial Attributes Dataset</a><br><a href="https://github.com/celebv-hq/celebv-hq"><img src="https://img.shields.io/github/stars/celebv-hq/celebv-hq.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.12393"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://celebv-hq.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1212.0402">UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</a><br><a href="https://github.com/wushidonguc/two-stream-action-recognition-keras"><img src="https://img.shields.io/github/stars/wushidonguc/two-stream-action-recognition-keras.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1212.0402"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.crcv.ucf.edu/data/UCF101.php"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1705.06950">The Kinetics Human Action Video Dataset</a><br><a href="https://github.com/deepmind/kinetics-i3d"><img src="https://img.shields.io/github/stars/deepmind/kinetics-i3d.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1705.06950"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.deepmind.com/open-source/kinetics"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/1334462">Recognizing human actions: a local SVM approach</a><br><a href="https://www.csc.kth.se/cvap/actions/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1808.01340">A Short Note about Kinetics-600</a><br><a href="https://github.com/rocksyne/kinetics-dataset-downloader"><img src="https://img.shields.io/github/stars/rocksyne/kinetics-dataset-downloader.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1808.01340"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.deepmind.com/open-source/kinetics"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2111.02114">LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs</a><br><a href="https://github.com/compvis/latent-diffusion"><img src="https://img.shields.io/github/stars/compvis/latent-diffusion.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.02114"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.00650">Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval</a> (ICCV 2021)<br><a href="https://github.com/m-bain/frozen-in-time"><img src="https://img.shields.io/github/stars/m-bain/frozen-in-time.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.00650"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1710.05268">Self-Supervised Visual Planning with Temporal Skip Connections</a><br><a href="https://arxiv.org/abs/1710.05268"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2008.08143">How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language</a> (CVPR 2021)<br><a href="https://github.com/how2sign/how2sign.github.io"><img src="https://img.shields.io/github/stars/how2sign/how2sign.github.io.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2008.08143"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://how2sign.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2204.02393">Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining</a> (ECCV 2022)<br><a href="https://github.com/metadriverse/aco"><img src="https://img.shields.io/github/stars/metadriverse/aco.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2204.02393"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://metadriverse.github.io/ACO/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1901.02212">FakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals</a><br><a href="https://arxiv.org/abs/1901.02212"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2008.04776">DTVNet: Dynamic Time-lapse Video Generation via Single Still Image</a> (ECCV 2020)<br><a href="https://github.com/zhangzjn/DTVNet"><img src="https://img.shields.io/github/stars/zhangzjn/DTVNet.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2008.04776"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2106.08285">Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy</a><br><a href="https://github.com/ChristophReich1996/Multi-StyleGAN"><img src="https://img.shields.io/github/stars/ChristophReich1996/Multi-StyleGAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.08285"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://christophreich1996.github.io/multi_stylegan/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.12734">DDH-QA: A Dynamic Digital Humans Quality Assessment Database</a><br><a href="https://github.com/zzc-1998/Point-cloud-quality-assessment"><img src="https://img.shields.io/github/stars/zzc-1998/Point-cloud-quality-assessment.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.12734"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.02978">Muscles in Action</a><br><a href="https://github.com/mchiquier/musclesinactionofficial"><img src="https://img.shields.io/github/stars/mchiquier/musclesinactionofficial.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.02978"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.13887">TPA-Net: Generate A Dataset for Text to Physics-based Animation</a><br><a href="https://arxiv.org/abs/2211.13887"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sites.google.com/view/tpa-net"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.12498">Touch and Go: Learning from Human-Collected Vision and Touch</a><br><a href="https://arxiv.org/abs/2211.12498"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://touch-and-go.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.00823">BVI-VFI: A Video Quality Database for Video Frame Interpolation</a><br><a href="https://github.com/danier97/BVI-VFI-database"><img src="https://img.shields.io/github/stars/danier97/BVI-VFI-database.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.00823"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.12694">Multi-modal Video Chapter Generation</a><br><a href="https://github.com/czt117/MVCG"><img src="https://img.shields.io/github/stars/czt117/MVCG.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.12694"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.12194">Merkel Podcast Corpus: A Multimodal Dataset Compiled from 16 Years of Angela Merkel&#39;s Weekly Video Podcasts</a> (LREC 2022)<br><a href="https://github.com/deeplsd/Merkel-Podcast-Corpus"><img src="https://img.shields.io/github/stars/deeplsd/Merkel-Podcast-Corpus.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.12194"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
</ul>
<h2>üöÄVideo-generation_subtopics</h2>
<ul>
<li><a href="https://github.com/zhiqic/Awesome-Video-Generation/blob/main/controllable_video_generation.md">Controllable Video generation</a><br>Text-to-video, image-to-video<br>The ‚Äúclassic‚Äù task: create a video scratch, i.e. starting from random noise. The generation process is sometimes given simple conditions, such as Text or image. Common goals include visual fidelity, temporal coherence, and logical plausibility.</li>
<li><a href="https://github.com/zhiqic/Awesome-Video-Generation/blob/main/video_prediction_interpolation.md">[Video prediction and Frame interpolation](https://github.com/zhiqic/Awesome-Video-Generation/blob/main/video_prediction.md</a>)<br>Video generation with (visual) constraints<br>Predict the next N frames following a sequence of input video frames, or predict N frames between the given start and final frames.
Could be viewed as a special case of video completion<br>Aimed at improving the motion smoothness of low frame rate videos, by inserting additional frames between existing video frames. Some works can ‚Äúinsert‚Äù frames after the input frames, so they technically can perform video prediction to some extent.</li>
<li><a href="https://github.com/zhiqic/Awesome-Video-Generation/blob/main/novel_view_synthesis.md">Novel view synthesis</a><br>These usually involve reconstructing a 3D scene from some observations (e.g. monocular video input, or static images), and then generating renderings of the scene from new perspectives.</li>
<li><a href="https://github.com/zhiqic/Awesome-Video-Generation/blob/main/human_motion_generation.md">Human motion generation</a><br>These are video generation tasks specifically geared to human (or humanoid) activities</li>
<li><a href="https://github.com/zhiqic/Awesome-Video-Generation/blob/main/talking_head_or_face_generation.md">Talking head or face generation</a><br>Talking head generation refers to the generation of animated video content that simulates a person&#39;s face and head movements while they are speaking</li>
<li><a href="https://github.com/zhiqic/Awesome-Video-Generation/blob/main/video-to-video.md">Video-to-video</a><br>These include enhancing the (textural) quality of videos, style transfer, motion transfer, Summarization, and various common video editing tasks (e.g. removal of a subject).</li>
</ul>
<h2>2023</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2307.09906">Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation</a> (ICCV 2023)<br><a href="https://github.com/harlanhong/iccv2023-mcnet"><img src="https://img.shields.io/github/stars/harlanhong/iccv2023-mcnet.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2307.09906"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://harlanhong.github.io/publications/mcnet.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.14135">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a><br><a href="https://github.com/dao-ailab/flash-attention"><img src="https://img.shields.io/github/stars/dao-ailab/flash-attention.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.14135"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2307.07754">Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer</a><br><a href="https://github.com/rocketappslab/bdmm"><img src="https://img.shields.io/github/stars/rocketappslab/bdmm.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2307.07754"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2307.06940">Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation</a><br><a href="https://github.com/videocrafter/animate-a-story"><img src="https://img.shields.io/github/stars/videocrafter/animate-a-story.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2307.06940"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://videocrafter.github.io/Animate-A-Story/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.11173">GD-VDM: Generated Depth for better Diffusion-based Video Generation</a><br><a href="https://github.com/lapid92/gd-vdm"><img src="https://img.shields.io/github/stars/lapid92/gd-vdm.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2306.11173"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.05957">DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles</a><br><a href="https://github.com/taldatech/ddlp"><img src="https://img.shields.io/github/stars/taldatech/ddlp.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2306.05957"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://taldatech.github.io/ddlp-web/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.02562">Video Diffusion Models with Local-Global Context Guidance</a> (IJCAI 2023)<br><a href="https://github.com/exisas/lgc-vd"><img src="https://img.shields.io/github/stars/exisas/lgc-vd.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2306.02562"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.18264">Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising</a><br><a href="https://github.com/g-u-n/gen-l-video"><img src="https://img.shields.io/github/stars/g-u-n/gen-l-video.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2305.18264"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://g-u-n.github.io/projects/gen-long-video/index.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.14330">Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation</a><br><a href="https://github.com/ku-cvlab/direct2v"><img src="https://img.shields.io/github/stars/ku-cvlab/direct2v.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2305.14330"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.01872">Probabilistic Adaptation of Text-to-Video Models</a><br><a href="https://arxiv.org/abs/2306.01872"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://video-adapter.github.io/video-adapter/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.00235">VIDM: Video Implicit Diffusion Models</a> (AAAI 2023)<br><a href="https://github.com/MKFMIKU/VIDM"><img src="https://img.shields.io/github/stars/MKFMIKU/VIDM.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.00235"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://kfmei.page/vidm/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.09478">Mm-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation</a> (CVPR 2023)<br><a href="https://github.com/researchmm/MM-Diffusion"><img src="https://img.shields.io/github/stars/researchmm/MM-Diffusion.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.09478"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.07685">Video Probabilistic Diffusion Models in Projected Latent Space</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2302.07685"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/sihyun-yu/PVDM"><img src="https://img.shields.io/github/stars/sihyun-yu/PVDM.svg?style=social&label=Star" alt="Star"></a>
<a href="https://sihyun.me/PVDM/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2307.03190">Synthesizing Artistic Cinemagraphs from Text</a><br><a href="https://arxiv.org/abs/2307.03190"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/text2cinemagraph/text2cinemagraph"><img src="https://img.shields.io/github/stars/text2cinemagraph/text2cinemagraph.svg?style=social&label=Star" alt="Star"></a>
<a href="https://text2cinemagraph.github.io/website/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2307.00574">Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation</a><br><a href="https://arxiv.org/abs/2307.00574"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2307.00040">DisCo: Disentangled Control for Referring Human Dance Generation in Real World</a><br><a href="https://arxiv.org/abs/2307.00040"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/Wangt-CN/DisCo"><img src="https://img.shields.io/github/stars/Wangt-CN/DisCo.svg?style=social&label=Star" alt="Star"></a>
<a href="https://disco-dance.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.17123">PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN</a><br><a href="https://arxiv.org/abs/2306.17123"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/ken2576/pvp"><img src="https://img.shields.io/github/stars/ken2576/pvp.svg?style=social&label=Star" alt="Star"></a>
<a href="https://cseweb.ucsd.edu//~viscomp/projects/EGSR23PVP/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.16940">BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2306.16940"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/pixelite1201/BEDLAM"><img src="https://img.shields.io/github/stars/pixelite1201/BEDLAM.svg?style=social&label=Star" alt="Star"></a>
<a href="https://bedlam.is.tue.mpg.de/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.16541">Envisioning a Next Generation Extended Reality Conferencing System with Efficient Photorealistic Human Rendering</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2306.16541"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.16003">Reprogramming Audio-driven Talking Face Synthesis into Text-driven</a><br><a href="https://arxiv.org/abs/2306.16003"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.15507">Self-supervised Learning of Event-guided Video Frame Interpolation for Rolling Shutter Frames</a><br><a href="https://arxiv.org/abs/2306.15507"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.13933">Boost Video Frame Interpolation via Motion Adaptation</a><br><a href="https://arxiv.org/abs/2306.13933"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.08707">VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing</a><br><a href="https://arxiv.org/abs/2306.08707"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://videdit.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.08068">DORSal: Diffusion for Object-centric Representations of Scenes</a><br><a href="https://arxiv.org/abs/2306.08068"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.sjoerdvansteenkiste.com/dorsal/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.07954">Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation</a><br><a href="https://arxiv.org/abs/2306.07954"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://anonymous-31415926.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.04321">Generative Semantic Communication: Diffusion Models Beyond Bit Recovery</a><br><a href="https://arxiv.org/abs/2306.04321"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.02903">Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions</a><br><a href="https://arxiv.org/abs/2306.02903"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/lsx0101/Instruct-Video2Avatar"><img src="https://img.shields.io/github/stars/lsx0101/Instruct-Video2Avatar.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.02252">MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning</a><br><a href="https://arxiv.org/abs/2306.02252"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.01732">Video Colorization with Pre-trained Text-to-Image Diffusion Models</a><br><a href="https://arxiv.org/abs/2306.01732"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://colordiffuser.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.01704">Temporal-controlled Frame Swap for Generating High-Fidelity Stereo Driving Data for Autonomy Analysis</a><br><a href="https://arxiv.org/abs/2306.01704"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/ostadabbas/Temporal-controlled-Frame-Swap-GTAV-TeFS-"><img src="https://img.shields.io/github/stars/ostadabbas/Temporal-controlled-Frame-Swap-GTAV-TeFS-.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.01344">Adjustable Visual Appearance for Generalizable Novel View Synthesis</a><br><a href="https://arxiv.org/abs/2306.01344"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://ava-nvs.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.01081">4DSR-GCN: 4D Video Point Cloud Upsampling using Graph Convolutional Networks</a><br><a href="https://arxiv.org/abs/2306.01081"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.00973">Intelligent Grimm -- Open-ended Visual Storytelling via Latent Diffusion Models</a><br><a href="https://arxiv.org/abs/2306.00973"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://haoningwu3639.github.io/StoryGen_Webpage/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.00576">MammalNet: A Large-scale Video Benchmark for Mammal Recognition and Behavior Understanding</a><br><a href="https://arxiv.org/abs/2306.00576"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.19556">Exploring Phonetic Context in Lip Movement for Authentic Talking Face Generation</a><br><a href="https://arxiv.org/abs/2305.19556"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.19193">Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models</a><br><a href="https://arxiv.org/abs/2305.19193"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.19135">Context-Preserving Two-Stage Video Domain Translation for Portrait Stylization</a><br><a href="https://arxiv.org/abs/2305.19135"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.14831">OD-NeRF: Efficient Training of On-the-Fly Dynamic Neural Radiance Fields</a><br><a href="https://arxiv.org/abs/2305.14831"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.14708">EgoVSR: Towards High-Quality Egocentric Video Super-Resolution</a><br><a href="https://arxiv.org/abs/2305.14708"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.14669">NegVSR: Augmenting Negatives for Generalized Noise Modeling in Real-World Video Super-Resolution</a><br><a href="https://arxiv.org/abs/2305.14669"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.14343">Video Prediction Models as Rewards for Reinforcement Learning</a><br><a href="https://arxiv.org/abs/2305.14343"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.14135">Reparo: Loss-Resilient Generative Codec for Video Conferencing</a><br><a href="https://arxiv.org/abs/2305.14135"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.13962">CPNet: Exploiting CLIP-based Attention Condenser and Probability Map Guidance for High-fidelity Talking Face Generation</a> (ICME 2023)<br><a href="https://arxiv.org/abs/2305.13962"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.12411">Synthesizing Diverse Human Motions in 3D Indoor Scenes</a><br><a href="https://arxiv.org/abs/2305.12411"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://zkf1997.github.io/DIMOS/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.12328">InstructVid2Vid: Controllable Video Editing with Natural Language Instructions</a><br><a href="https://arxiv.org/abs/2305.12328"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.11281">SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models</a> (ICLR Workshop 2023)<br><a href="https://arxiv.org/abs/2305.11281"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://slotdiffusion.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.10198">IDO-VFI: Identifying Dynamics via Optical Flow Guidance for Video Frame Interpolation with Events</a><br><a href="https://arxiv.org/abs/2305.10198"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.09512">Light-VQA: A Multi-Dimensional Quality Assessment Model for Low-Light Video Enhancement</a><br><a href="https://arxiv.org/abs/2305.09512"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.08854">Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models</a><br><a href="https://arxiv.org/abs/2305.08854"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.08293">Identity-Preserving Talking Face Generation with Landmark and Appearance Priors</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2305.08293"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/Weizhi-Zhong/IP_LAP"><img src="https://img.shields.io/github/stars/Weizhi-Zhong/IP_LAP.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.06356">HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion</a><br><a href="https://arxiv.org/abs/2305.06356"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/synthesiaresearch/humanrf"><img src="https://img.shields.io/github/stars/synthesiaresearch/humanrf.svg?style=social&label=Star" alt="Star"></a>
<a href="https://synthesiaresearch.github.io/humanrf/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.05464">Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer</a><br><a href="https://arxiv.org/abs/2305.05464"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/haha-lisa/Style-A-Video"><img src="https://img.shields.io/github/stars/haha-lisa/Style-A-Video.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.03049">NeuralEditor: Editing Neural Radiance Fields via Manipulating Point Clouds</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2305.03049"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://immortalco.github.io/NeuralEditor/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.00126">DSEC-MOS: Segment Any Moving Object with Moving Ego Vehicle</a><br><a href="https://arxiv.org/abs/2305.00126"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/ZZY-Zhou/DSEC-MOS"><img src="https://img.shields.io/github/stars/ZZY-Zhou/DSEC-MOS.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.13596">Video Frame Interpolation with Densely Queried Bilateral Correlation</a> (IJCAI 2023)<br><a href="https://arxiv.org/abs/2304.13596"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/kinoud/DQBC"><img src="https://img.shields.io/github/stars/kinoud/DQBC.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.12664">Dynamic Video Frame Interpolation with integrated Difficulty Pre-Assessment</a><br><a href="https://arxiv.org/abs/2304.12664"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.09790">AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2304.09790"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/MCG-NKU/AMT"><img src="https://img.shields.io/github/stars/MCG-NKU/AMT.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.08477">Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation</a><br><a href="https://arxiv.org/abs/2304.08477"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://latent-shift.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.07915">CAT-NeRF: Constancy-Aware Tx$^2$Former for Dynamic Body Modeling</a> (CVPR Workshop 2023)<br><a href="https://arxiv.org/abs/2304.07915"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.06818">Soundini: Sound-Guided Diffusion for Natural Video Editing</a><br><a href="https://arxiv.org/abs/2304.06818"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://kuai-lab.github.io/soundini-gallery/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.06211">Boosting Video Object Segmentation via Space-time Correspondence Learning</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2304.06211"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/wenguanwang/VOS_Correspondence"><img src="https://img.shields.io/github/stars/wenguanwang/VOS_Correspondence.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.06020">VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs</a><br><a href="https://arxiv.org/abs/2304.06020"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://cyberiada.github.io/VidStyleODE/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.05930">MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2304.05930"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/rkyuca/medvt"><img src="https://img.shields.io/github/stars/rkyuca/medvt.svg?style=social&label=Star" alt="Star"></a>
<a href="https://rkyuca.github.io/medvt/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.04897">Neural Image-based Avatars: Generalizable Radiance Fields for Human Avatar Modeling</a><br><a href="https://arxiv.org/abs/2304.04897"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://youngjoongunc.github.io/nia/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.03275">That&#39;s What I Said: Fully-Controllable Talking Face Generation</a><br><a href="https://arxiv.org/abs/2304.03275"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://mm.kaist.ac.kr/projects/FC-TFG/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.02633">HNeRV: A Hybrid Neural Representation for Videos</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2304.02633"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/haochen-rye/HNeRV"><img src="https://img.shields.io/github/stars/haochen-rye/HNeRV.svg?style=social&label=Star" alt="Star"></a>
<a href="https://haochen-rye.github.io/HNeRV/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.02225">BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2304.02225"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/JunHeum/BiFormer"><img src="https://img.shields.io/github/stars/JunHeum/BiFormer.svg?style=social&label=Star" alt="Star"></a>
<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_BiFormer_Learning_Bilateral_Motion_Estimation_via_Bilateral_Transformer_for_4K_CVPR_2023_paper.pdf"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.00334">TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles</a><br><a href="https://arxiv.org/abs/2304.00334"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.17789">FONT: Flow-guided One-shot Talking Head Generation with Natural Head Motions</a> (ICME 2023)<br><a href="https://arxiv.org/abs/2303.17789"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.17599">Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models</a><br><a href="https://arxiv.org/abs/2303.17599"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/baaivision/vid2vid-zero"><img src="https://img.shields.io/github/stars/baaivision/vid2vid-zero.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.17598">Consistent View Synthesis with Pose-Guided Diffusion Models</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.17598"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://poseguided-diffusion.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.17550">DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder</a><br><a href="https://arxiv.org/abs/2303.17550"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://daetalker.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.15880">Novel View Synthesis of Humans using Differentiable Rendering</a><br><a href="https://arxiv.org/abs/2303.15880"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/GuillaumeRochette/HumanViewSynthesis"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.14717">CelebV-Text: A Large-Scale Facial Text-Video Dataset</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.14717"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/celebv-text/CelebV-Text"><img src="https://img.shields.io/github/stars/celebv-text/CelebV-Text.svg?style=social&label=Star" alt="Star"></a>
<a href="https://celebv-text.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.14613">GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents</a> (SIGGRAPH 2023)<br><a href="https://arxiv.org/abs/2303.14613"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.14536">SUDS: Scalable Urban Dynamic Scenes</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.14536"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/hturki/suds"><img src="https://img.shields.io/github/stars/hturki/suds.svg?style=social&label=Star" alt="Star"></a>
<a href="https://haithemturki.com/suds/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.14435">NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.14435"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/JokerYan/NeRF-DS"><img src="https://img.shields.io/github/stars/JokerYan/NeRF-DS.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.13825">HandNeRF: Neural Radiance Fields for Animatable Interacting Hands</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.13825"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.13439">Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</a><br><a href="https://arxiv.org/abs/2303.13439"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/Picsart-AI-Research/Text2Video-Zero"><img src="https://img.shields.io/github/stars/Picsart-AI-Research/Text2Video-Zero.svg?style=social&label=Star" alt="Star"></a>
<a href="https://text2video-zero.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.12791">SHERF: Generalizable Human NeRF from a Single Image</a><br><a href="https://arxiv.org/abs/2303.12791"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/skhu101/SHERF"><img src="https://img.shields.io/github/stars/skhu101/SHERF.svg?style=social&label=Star" alt="Star"></a>
<a href="https://skhu101.github.io/SHERF/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.12688">Pix2Video: Video Editing using Image Diffusion</a><br><a href="https://arxiv.org/abs/2303.12688"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/G-U-N/Pix2Video.pytorch"><img src="https://img.shields.io/github/stars/G-U-N/Pix2Video.pytorch.svg?style=social&label=Star" alt="Star"></a>
<a href="https://duyguceylan.github.io/pix2video.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.12337">Music-Driven Group Choreography</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.12337"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/aioz-ai/AIOZ-GDANCE"><img src="https://img.shields.io/github/stars/aioz-ai/AIOZ-GDANCE.svg?style=social&label=Star" alt="Star"></a>
<a href="https://aioz-ai.github.io/AIOZ-GDANCE/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.12234">Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields</a><br><a href="https://arxiv.org/abs/2303.12234"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/CVUBLab/pre-nerf"><img src="https://img.shields.io/github/stars/CVUBLab/pre-nerf.svg?style=social&label=Star" alt="Star"></a>
<a href="https://amughrabi.github.io/prenerf/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.11548">Emotionally Enhanced Talking Face Generation</a><br><a href="https://arxiv.org/abs/2303.11548"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/sahilg06/EmoGen"><img src="https://img.shields.io/github/stars/sahilg06/EmoGen.svg?style=social&label=Star" alt="Star"></a>
<a href="https://midas.iiitd.edu.in/emo/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.11003">Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization</a><br><a href="https://arxiv.org/abs/2303.11003"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.10452">Confidence Attention and Generalization Enhanced Distillation for Continuous Video Domain Adaptation</a><br><a href="https://arxiv.org/abs/2303.10452"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.10275">MoRF: Mobile Realistic Fullbody Avatars from a Monocular Video</a><br><a href="https://arxiv.org/abs/2303.10275"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.10100">Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation</a><br><a href="https://arxiv.org/abs/2303.10100"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.09941">Leaping Into Memories: Space-Time Deep Feature Synthesis</a><br><a href="https://arxiv.org/abs/2303.09941"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.09826">Learning Data-Driven Vector-Quantized Degradation Model for Animation Video Super-Resolution</a><br><a href="https://arxiv.org/abs/2303.09826"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.09535">FateZero: Fusing Attentions for Zero-shot Text-based Video Editing</a><br><a href="https://arxiv.org/abs/2303.09535"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/ChenyangQiQi/FateZero"><img src="https://img.shields.io/github/stars/ChenyangQiQi/FateZero.svg?style=social&label=Star" alt="Star"></a>
<a href="https://fate-zero-edit.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.09508">LDMVFI: Video Frame Interpolation with Latent Diffusion Models</a><br><a href="https://arxiv.org/abs/2303.09508"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.09370">Learning Physical-Spatio-Temporal Features for Video Shadow Removal</a><br><a href="https://arxiv.org/abs/2303.09370"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.09170">NLUT: Neural-based 3D Lookup Tables for Video Photorealistic Style Transfer</a><br><a href="https://arxiv.org/abs/2303.09170"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/semchan/NLUT"><img src="https://img.shields.io/github/stars/semchan/NLUT.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.08639">Blowing in the Wind: CycleNet for Human Cinemagraphs from Still Images</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.08639"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://hbertiche.github.io/CycleNet/"><img src="https://img.shields.io/github/stars/CycleNet/.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.08120">Blind Video Deflickering by Neural Filtering with a Flawed Atlas</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.08120"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/ChenyangLEI/All-In-One-Deflicker"><img src="https://img.shields.io/github/stars/ChenyangLEI/All-In-One-Deflicker.svg?style=social&label=Star" alt="Star"></a>
<a href="https://chenyanglei.github.io/deflicker/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.02959">Butterfly: Multiple Reference Frames Feature Propagation Mechanism for Neural Video Compression</a> (DCC 2023)<br><a href="https://arxiv.org/abs/2303.02959"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.14362">One-Shot Video Inpainting</a> (AAAI 2023)<br><a href="https://arxiv.org/abs/2302.14362"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.13256">Continuous Space-Time Video Super-Resolution Utilizing Long-Range Temporal Information</a><br><a href="https://arxiv.org/abs/2302.13256"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.12237">Learning Neural Volumetric Representations of Dynamic Humans in Minutes</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2302.12237"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/zju3dv/instant-nvr"><img src="https://img.shields.io/github/stars/zju3dv/instant-nvr.svg?style=social&label=Star" alt="Star"></a>
<a href="https://zju3dv.github.io/instant_nvr/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.10001">STB-VMM: Swin Transformer Based Video Motion Magnification</a><br><a href="https://arxiv.org/abs/2302.10001"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/RLado/STB-VMM"><img src="https://img.shields.io/github/stars/RLado/STB-VMM.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.08197">OPT: One-shot Pose-Controllable Talking Head Generation</a> (ICASSP 2023)<br><a href="https://arxiv.org/abs/2302.08197"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.07848">One-Shot Face Video Re-enactment using Hybrid Latent Spaces of StyleGAN2</a><br><a href="https://arxiv.org/abs/2302.07848"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://trevineoorloff.github.io/FaceVideoReenactment_HybridLatents.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.05916">Video Waterdrop Removal via Spatio-Temporal Fusion in Driving Scenes</a><br><a href="https://arxiv.org/abs/2302.05916"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/csqiangwen/Video_Waterdrop_Removal_in_Driving_Scenes"><img src="https://img.shields.io/github/stars/csqiangwen/Video_Waterdrop_Removal_in_Driving_Scenes.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.03011">Structure and Content-Guided Video Synthesis with Diffusion Models</a><br><a href="https://arxiv.org/abs/2302.03011"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://research.runwayml.com/gen1"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.02088">AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</a><br><a href="https://arxiv.org/abs/2302.02088"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://liangsusan-git.github.io/project/avnerf/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.01329">Dreamix: Video Diffusion Models are General Video Editors</a><br><a href="https://arxiv.org/abs/2302.01329"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://dreamix-video-editing.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.01133">SceneScape: Text-Driven Consistent Scene Generation</a><br><a href="https://arxiv.org/abs/2302.01133"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/RafailFridman/SceneScape"><img src="https://img.shields.io/github/stars/RafailFridman/SceneScape.svg?style=social&label=Star" alt="Star"></a>
<a href="https://scenescape.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.12352">Maximal Cliques on Multi-Frame Proposal Graph for Unsupervised Video Object Segmentation</a><br><a href="https://arxiv.org/abs/2301.12352"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.11880">Optical Flow Estimation in 360$^\circ$ Videos: Dataset, Model and Application</a><br><a href="https://arxiv.org/abs/2301.11880"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://siamlof.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.11326">Unsupervised Volumetric Animation</a><br><a href="https://arxiv.org/abs/2301.11326"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://snap-research.github.io/unsupervised-volumetric-animation/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.11280">Text-To-4D Dynamic Scene Generation</a><br><a href="https://arxiv.org/abs/2301.11280"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://make-a-video3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.08846">Regeneration Learning: A Learning Paradigm for Data Generation</a><br><a href="https://arxiv.org/abs/2301.08846"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.05191">Event-Based Frame Interpolation with Ad-hoc Deblurring</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2301.05191"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/AHupuJR/REFID"><img src="https://img.shields.io/github/stars/AHupuJR/REFID.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.03786">DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2301.03786"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sstzal.github.io/DiffTalk/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.03396">Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation</a><br><a href="https://arxiv.org/abs/2301.03396"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://mstypulkowski.github.io/diffusedheads/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.02238">HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2301.02238"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/facebookresearch/hyperreel"><img src="https://img.shields.io/github/stars/facebookresearch/hyperreel.svg?style=social&label=Star" alt="Star"></a>
<a href="https://hyperreel.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.01081">StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles</a> (AAAI 2023)<br><a href="https://arxiv.org/abs/2301.01081"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/FuxiVirtualHuman/styletalk"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.00411">Detachable Novel Views Synthesis of Dynamic Scenes Using Distribution-Driven Neural Radiance Fields</a><br><a href="https://arxiv.org/abs/2301.00411"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/Luciferbobo/D4NeRF"><img src="https://img.shields.io/github/stars/Luciferbobo/D4NeRF.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.11682">SkyGPT: Probabilistic Short-term Solar Forecasting Using Synthetic Sky Videos from Physics-constrained VideoGPT</a><br><a href="https://arxiv.org/abs/2306.11682"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.07257">MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images</a><br><a href="https://arxiv.org/abs/2306.07257"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2306.03594">Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks</a><br><a href="https://arxiv.org/abs/2306.03594"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.11772">Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes</a><br><a href="https://arxiv.org/abs/2305.11772"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.03713">Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos</a><br><a href="https://arxiv.org/abs/2305.03713"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.02296">DynamicStereo: Consistent Dynamic Depth from Stereo Videos</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2305.02296"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/facebookresearch/dynamic_stereo"><img src="https://img.shields.io/github/stars/facebookresearch/dynamic_stereo.svg?style=social&label=Star" alt="Star"></a>
<a href="https://dynamic-stereo.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.14401">ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs</a><br><a href="https://arxiv.org/abs/2304.14401"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://jitengmu.github.io/ActorsNeRF/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.12317">Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis</a><br><a href="https://arxiv.org/abs/2304.12317"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/andrewsonga/Total-Recon"><img src="https://img.shields.io/github/stars/andrewsonga/Total-Recon.svg?style=social&label=Star" alt="Star"></a>
<a href="https://andrewsonga.github.io/totalrecon/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.11470">3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2304.11470"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.06403">Leveraging triplet loss for unsupervised action segmentation</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2304.06403"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/elenabbbuenob/tsa-actionseg"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.02001">MonoHuman: Animatable Human Neural Field from Monocular Video</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2304.02001"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://yzmblog.github.io/projects/MonoHuman/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2304.01893">Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2304.01893"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://research.nvidia.com/labs/toronto-ai/trace-pace/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.17480">Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.17480"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/Sxjdwang/TalkLip"><img src="https://img.shields.io/github/stars/Sxjdwang/TalkLip.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.15893">VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.15893"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="http://afruehstueck.github.io/vive3D/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.15469">CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.15469"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/cams-hoi/CAMS"><img src="https://img.shields.io/github/stars/cams-hoi/CAMS.svg?style=social&label=Star" alt="Star"></a>
<a href="https://cams-hoi.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.14109">Prediction of the morphological evolution of a splashing drop using an encoder-decoder</a><br><a href="https://arxiv.org/abs/2303.14109"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.13497">TriPlaneNet: An Encoder for EG3D Inversion</a><br><a href="https://arxiv.org/abs/2303.13497"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://anantarb.github.io/triplanenet"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.09857">Dual-path Adaptation from Image to Video Transformers</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2303.09857"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/park-jungin/DualPath"><img src="https://img.shields.io/github/stars/park-jungin/DualPath.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2303.04761">Video-P2P: Video Editing with Cross-attention Control</a><br><a href="https://arxiv.org/abs/2303.04761"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/ShaoTengLiu/Video-P2P"><img src="https://img.shields.io/github/stars/ShaoTengLiu/Video-P2P.svg?style=social&label=Star" alt="Star"></a>
<a href="https://video-p2p.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.14683">IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF</a><br><a href="https://arxiv.org/abs/2302.14683"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://ustc3dv.github.io/IntrinsicNGP/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2301.02239">Robust Dynamic Radiance Fields</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2301.02239"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/facebookresearch/robust-dynrf"><img src="https://img.shields.io/github/stars/facebookresearch/robust-dynrf.svg?style=social&label=Star" alt="Star"></a>
<a href="https://robust-dynrf.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
</ul>
<h2>2022</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2204.03458">Video Diffusion Models</a> (NeurIPS 2022)<br><a href="https://arxiv.org/abs/2204.03458"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://video-diffusion.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.09853">McVd: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation</a> (NeurIPS 2022)<br><a href="https://github.com/voletiv/mcvd-pytorch"><img src="https://img.shields.io/github/stars/Tobi-r9/RaMViD.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.09853"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://mask-cond-video-diffusion.github.io"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.07696">Diffusion Models for Video Prediction and Infilling</a> (TMLR 2022)<br><a href="https://github.com/Tobi-r9/RaMViD"><img src="https://img.shields.io/github/stars/Tobi-r9/RaMViD.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.07696"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sites.google.com/view/video-diffusion-prediction"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://openreview.net/forum?id=nJfylDvgzlq">Make-A-Video: Text-to-Video Generation without Text-Video Data</a> (ICLR 2023)<br><a href="https://github.com/lucidrains/make-a-video-pytorch"><img src="https://img.shields.io/github/stars/lucidrains/make-a-video-pytorch.svg?style=social&label=Star" alt="Star"></a>
<a href="https://openreview.net/forum?id=nJfylDvgzlq"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://makeavideo.studio"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.06605">DaGAN: Depth-Aware Generative Adversarial Network for Talking Head Video Generation</a> (CVPR 2022)<br><a href="https://github.com/harlanhong/CVPR2022-DaGAN"><img src="https://img.shields.io/github/stars/harlanhong/CVPR2022-DaGAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.06605"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://harlanhong.github.io/publications/dagan.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.02573">Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning</a> (CVPR 2022)<br><a href="https://github.com/snap-research/MMVID"><img src="https://img.shields.io/github/stars/snap-research/MMVID.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.02573"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://snap-research.github.io/MMVID/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.01914">Playable Environments: Video Manipulation in Space and Time</a> (CVPR 2022)<br><a href="https://github.com/willi-menapace/PlayableEnvironments"><img src="https://img.shields.io/github/stars/willi-menapace/PlayableEnvironments.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.01914"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://willi-menapace.github.io/playable-environments-website/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.05049">Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis</a> (ECCV 2022)<br><a href="https://github.com/fast-vid2vid/fast-vid2vid"><img src="https://img.shields.io/github/stars/fast-vid2vid/fast-vid2vid.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.05049"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://fast-vid2vid.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.01696">TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts</a> (ECCV 2022)<br><a href="https://github.com/EricGuo5513/TM2T"><img src="https://img.shields.io/github/stars/EricGuo5513/TM2T.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.01696"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://ericguo5513.github.io/TM2T/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.02303">Imagen Video: High Definition Video Generation with Diffusion Models</a><br><a href="https://arxiv.org/abs/2210.02303"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://video-diffusion.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.02399">Phenaki: Variable length video generation from open domain textual description</a><br><a href="https://arxiv.org/abs/2210.02399"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sites.research.google/phenaki/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
<p>Code (unofficial?): <a href="https://github.com/XX/YY"><img src="https://img.shields.io/github/stars/lucidrains/phenaki-pytorch.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.11565">Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation</a><br><a href="https://github.com/showlab/Tune-A-Video"><img src="https://img.shields.io/github/stars/bbzhu-jy16/motionvideogan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.11565"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.07413">Towards Smooth Video Composition</a><br><a href="https://github.com/genforce/StyleSV"><img src="https://img.shields.io/github/stars/genforce/StyleSV.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.07413"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.13221">Latent Video Diffusion Models for High-Fidelity Long Video Generation</a><br><a href="https://github.com/yingqinghe/lvdm"><img src="https://img.shields.io/github/stars/yingqinghe/lvdm.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.13221"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://yingqinghe.github.io/LVDM/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.11743">SinFusion: Training Diffusion Models on a Single Image or Video</a><br><a href="https://github.com/yanivnik/sinfusion-code"><img src="https://img.shields.io/github/stars/yanivnik/sinfusion-code.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.11743"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://yanivnik.github.io/sinfusion/static/video_comparisons.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.16579">INR-V: A Continuous Representation Space for Video-based Generative Tasks</a><br><a href="https://github.com/bipashasen/INRV"><img src="https://img.shields.io/github/stars/bipashasen/INRV.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.16579"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://skymanaditya1.github.io/INRV/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.04366">Computational Choreography using Human Motion Synthesis</a><br><a href="https://github.com/patrickrperrine/comp-choreo"><img src="https://img.shields.io/github/stars/patrickrperrine/comp-choreo.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.04366"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.02399">Phenaki: Variable Length Video Generation From Open Domain Textual Description</a><br><a href="https://github.com/lucidrains/phenaki-pytorch"><img src="https://img.shields.io/github/stars/lucidrains/phenaki-pytorch.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.02399"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://phenaki.video/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.02396">Temporally Consistent Transformers for Video Generation</a><br><a href="https://github.com/wilson1yan/teco"><img src="https://img.shields.io/github/stars/wilson1yan/teco.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.02396"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://wilson1yan.github.io/teco/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.07862">StyleFaceV: Face Video Generation via Decomposing and Recomposing Pretrained StyleGAN3</a><br><a href="https://github.com/arthur-qiu/stylefacev"><img src="https://img.shields.io/github/stars/arthur-qiu/stylefacev.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2208.07862"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="http://haonanqiu.com/projects/StyleFaceV.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.09814">NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis</a><br><a href="https://github.com/microsoft/nuwa"><img src="https://img.shields.io/github/stars/arthur-qiu/stylefacev.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.09814"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://nuwa-infinity.microsoft.com/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.14797">3D-Aware Video Generation</a><br><a href="https://github.com/sherwinbahmani/3dvideogeneration/"><img src="https://img.shields.io/github/stars/sherwinbahmani/3dvideogeneration.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.14797"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sherwinbahmani.github.io/3dvidgen/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.04003">Patch-based Object-centric Transformers for Efficient Video Generation</a><br><a href="https://github.com/wilson1yan/povt"><img src="https://img.shields.io/github/stars/wilson1yan/povt.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.04003"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sites.google.com/view/povt-public"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.03429">Generating Long Videos of Dynamic Scenes</a><br><a href="https://github.com/nvlabs/long-video-gan"><img src="https://img.shields.io/github/stars/nvlabs/long-video-gan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.03429"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.timothybrooks.com/tech/long-video-gan/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.01651">D&#39;ARTAGNAN: Counterfactual Video Generation</a><br><a href="https://github.com/hreynaud/dartagnan"><img src="https://img.shields.io/github/stars/nvlabs/long-video-gan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.01651"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.15868">CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers</a><br><a href="https://github.com/thudm/cogvideo"><img src="https://img.shields.io/github/stars/nvlabs/long-video-gan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.15868"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.13221">Latent Video Diffusion Models for High-Fidelity Video Generation With Arbitrary Lengths</a><br><a href="https://github.com/YingqingHe/LVDM"><img src="https://img.shields.io/github/stars/YingqingHe/LVDM.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.13221"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://yingqinghe.github.io/LVDM/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.11018">MagicVideo: Efficient Video Generation With Latent Diffusion Models</a><br><a href="https://arxiv.org/abs/2211.11018"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://magicvideo.github.io/#"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.09481">Diffusion Probabilistic Modeling for Video Generation</a><br><a href="https://github.com/buggyyang/RVD"><img src="https://img.shields.io/github/stars/buggyyang/RVD.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.09481"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.11495">Flexible Diffusion Modeling of Long Videos</a><br><a href="https://github.com/plai-group/flexible-video-diffusion-modeling"><img src="https://img.shields.io/github/stars/plai-group/flexible-video-diffusion-modeling.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.11495"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://fdmolv.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2204.03638">Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer</a> (ECCV 2022)<br><a href="https://github.com/songweige/tats"><img src="https://img.shields.io/github/stars/songweige/tats.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2204.03638"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://songweige.github.io/projects/tats/index.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2203.09481">Diffusion Probabilistic Modeling for Video Generation</a><br><a href="https://github.com/buggyyang/rvd"><img src="https://img.shields.io/github/stars/buggyyang/rvd.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.09481"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2203.04036">StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN</a><br><a href="https://github.com/OpenTalker/StyleHEAT"><img src="https://img.shields.io/github/stars/OpenTalker/StyleHEAT.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.04036"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://feiiyin.github.io/StyleHEAT/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2202.10571">Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks</a> (ICLR 2022)<br><a href="https://github.com/sihyun-yu/digan"><img src="https://img.shields.io/github/stars/sihyun-yu/digan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2202.10571"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sihyun.me/digan/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.14683">StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2</a> (CVPR 2022)<br><a href="https://github.com/universome/stylegan-v"><img src="https://img.shields.io/github/stars/universome/stylegan-v.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.14683"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://universome.github.io/stylegan-v.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.02815">Make It Move: Controllable Image-to-Video Generation with Text Descriptions</a> (CVPR 2022)<br><a href="https://github.com/youncy-hu/mage"><img src="https://img.shields.io/github/stars/youncy-hu/mage.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.02815"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.13660">NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action</a> (CVPR 2023)<br><a href="https://github.com/wangkua1/nemo-cvpr2023"><img src="https://img.shields.io/github/stars/wangkua1/nemo-cvpr2023.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.13660"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sites.google.com/view/nemo-neural-motion-field"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.13525">Cross-Resolution Flow Propagation for Foveated Video Super-Resolution</a> (WACV 2023)<br><a href="https://github.com/eugenelet/CRFP"><img src="https://img.shields.io/github/stars/eugenelet/CRFP.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.13525"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.13056">MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular Videos</a><br><a href="https://arxiv.org/abs/2212.13056"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.11972">Scalable Adaptive Computation for Iterative Generation</a> (ICML 2023)<br><a href="https://github.com/google-research/pix2seq"><img src="https://img.shields.io/github/stars/google-research/pix2seq.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.11972"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.11642">Predictive Coding Based Multiscale Network with Encoder-Decoder LSTM for Video Prediction</a><br><a href="https://github.com/Ling-CF/MSPN"><img src="https://img.shields.io/github/stars/Ling-CF/MSPN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.11642"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.10550">InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds</a><br><a href="https://github.com/tijiang13/InstantAvatar"><img src="https://img.shields.io/github/stars/tijiang13/InstantAvatar.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.10550"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.09478">MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation</a> (CVPR 2023)<br><a href="https://github.com/researchmm/MM-Diffusion"><img src="https://img.shields.io/github/stars/researchmm/MM-Diffusion.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.09478"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.08377">PointAvatar: Deformable Point-based Head Avatars from Videos</a><br><a href="https://github.com/zhengyuf/pointavatar"><img src="https://img.shields.io/github/stars/zhengyuf/pointavatar.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.08377"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://zhengyuf.github.io/PointAvatar/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.06384">PV3D: A 3D Generative Model for Portrait Video Generation</a> (ICLR 2023)<br><a href="https://github.com/bytedance/pv3d"><img src="https://img.shields.io/github/stars/bytedance/pv3d.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.06384"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://showlab.github.io/pv3d/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.06026">Video Prediction by Efficient Transformers</a> (ICPR 2022)<br><a href="https://github.com/XiYe20/VPTR"><img src="https://img.shields.io/github/stars/XiYe20/VPTR.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.06026"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.05199">MAGVIT: Masked Generative Video Transformer</a> (CVPR 2023)<br><a href="https://github.com/google-research/magvit"><img src="https://img.shields.io/github/stars/google-research/magvit.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.05199"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://magvit.cs.cmu.edu/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.04741">Physically Plausible Animation of Human Upper Body from a Single Image</a> (WACV 2023)<br><a href="https://arxiv.org/abs/2212.04741"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2212.04655.pdf">MIMO Is All You Need : A Strong Multi-In-Multi-Out Baseline for Video Prediction</a><br><a href="https://arxiv.org/pdf/2212.04655.pdf"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.03250">Neural Cell Video Synthesis via Optical-Flow Diffusion</a><br><a href="https://arxiv.org/abs/2212.03250"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.02871">Video Object of Interest Segmentation</a><br><a href="https://arxiv.org/abs/2212.02871"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.02350">Audio-Driven Co-Speech Gesture Video Generation</a> (NeurIPS 2022)<br><a href="https://github.com/alvinliu0/ANGIE"><img src="https://img.shields.io/github/stars/alvinliu0/ANGIE.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.02350"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://alvinliu0.github.io/projects/ANGIE"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.00235">VIDM: Video Implicit Diffusion Models</a> (AAAI 2023)<br><a href="https://github.com/MKFMIKU/vidm"><img src="https://img.shields.io/github/stars/MKFMIKU/vidm.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.00235"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://kfmei.page/vidm/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.00190">Mixed Neural Voxels for Fast Multi-view Video Synthesis</a><br><a href="https://github.com/fengres/mixvoxels"><img src="https://img.shields.io/github/stars/fengres/mixvoxels.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2212.00190"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.14758">VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild</a> (SIGGRAPH Asia 2022)<br><a href="https://github.com/OpenTalker/video-retalking"><img src="https://img.shields.io/github/stars/OpenTalker/video-retalking.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.14758"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://opentalker.github.io/video-retalking/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.14575">Randomized Conditional Flow Matching for Video Prediction</a><br><a href="https://arxiv.org/abs/2211.14575"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.14506">Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis</a><br><a href="https://github.com/Dorniwang/PD-FGC-inference"><img src="https://img.shields.io/github/stars/Dorniwang/PD-FGC-inference.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.14506"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://dorniwang.github.io/PD-FGC/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.14308">WALDO: Future Video Synthesis using Object Layer Decomposition and Parametric Flow Prediction</a><br><a href="https://arxiv.org/abs/2211.14308"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://16lemoing.github.io/waldo/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.14005">Efficient Feature Extraction for High-resolution Video Frame Interpolation</a> (BMVC 2022)<br><a href="https://github.com/visinf/fldr-vfi"><img src="https://img.shields.io/github/stars/visinf/fldr-vfi.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.14005"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.13994">Dynamic Neural Portraits</a> (WACV 2023)<br><a href="https://arxiv.org/abs/2211.13994"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://michaildoukas.github.io/DynamicNeuralPortraits/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.13319">Make-A-Story: Visual Memory Conditioned Consistent Story Generation</a> (CVPR 2023)<br><a href="https://github.com/ubc-vision/Make-A-Story"><img src="https://img.shields.io/github/stars/ubc-vision/Make-A-Story.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.13319"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.12824">Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2211.12824"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://tvc-mmvg.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.12782">Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video</a> (CVPR 2023)<br><a href="https://github.com/SeanChenxy/HandAvatar"><img src="https://img.shields.io/github/stars/SeanChenxy/HandAvatar.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.12782"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://seanchenxy.github.io/HandAvatarWeb/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.12604#:~:text=22%20Nov%202022%5D-,SuperTran%3A%20Reference%20Based%20Video%20Transformer%20for%20Enhancing,Bitrate%20Streams%20in%20Real%20Time&text=This%20work%20focuses%20on%20low,video%20quality%20is%20severely%20compromised.">SuperTran: Reference Based Video Transformer for Enhancing Low Bitrate Streams in Real Time</a><br><a href="https://arxiv.org/abs/2211.12604#:~:text=22%20Nov%202022%5D-,SuperTran%3A%20Reference%20Based%20Video%20Transformer%20for%20Enhancing,Bitrate%20Streams%20in%20Real%20Time&text=This%20work%20focuses%20on%20low,video%20quality%20is%20severely%20compromised."><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.12436">Depth-Supervised NeRF for Multi-View RGB-D Operating Room Images</a><br><a href="https://arxiv.org/abs/2211.12436"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.12194">SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</a> (CVPR 2023)<br><a href="https://github.com/OpenTalker/SadTalker"><img src="https://img.shields.io/github/stars/OpenTalker/SadTalker.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.12194"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sadtalker.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.11903">FLEX: Full-Body Grasping Without Full-Body Grasps</a> (CVPR 2023)<br><a href="https://github.com/purvaten/FLEX"><img src="https://img.shields.io/github/stars/purvaten/FLEX.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.11903"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://flex.cs.columbia.edu/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.11423">Blur Interpolation Transformer for Real-World Motion from Blur</a> (CVPR 2023)<br><a href="https://github.com/zzh-tech/BiT"><img src="https://img.shields.io/github/stars/zzh-tech/BiT.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.11423"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://zzh-tech.github.io/BiT/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.11417">DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata</a> (CVPR 2023)<br><a href="https://github.com/IVRL/DyNCA"><img src="https://img.shields.io/github/stars/IVRL/DyNCA.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.11417"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://dynca.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.11309">H-VFI: Hierarchical Frame Interpolation for Videos with Large Motions</a><br><a href="https://arxiv.org/abs/2211.11309"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.10791">AdaFNIO: Adaptive Fourier Neural Interpolation Operator for video frame interpolation</a><br><a href="https://arxiv.org/abs/2211.10791"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.09809">SPACE: Speech-driven Portrait Animation with Controllable Expression</a><br><a href="https://arxiv.org/abs/2211.09809"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.08861">Creative divergent synthesis with generative models</a><br><a href="https://arxiv.org/abs/2211.08861"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.08428">CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming</a><br><a href="https://arxiv.org/abs/2211.08428"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.07004">Advancing Learned Video Compression with In-loop Frame Prediction</a> (IEEE T-CSVT 2022)<br><a href="https://github.com/RenYang-home/ALVC"><img src="https://img.shields.io/github/stars/RenYang-home/ALVC.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.07004"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.06119">SSGVS: Semantic Scene Graph-to-Video Synthesis</a><br><a href="https://arxiv.org/abs/2211.06119"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.03889">Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories</a><br><a href="https://github.com/facebookresearch/cop3d"><img src="https://img.shields.io/github/stars/facebookresearch/cop3d.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.03889"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://cop3d.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.01639">Temporal Consistency Learning of inter-frames for Video Super-Resolution</a> (IEEE T-CSVT 2022)<br><a href="https://arxiv.org/abs/2211.01639"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.00924">SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory</a> (AAAI 2022)<br><a href="https://arxiv.org/abs/2211.00924"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.15134">Learning Variational Motion Prior for Video-based Motion Capture</a><br><a href="https://arxiv.org/abs/2210.15134"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.14831">Streaming Radiance Fields for 3D Video Synthesis</a> (NeurIPS 2022)<br><a href="https://github.com/AlgoHunt/StreamRF"><img src="https://img.shields.io/github/stars/AlgoHunt/StreamRF.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.14831"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.13648">Learning to forecast vegetation greenness at fine resolution over Africa with ConvLSTMs</a> (NeurIPS 2022)<br><a href="https://arxiv.org/abs/2210.13648"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.13077">EpipolarNVS: leveraging on Epipolar geometry for single-image Novel View Synthesis</a> (BMVC 2022)<br><a href="https://arxiv.org/abs/2210.13077"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.12826">Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization</a><br><a href="https://github.com/pschaldenbrand/Text2Video"><img src="https://img.shields.io/github/stars/pschaldenbrand/Text2Video.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.12826"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://pschaldenbrand.github.io/text2video/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.11182">Facial Expression Video Generation Based-On Spatio-temporal Convolutional GAN: FEV-GAN</a> (ISWA)<br><a href="https://arxiv.org/abs/2210.11182"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.08737">Temporal and Contextual Transformer for Multi-Camera Editing of TV Shows</a> (ECCV 2022)<br><a href="https://github.com/VirtualFilmStudio/TVMCE"><img src="https://img.shields.io/github/stars/VirtualFilmStudio/TVMCE.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.08737"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://virtualfilmstudio.github.io/projects/multicam/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.08577">Stochastic Occupancy Grid Map Prediction in Dynamic Scenes</a><br><a href="https://github.com/TempleRAIL/SOGMP"><img src="https://img.shields.io/github/stars/TempleRAIL/SOGMP.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.08577"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.07181">MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Pose</a> (ICML 2023)<br><a href="https://arxiv.org/abs/2210.07181"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://oasisyang.github.io/mononerf/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.06877">Pre-Avatar: An Automatic Presentation Generation Framework Leveraging Talking Avatar</a> (ICTAI 2022)<br><a href="https://arxiv.org/abs/2210.06877"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.06465">AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars</a> (NeurIPS 2022)<br><a href="https://arxiv.org/abs/2210.06465"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://yuewuhkust.github.io/AniFaceGAN/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.06366">A Generalist Framework for Panoptic Segmentation of Images and Videos</a><br><a href="https://arxiv.org/abs/2210.06366"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.06096">Masked Motion Encoding for Self-Supervised Video Representation Learning</a> (CVPR 2023)<br><a href="https://github.com/XinyuSun/MME"><img src="https://img.shields.io/github/stars/XinyuSun/MME.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.06096"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.05861">SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models</a> (ICLR 2023)<br><a href="https://github.com/pairlab/SlotFormer"><img src="https://img.shields.io/github/stars/pairlab/SlotFormer.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.05861"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://slotformer.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.05825">Controllable Radiance Fields for Dynamic Face Synthesis</a> (3DV 2022)<br><a href="https://github.com/KelestZ/CoRF"><img src="https://img.shields.io/github/stars/KelestZ/CoRF.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.05825"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.05810">A unified model for continuous conditional video prediction</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2210.05810"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://npvp.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.04429">DeepHS-HDRVideo: Deep High Speed High Dynamic Range Video Reconstruction</a> (ICPR 2022)<br><a href="https://arxiv.org/abs/2210.04429"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.04154">Self-supervised Video Representation Learning with Motion-Aware Masked Autoencoders</a><br><a href="https://arxiv.org/abs/2210.04154"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/happy-hsy/MotionMAE"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.03692v1">Compressing Video Calls using Synthetic Talking Heads</a> (BMVC 2022)<br><a href="https://arxiv.org/abs/2210.03692v1"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://cvit.iiit.ac.in/research/projects/cvit-projects/talking-video-compression"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.02872">Text-driven Video Prediction</a><br><a href="https://arxiv.org/abs/2210.02872"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.02755">Audio-Visual Face Reenactment</a> (WACV 2023)<br><a href="https://github.com/mdv3101/AVFR-Gan"><img src="https://img.shields.io/github/stars/mdv3101/AVFR-Gan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.02755"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/avfr"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.02391">Geometry Driven Progressive Warping for One-Shot Face Animation</a><br><a href="https://arxiv.org/abs/2210.02391"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.01559">Cross-identity Video Motion Retargeting with Joint Transformation and Synthesis</a> (WACV 2023)<br><a href="https://github.com/nihaomiao/WACV23_TSNet"><img src="https://img.shields.io/github/stars/nihaomiao/WACV23_TSNet.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2210.01559"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.12475">Real-RawVSR: Real-World Raw Video Super-Resolution with a Benchmark Dataset</a> (ECCV 2022)<br><a href="https://github.com/zmzhang1998/Real-RawVSR"><img src="https://img.shields.io/github/stars/zmzhang1998/Real-RawVSR.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.12475"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.11224">VToonify: Controllable High-Resolution Portrait Video Style Transfer</a> (SIGGRAPH Asia 2022)<br><a href="https://github.com/williamyang1991/VToonify"><img src="https://img.shields.io/github/stars/williamyang1991/VToonify.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.11224"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.mmlab-ntu.com/project/vtoonify/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.11693">T3VIP: Transformation-based 3D Video Prediction</a> (IEEE)<br><a href="https://github.com/nematoli/t3vip"><img src="https://img.shields.io/github/stars/nematoli/t3vip.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.11693"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="http://t3vip.cs.uni-freiburg.de/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.08896">NeuralMarker: A Framework for Learning General Marker Correspondence</a> (SIGGRAPH Asia 2022)<br><a href="https://github.com/drinkingcoder/NeuralMarker"><img src="https://img.shields.io/github/stars/drinkingcoder/NeuralMarker.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.08896"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://drinkingcoder.github.io/publication/neuralmarker/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.08795">AutoLV: Automatic Lecture Video Generator</a><br><a href="https://arxiv.org/abs/2209.08795"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.youtube.com/watch?v=cY6TYkI0cog"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.08289">Continuously Controllable Facial Expression Editing in Talking Face Videos</a><br><a href="https://arxiv.org/abs/2209.08289"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.youtube.com/watch?v=WD-bNVya6kM"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.07923">A Deep Moving-camera Background Model</a> (ECCV 2022)<br><a href="https://github.com/BGU-CS-VIL/DeepMCBM"><img src="https://img.shields.io/github/stars/BGU-CS-VIL/DeepMCBM.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.07923"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.07143">HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator</a> (ICIP 2022)<br><a href="https://arxiv.org/abs/2209.07143"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sites.google.com/view/harp-videos/home"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.04252">Talking Head from Speech Audio using a Pre-trained Image Generator</a> (ACM Multimedia 2022)<br><a href="https://github.com/MohammedAlghamdi/talking-heads-acm-mm"><img src="https://img.shields.io/github/stars/MohammedAlghamdi/talking-heads-acm-mm.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.04252"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://mohammedalghamdi.github.io/talking-heads-acm-mm/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.03138">Treating Motion as Option to Reduce Motion Dependency in Unsupervised Video Object Segmentation</a> (WACV 2023)<br><a href="https://github.com/suhwan-cho/tmo"><img src="https://img.shields.io/github/stars/suhwan-cho/tmo.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.03138"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.01470">Neural Sign Reenactor: Deep Photorealistic Sign Language Retargeting</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2209.01470"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.youtube.com/watch?v=xKAfguacOkE"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.00475">REMOT: A Region-to-Whole Framework for Realistic Human Motion Transfer</a> (ACMMM 2022)<br><a href="https://arxiv.org/abs/2209.00475"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.00185">SketchBetween: Video-to-Video Synthesis for Sprite Animation via Sketches</a> (ACM conference on the Foundations of Digital Games)<br><a href="https://arxiv.org/abs/2209.00185"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.13717">StableFace: Analyzing and Improving Motion Stability for Talking Face Generation</a><br><a href="https://arxiv.org/abs/2208.13717"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://stable-face.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.12801">VMFormer: End-to-End Video Matting with Transformer</a><br><a href="https://github.com/SHI-Labs/VMFormer"><img src="https://img.shields.io/github/stars/SHI-Labs/VMFormer.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2208.12801"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://chrisjuniorli.github.io/project/VMFormer/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.11905">Neural Novel Actor: Learning a Generalized Animatable Neural Representation for Human Actors</a><br><a href="https://github.com/Talegqz/neural_novel_actor"><img src="https://img.shields.io/github/stars/Talegqz/neural_novel_actor.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2208.11905"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://talegqz.github.io/neural_novel_actor/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.10922">StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation</a><br><a href="https://arxiv.org/abs/2208.10922"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.09796">Towards MOOCs for Lipreading: Using Synthetic Talking Heads to Train Humans in Lipreading at Scale</a> (WACV 2023)<br><a href="https://arxiv.org/abs/2208.09796"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.09463">Temporal View Synthesis of Dynamic Scenes through 3D Object Motion Estimation with Multi-Plane Images</a> (ISMAR 2022)<br><a href="https://github.com/NagabhushanSN95/DeCOMPnet"><img src="https://img.shields.io/github/stars/NagabhushanSN95/DeCOMPnet.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2208.09463"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://nagabhushansn95.github.io/publications/2022/DeCOMPnet.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.09411">Wildfire Forecasting with Satellite Images and Deep Generative Model</a><br><a href="https://arxiv.org/abs/2208.09411"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.09127">Video Interpolation by Event-driven Anisotropic Adjustment of Optical Flow</a> (ECCV 2022)<br><a href="https://arxiv.org/abs/2208.09127"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.08118">Extreme-scale Talking-Face Video Upsampling with Audio-Visual Priors</a> (ACMMM 2022)<br><a href="https://github.com/Sindhu-Hegde/video-super-resolver"><img src="https://img.shields.io/github/stars/Sindhu-Hegde/video-super-resolver.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2208.08118"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/talking-face-video-upsampling"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.06807">Semi-Supervised Video Inpainting with Cycle Consistency Constraints</a><br><a href="https://arxiv.org/abs/2208.06807"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.06702">UAV-CROWD: Violent and non-violent crowd activity simulator from the perspective of UAV</a><br><a href="https://arxiv.org/abs/2208.06702"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.05701">Cine-AI: Generating Video Game Cutscenes in the Style of Human Directors</a> (ACMHCI)<br><a href="https://arxiv.org/abs/2208.05701"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.05617">Language-Guided Face Animation by Recurrent StyleGAN-based Generator</a><br><a href="https://github.com/researchmm/language-guided-animation"><img src="https://img.shields.io/github/stars/researchmm/language-guided-animation.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2208.05617"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.04303">Boosting neural video codecs by exploiting hierarchical redundancy</a><br><a href="https://arxiv.org/abs/2208.04303"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.03742">PS-NeRV: Patch-wise Stylized Neural Representations for Videos</a><br><a href="https://arxiv.org/abs/2208.03742"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.03244">Real-time Gesture Animation Generation from Speech for Virtual Human Interaction</a> (CHI EA 2021)<br><a href="https://github.com/mrebol/Gestures-From-Speech"><img src="https://img.shields.io/github/stars/mrebol/Gestures-From-Speech.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2208.03244"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.13670">Meta-Interpolation: Time-Arbitrary Frame Interpolation via Dual Meta-Learning</a><br><a href="https://arxiv.org/abs/2207.13670"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.13374">Efficient Video Deblurring Guided by Motion Magnitude</a> (ECCV 2022)<br><a href="https://github.com/sollynoay/MMP-RNN"><img src="https://img.shields.io/github/stars/sollynoay/MMP-RNN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.13374"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.12305">Error-Aware Spatial Ensembles for Video Frame Interpolation</a><br><a href="https://arxiv.org/abs/2207.12305"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.youtube.com/watch?v=_32GNANSr5U"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.11770">Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis</a> (ECCV 2022)<br><a href="https://github.com/sstzal/DFRF"><img src="https://img.shields.io/github/stars/sstzal/DFRF.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.11770"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sstzal.github.io/DFRF/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.11148">InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images</a> (ECCV 2022)<br><a href="https://arxiv.org/abs/2207.11148"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.11075">RealFlow: EM-based Realistic Optical Flow Dataset Generation from Videos</a> (ECCV 2022 Oral)<br><a href="https://github.com/megvii-research/RealFlow"><img src="https://img.shields.io/github/stars/megvii-research/RealFlow.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.11075"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.10765">Towards Interpretable Video Super-Resolution via Alternating Optimization</a> (ECCV 2022)<br><a href="https://github.com/caojiezhang/DAVSR"><img src="https://img.shields.io/github/stars/caojiezhang/DAVSR.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.10765"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.10391">Error Compensation Framework for Flow-Guided Video Inpainting</a> (ECCV 2022)<br><a href="https://github.com/JaeYeonKang/ECFVI"><img src="https://img.shields.io/github/stars/JaeYeonKang/ECFVI.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.10391"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.10123">Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance</a> (ECCV 2022)<br><a href="https://github.com/zzh-tech/Animation-from-Blur"><img src="https://img.shields.io/github/stars/zzh-tech/Animation-from-Blur.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.10123"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.09048">TTVFI: Learning Trajectory-Aware Transformer for Video Frame Interpolation</a> (CVPR 2022 Oral)<br><a href="https://github.com/researchmm/TTVSR"><img src="https://img.shields.io/github/stars/researchmm/TTVSR.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.09048"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.08813">Audio Input Generates Continuous Frames to Synthesize Facial Video Using Generative Adiversarial Networks</a><br><a href="https://arxiv.org/abs/2207.08813"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.06763">Neighbor Correspondence Matching for Flow-based Video Frame Synthesis</a> (ACMMM 2022)<br><a href="https://arxiv.org/abs/2207.06763"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.06345">You Only Align Once: Bidirectional Interaction for Spatial-Temporal Video Super-Resolution</a> (ACMMM 2022)<br><a href="https://arxiv.org/abs/2207.06345"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.05315">CANF-VC: Conditional Augmented Normalizing Flows for Video Compression</a><br><a href="https://github.com/NYCU-MAPL/CANF-VC"><img src="https://img.shields.io/github/stars/NYCU-MAPL/CANF-VC.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.05315"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.04566">A Probabilistic Model Of Interaction Dynamics for Dyadic Face-to-Face Settings</a><br><a href="https://arxiv.org/abs/2207.04566"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.04132">Cross-Attention Transformer for Video Interpolation</a><br><a href="https://arxiv.org/abs/2207.04132"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.03714">Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation</a><br><a href="https://arxiv.org/abs/2207.03714"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.02206">Segmenting Moving Objects via an Object-Centric Layered Representation</a> (NeurIPS 2022)<br><a href="https://github.com/jyxarthur/oclr_model"><img src="https://img.shields.io/github/stars/jyxarthur/oclr_model.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.02206"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.robots.ox.ac.uk/~vgg/research/oclr/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.13502">Programmatic Concept Learning for Human Motion Description and Synthesis</a> (CVPR 2022)<br><a href="https://arxiv.org/abs/2206.13502"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sumith1896.github.io/motion-concepts/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.13454">Optimizing Video Prediction via Video Frame Interpolation</a> (CVPR 2022)<br><a href="https://github.com/YueWuHKUST/CVPR2022-Optimizing-Video-Prediction-via-Video-Frame-Interpolation"><img src="https://img.shields.io/github/stars/YueWuHKUST/CVPR2022-Optimizing-Video-Prediction-via-Video-Frame-Interpolation.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.13454"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://yuewuhkust.github.io/OVP_VFI/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.12837">Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer</a> (ACMMM 2022)<br><a href="https://github.com/megvii-research/MM2022-ViCoPerceptualHeadGeneration"><img src="https://img.shields.io/github/stars/megvii-research/MM2022-ViCoPerceptualHeadGeneration.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.12837"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.11894">MaskViT: Masked Visual Pre-Training for Video Prediction</a><br><a href="https://github.com/agrimgupta92/maskvit"><img src="https://img.shields.io/github/stars/agrimgupta92/maskvit.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.11894"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://maskedvit.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.08572">Enhanced Bi-directional Motion Estimation for Video Frame Interpolation</a> (WACV 2023)<br><a href="https://github.com/srcn-ivl/EBME"><img src="https://img.shields.io/github/stars/srcn-ivl/EBME.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.08572"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.04523">Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos</a><br><a href="https://arxiv.org/abs/2206.04523"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.04381">STIP: A SpatioTemporal Information-Preserving and Perception-Augmented Model for High-Resolution Video Prediction</a> (CVPR 2022)<br><a href="https://github.com/ZhengChang467/STIPHR"><img src="https://img.shields.io/github/stars/ZhengChang467/STIPHR.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.04381"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.04231v2">JNMR: Joint Non-linear Motion Regression for Video Frame Interpolation</a><br><a href="https://arxiv.org/abs/2206.04231v2"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.05099">SimVP: Simpler yet Better Video Prediction</a> (CVPR 2022)<br><a href="https://github.com/A4Bio/SimVP-Simpler-yet-Better-Video-Prediction"><img src="https://img.shields.io/github/stars/A4Bio/SimVP-Simpler-yet-Better-Video-Prediction.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.05099"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.02146">Recurrent Video Restoration Transformer with Guided Deformable Attention</a> (NeurIPS 2022)<br><a href="https://github.com/JingyunLiang/RVRT"><img src="https://img.shields.io/github/stars/JingyunLiang/RVRT.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2206.02146"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.00735">Cascaded Video Generation for Videos In-the-Wild</a> (ICPR 2022)<br><a href="https://arxiv.org/abs/2206.00735"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.15838">D$^2$NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video</a><br><a href="https://github.com/ChikaYan/d2nerf"><img src="https://img.shields.io/github/stars/ChikaYan/d2nerf.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.15838"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.15361">TubeFormer-DeepLab: Video Mask Transformer</a> (CVPR 2022)<br><a href="https://arxiv.org/abs/2205.15361"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.14620">IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation</a> (CVPR 2022)<br><a href="https://github.com/ltkong218/IFRNet"><img src="https://img.shields.io/github/stars/ltkong218/IFRNet.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.14620"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.14574">Feature-Aligned Video Raindrop Removal with Temporal Constraints</a><br><a href="https://arxiv.org/abs/2205.14574"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.14022">Future Transformer for Long-term Action Anticipation</a> (CVPR 2022)<br><a href="https://github.com/gongda0e/FUTR"><img src="https://img.shields.io/github/stars/gongda0e/FUTR.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.14022"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="http://cvlab.postech.ac.kr/research/FUTR/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.13996">Video2StyleGAN: Disentangling Local and Global Variations in a Video</a><br><a href="https://arxiv.org/abs/2205.13996"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.10368">Automatic Generation of Synthetic Colonoscopy Videos for Domain Randomization</a><br><a href="https://arxiv.org/abs/2205.10368"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.10367">Latent-space disentanglement with untrained generator networks for the isolation of different motion types in video data</a><br><a href="https://github.com/hollerm/generator_based_motion_isolation"><img src="https://img.shields.io/github/stars/hollerm/generator_based_motion_isolation.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.10367"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.07230">Video Frame Interpolation with Transformer</a> (CVPR 2022)<br><a href="https://github.com/dvlab-research/VFIformer"><img src="https://img.shields.io/github/stars/dvlab-research/VFIformer.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.07230"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.06723">Multi-encoder Network for Parameter Reduction of a Kernel-based Interpolation Architecture</a> (NTIRE)<br><a href="https://arxiv.org/abs/2205.06723"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.05725">Diverse Video Generation from a Single Video</a> (CVPR 2022)<br><a href="https://arxiv.org/abs/2205.05725"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.05609">Video-ReTime: Learning Temporally Varying Speediness for Time Remapping</a> (AICC)<br><a href="https://arxiv.org/abs/2205.05609"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.05264">Spatial-Temporal Space Hand-in-Hand: Spatial-Temporal Video Super-Resolution via Cycle-Projected Mutual Learning</a><br><a href="https://github.com/hhhhhumengshun/SFI-STVR"><img src="https://img.shields.io/github/stars/hhhhhumengshun/SFI-STVR.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.05264"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.04519">Image2Gif: Generating Continuous Realistic Animations with Warping NODEs</a> (CVPR 2022)<br><a href="https://github.com/JurijsNazarovs/warping_node"><img src="https://img.shields.io/github/stars/JurijsNazarovs/warping_node.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.04519"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.03599">GAN-Based Multi-View Video Coding with Spatio-Temporal EPI Reconstruction</a><br><a href="https://arxiv.org/abs/2205.03599"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.02538v1">Parametric Reshaping of Portraits in Videos</a><br><a href="https://arxiv.org/abs/2205.02538v1"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.02084">Video Extrapolation in Space and Time</a> (ECCV 2022)<br><a href="https://github.com/zzyunzhi/vest"><img src="https://img.shields.io/github/stars/zzyunzhi/vest.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2205.02084"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://cs.stanford.edu/~yzzhang/projects/vest/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.01924">Zero-Episode Few-Shot Contrastive Predictive Coding: Solving intelligence tests without prior training</a><br><a href="https://arxiv.org/abs/2205.01924"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2205.01373">Copy Motion From One to Another: Fake Motion Video Generation</a><br><a href="https://arxiv.org/abs/2205.01373"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.14030">Neural Implicit Representations for Physical Parameter Inference from a Single Video</a> (WACV 2023)<br><a href="https://arxiv.org/abs/2204.14030"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.09951">Talking Head Generation Driven by Speech-Related Facial Action Units and Audio- Based on Multimodal Representation Fusion</a> (BMVC 2021)<br><a href="https://arxiv.org/abs/2110.09951"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.12151">ClothFormer:Taming Video Virtual Try-on in All Module</a> (CVPR 2022 Oral)<br><a href="https://github.com/luxiangju-PersonAI/ClothFormer"><img src="https://img.shields.io/github/stars/luxiangju-PersonAI/ClothFormer.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2204.12151"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://cloth-former.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.10321">Future Object Detection with Spatiotemporal Transformers</a><br><a href="https://github.com/atonderski/future-object-detection"><img src="https://img.shields.io/github/stars/atonderski/future-object-detection.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2204.10321"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.09456">STAU: A SpatioTemporal-Aware Unit for Video Prediction and Beyond</a> (TPAMI)<br><a href="https://arxiv.org/abs/2204.09456"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.09273">Sound-Guided Semantic Video Generation</a> (ECCV 2022)<br><a href="https://arxiv.org/abs/2204.09273"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://kuai-lab.github.io/eccv2022sound/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.08874">Less than Few: Self-Shot Video Instance Segmentation</a><br><a href="https://arxiv.org/abs/2204.08874"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.08451">Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion</a><br><a href="https://github.com/evonneng/learning2listen"><img src="https://img.shields.io/github/stars/evonneng/learning2listen.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2204.08451"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://evonneng.github.io/learning2listen/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.08058">MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration</a><br><a href="https://github.com/mugen-org/MUGEN_baseline"><img src="https://img.shields.io/github/stars/mugen-org/MUGEN_baseline.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2204.08058"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.06558">Controllable Video Generation through Global and Local Motion Dynamics</a><br><a href="https://arxiv.org/abs/2204.06558"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.06180">Dynamic Neural Textures: Generating Talking-Face Videos with Continuously Controllable Expressions</a><br><a href="https://arxiv.org/abs/2204.06180"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.06171">Self-Supervised Traffic Advisors: Distributed, Multi-view Traffic Prediction for Smart Cities</a> (ITSC)<br><a href="https://arxiv.org/abs/2204.06171"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.05018">Structure-Aware Motion Transfer with Deformable Anchor Model</a> (CVPR 2022)<br><a href="https://github.com/JialeTao/DAM"><img src="https://img.shields.io/github/stars/JialeTao/DAM.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2204.05018"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.04435">HSTR-Net: High Spatio-Temporal Resolution Video Generation For Wide Area Surveillance</a><br><a href="https://arxiv.org/abs/2204.04435"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.03648">SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage</a> (CVPR 2023)<br><a href="https://github.com/adobe-research/sunstage"><img src="https://img.shields.io/github/stars/adobe-research/sunstage.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2204.03648"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sunstage.cs.washington.edu/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.03513">Many-to-many Splatting for Efficient Video Frame Interpolation</a> (CVPR 2022)<br><a href="https://github.com/feinanshan/M2M_VFI"><img src="https://img.shields.io/github/stars/feinanshan/M2M_VFI.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2204.03513"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.02957">Video Demoireing with Relation-Based Temporal Consistency</a> (CVPR 2022)<br><a href="https://github.com/CVMI-Lab/VideoDemoireing"><img src="https://img.shields.io/github/stars/CVMI-Lab/VideoDemoireing.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2204.02957"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://daipengwa.github.io/VDmoire_ProjectPage/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.01218">Neural Rendering of Humans in Novel View and Pose from Monocular Video</a><br><a href="https://arxiv.org/abs/2204.01218"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.16875">MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images</a> (TPAMI 2022)<br><a href="https://github.com/gaoxiangjun/MPS-NeRF"><img src="https://img.shields.io/github/stars/gaoxiangjun/MPS-NeRF.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.16875"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.16490">Foveation-based Deep Video Compression without Motion Search</a><br><a href="https://arxiv.org/abs/2203.16490"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.16084">STRPM: A Spatiotemporal Residual Predictive Model for High-Resolution Video Prediction</a> (CVPR 2022)<br><a href="https://github.com/ZhengChang467/STIPHR"><img src="https://img.shields.io/github/stars/ZhengChang467/STIPHR.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.16084"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.15958">High-resolution Face Swapping via Latent Semantics Disentanglement</a> (CVPR 2022)<br><a href="https://github.com/cnnlstm/FSLSD_HiRes"><img src="https://img.shields.io/github/stars/cnnlstm/FSLSD_HiRes.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.15958"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.15836">VPTR: Efficient Transformers for Video Prediction</a> (ICPR 2022)<br><a href="https://github.com/XiYe20/VPTR"><img src="https://img.shields.io/github/stars/XiYe20/VPTR.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.15836"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.15427">Long-term Video Frame Interpolation via Feature Propagation</a> (CVPR 2022)<br><a href="https://arxiv.org/abs/2203.15427"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.15354">Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production</a><br><a href="https://arxiv.org/abs/2203.15354"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.15320">Dressing in the Wild by Watching Dance Videos</a> (CVPR 2022)<br><a href="https://arxiv.org/abs/2203.15320"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://awesome-wflow.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.14478">Structured Local Radiance Fields for Human Avatar Modeling</a> (CVPR 2022)<br><a href="https://github.com/ZhengZerong/THUman4.0-Dataset"><img src="https://img.shields.io/github/stars/ZhengZerong/THUman4.0-Dataset.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.14478"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="http://www.liuyebin.com/slrf/slrf.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.14074">V3GAN: Decomposing Background, Foreground and Motion for Video Generation</a><br><a href="https://arxiv.org/abs/2203.14074"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.12848">Keypoints Tracking via Transformer Networks</a><br><a href="https://github.com/LexaNagiBator228/Keypoints-Tracking-via-Transformer-Networks"><img src="https://img.shields.io/github/stars/LexaNagiBator228/Keypoints-Tracking-via-Transformer-Networks.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.12848"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.12602">VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training</a> (NeurIPS 2022)<br><a href="https://github.com/MCG-NJU/VideoMAE"><img src="https://img.shields.io/github/stars/MCG-NJU/VideoMAE.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.12602"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.12178">Unifying Motion Deblurring and Frame Interpolation with Events</a> (CVPR 2022)<br><a href="https://github.com/XiangZ-0/EVDI"><img src="https://img.shields.io/github/stars/XiangZ-0/EVDI.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.12178"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.11632">QS-Craft: Learning to Quantize, Scrabble and Craft for Conditional Human Motion Animation</a><br><a href="https://arxiv.org/abs/2203.11632"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.10821">Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields</a> (ECCV 2022)<br><a href="https://github.com/donydchen/sem2nerf"><img src="https://img.shields.io/github/stars/donydchen/sem2nerf.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.10821"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://donydchen.github.io/sem2nerf/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.10528">Stochastic Video Prediction with Structure and Motion</a> (TPAMI)<br><a href="https://arxiv.org/abs/2203.10528"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.10291">Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation</a><br><a href="https://arxiv.org/abs/2203.10291"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.09771">Beyond a Video Frame Interpolator: A Space Decoupled Learning Approach to Continuous Image Transition</a><br><a href="https://github.com/yangxy/SDL"><img src="https://img.shields.io/github/stars/yangxy/SDL.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.09771"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.09494">Transframer: Arbitrary Frame Prediction with Generative Models</a><br><a href="https://github.com/lucidrains/transframer-pytorch"><img src="https://img.shields.io/github/stars/lucidrains/transframer-pytorch.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.09494"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.09457">Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image</a> (CVPR 2022)<br><a href="https://github.com/xrenaa/Look-Outside-Room"><img src="https://img.shields.io/github/stars/xrenaa/Look-Outside-Room.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.09457"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://xrenaa.github.io/look-outside-room/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.09303">MSPred: Video Prediction at Multiple Spatio-Temporal Scales with Hierarchical Recurrent Networks</a><br><a href="https://github.com/AIS-Bonn/MSPred"><img src="https://img.shields.io/github/stars/AIS-Bonn/MSPred.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.09303"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sites.google.com/view/mspred/home"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.09043">Latent Image Animator: Learning to Animate Images via Latent Space Navigation</a> (ICLR 2022)<br><a href="https://github.com/wyhsirius/LIA"><img src="https://img.shields.io/github/stars/wyhsirius/LIA.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.09043"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://wyhsirius.github.io/LIA-project/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.07931">DialogueNeRF: Towards Realistic Avatar Face-to-face Conversation Video Generation</a><br><a href="https://arxiv.org/abs/2203.07931"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.06421">One-stage Video Instance Segmentation: From Frame-in Frame-out to Clip-in Clip-out</a><br><a href="https://arxiv.org/abs/2203.06421"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.05189">NeRFocus: Neural Radiance Field for 3D Synthetic Defocus</a><br><a href="https://github.com/wyhuai/NeRFocus"><img src="https://img.shields.io/github/stars/wyhuai/NeRFocus.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.05189"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.02723">A Novel Dual Dense Connection Network for Video Super-resolution</a><br><a href="https://arxiv.org/abs/2203.02723"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.01978">Region-of-Interest Based Neural Video Compression</a> (BMVC 2022)<br><a href="https://arxiv.org/abs/2203.01978"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2202.10758v1">Thinking the Fusion Strategy of Multi-reference Face Reenactment</a> (ICIP 2022)<br><a href="https://arxiv.org/abs/2202.10758v1"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2202.08418">Neural Marionette: Unsupervised Learning of Motion Skeleton and Latent Dynamics from Volumetric Video</a> (AAAI 2022)<br><a href="https://github.com/jinseokbae/neural_marionette"><img src="https://img.shields.io/github/stars/jinseokbae/neural_marionette.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2202.08418"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2202.07731">Enhancing Deformable Convolution based Video Frame Interpolation with Coarse-to-fine 3D CNN</a><br><a href="https://github.com/danier97/EDC"><img src="https://img.shields.io/github/stars/danier97/EDC.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2202.07731"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://danier97.github.io/EDC/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2202.07291">Exploring Discontinuity for Video Frame Interpolation</a> (CVPR 2023)<br><a href="https://github.com/pandatimo/Exploring-Discontinuity-for-VFI"><img src="https://img.shields.io/github/stars/pandatimo/Exploring-Discontinuity-for-VFI.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2202.07291"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2202.03046v1">A new face swap method for image and video domains: a technical report</a><br><a href="https://arxiv.org/abs/2202.03046v1"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2202.02183">Feature-Style Encoder for Style-Based GAN Inversion</a><br><a href="https://github.com/InterDigitalInc/FeatureStyleEncoder"><img src="https://img.shields.io/github/stars/InterDigitalInc/FeatureStyleEncoder.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2202.02183"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.13433">Third Time&#39;s the Charm? Image and Video Editing with StyleGAN3</a><br><a href="https://github.com/yuval-alaluf/stylegan3-editing"><img src="https://img.shields.io/github/stars/yuval-alaluf/stylegan3-editing.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2201.13433"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://yuval-alaluf.github.io/stylegan3-editing/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.11632">Deep Video Prior for Video Consistency and Propagation</a> (TPAMI 2021)<br><a href="https://github.com/ChenyangLEI/deep-video-prior"><img src="https://img.shields.io/github/stars/ChenyangLEI/deep-video-prior.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2201.11632"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.11407">Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions</a> (CLIC, CVPR 2022)<br><a href="https://github.com/saikatdutta/NME-VFI"><img src="https://img.shields.io/github/stars/saikatdutta/NME-VFI.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2201.11407"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.10075">Splatting-based Synthesis for Video Frame Interpolation</a> (WACV 2023)<br><a href="https://arxiv.org/abs/2201.10075"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="http://sniklaus.com/splatsyn"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.08361">Stitch it in Time: GAN-Based Facial Editing of Real Videos</a><br><a href="https://github.com/rotemtzaban/STIT"><img src="https://img.shields.io/github/stars/rotemtzaban/STIT.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2201.08361"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://stitch-time.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.07422">Self-Supervised Deep Blind Video Super-Resolution</a><br><a href="https://github.com/csbhr/Self-Blind-VSR"><img src="https://img.shields.io/github/stars/csbhr/Self-Blind-VSR.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2201.07422"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.06888">Autoencoding Video Latents for Adversarial Video Generation</a><br><a href="https://arxiv.org/abs/2201.06888"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.06494">AugLy: Data Augmentations for Robustness</a><br><a href="https://github.com/facebookresearch/AugLy"><img src="https://img.shields.io/github/stars/facebookresearch/AugLy.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2201.06494"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.06260">Towards Realistic Visual Dubbing with Heterogeneous Sources</a> (ACMMM 2021)<br><a href="https://arxiv.org/abs/2201.06260"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.05986">Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels</a> (IEEE)<br><a href="https://arxiv.org/abs/2201.05986"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.05723">Learning Temporally and Semantically Consistent Unpaired Video-to-video Translation Through Pseudo-Supervision From Synthetic Optical Flow</a> (AAAI 2022)<br><a href="https://github.com/wangkaihong/Unsup_Recycle_GAN/"><img src="https://img.shields.io/github/stars/Unsup_Recycle_GAN/.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2201.05723"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.04851">MetaDance: Few-shot Dancing Video Retargeting via Temporal-aware Meta-learning</a><br><a href="https://arxiv.org/abs/2201.04851"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://github.com/geyuying/MetaDance"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.03809">Music2Video: Automatic Generation of Music Video with fusion of audio and text</a><br><a href="https://github.com/joeljang/music2video"><img src="https://img.shields.io/github/stars/joeljang/music2video.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2201.03809"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.03808">MobileFaceSwap: A Lightweight Framework for Video Face Swapping</a> (AAAI 2022)<br><a href="https://github.com/Seanseattle/MobileFaceSwap"><img src="https://img.shields.io/github/stars/Seanseattle/MobileFaceSwap.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2201.03808"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.06820">Structured 3D Features for Reconstructing Controllable Avatars</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2212.06820"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://enriccorona.github.io/s3f/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2212.04495">MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis</a> (CVPR 2023)<br><a href="https://arxiv.org/abs/2212.04495"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://vcai.mpi-inf.mpg.de/projects/MoFusion/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.15064">High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors</a><br><a href="https://github.com/bbaaii/HFA-GP"><img src="https://img.shields.io/github/stars/bbaaii/HFA-GP.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2211.15064"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.14108">3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models</a><br><a href="https://arxiv.org/abs/2211.14108"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://3ddesigner-diffusion.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2211.10883">Audio-visual video face hallucination with frequency supervision and cross modality support by speech based lip reading loss</a><br><a href="https://arxiv.org/abs/2211.10883"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.05234">It Takes Two: Masked Appearance-Motion Modeling for Self-supervised Video Transformer Pre-training</a><br><a href="https://arxiv.org/abs/2210.05234"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2210.03825">See, Plan, Predict: Language-guided Cognitive Planning with Video Prediction</a><br><a href="https://arxiv.org/abs/2210.03825"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://see-pp.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2209.14024">Motion Transformer for Unsupervised Image Animation</a> (ECCV 2022)<br><a href="https://github.com/JialeTao/MoTrans"><img src="https://img.shields.io/github/stars/JialeTao/MoTrans.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2209.14024"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.11014">Low-Light Video Enhancement with Synthetic Event Guidance</a><br><a href="https://arxiv.org/abs/2208.11014"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2208.08728">Neural Capture of Animatable 3D Human from Monocular Video</a> (ECCV 2022)<br><a href="https://arxiv.org/abs/2208.08728"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.09193">NDF: Neural Deformable Fields for Dynamic Human Modelling</a> (ECCV 2022)<br><a href="https://github.com/HKBU-VSComputing/2022_ECCV_NDF"><img src="https://img.shields.io/github/stars/HKBU-VSComputing/2022_ECCV_NDF.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.09193"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.05906">Diverse Dance Synthesis via Keyframes with Transformer Controllers</a><br><a href="https://github.com/godzillalla/Dance-Synthesis-Project"><img src="https://img.shields.io/github/stars/godzillalla/Dance-Synthesis-Project.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2207.05906"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.15248">CTrGAN: Cycle Transformers GAN for Gait Transfer</a><br><a href="https://arxiv.org/abs/2206.15248"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2206.12657">Enhanced Deep Animation Video Interpolation</a><br><a href="https://arxiv.org/abs/2206.12657"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.06862">An Identity-Preserved Framework for Human Motion Transfer</a><br><a href="https://arxiv.org/abs/2204.06862"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2204.00795">Unsupervised Coherent Video Cartoonization with Perceptual Motion Consistency</a><br><a href="https://arxiv.org/abs/2204.00795"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2202.11855">Learning Multi-Object Dynamics with Compositional Neural Radiance Fields</a> (CoRL 2022)<br><a href="https://arxiv.org/abs/2202.11855"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://dannydriess.github.io/compnerfdyn/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2201.12288">VRT: A Video Restoration Transformer</a><br><a href="https://github.com/JingyunLiang/VRT"><img src="https://img.shields.io/github/stars/JingyunLiang/VRT.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2201.12288"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
</ul>
<br>


<h2>2021</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/2111.12417">N√úWA: Visual Synthesis Pre-training for Neural visUal World creAtion</a><br><a href="https://github.com/lucidrains/nuwa-pytorch"><img src="https://img.shields.io/github/stars/lucidrains/nuwa-pytorch.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.12417"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.microsoft.com/en-us/research/project/nuwa-infinity/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2110.11191">Generative Adversarial Graph Convolutional Networks for Human Action Synthesis</a> (WACV 2022)<br><a href="https://github.com/degardinbruno/kinetic-gan"><img src="https://img.shields.io/github/stars/degardinbruno/kinetic-gan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2110.11191"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2110.11894">Towards Using Clothes Style Transfer for Scenario-aware Person Video Generation</a><br><a href="https://github.com/xsimba123/demos-of-csf-sa"><img src="https://img.shields.io/github/stars/xsimba123/demos-of-csf-sa.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/pdf/2110.11894"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2203.09043">Latent Image Animator: Learning to animate image via latent space navigation</a> (ICLR 2022)<br><a href="https://github.com/wyhsirius/LIA"><img src="https://img.shields.io/github/stars/wyhsirius/LIA.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2203.09043"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://wyhsirius.github.io/LIA-project/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2108.02760">SLAMP: Stochastic Latent Appearance and Motion Prediction</a> (ICCV 2021)<br><a href="https://github.com/kaanakan/slamp"><img src="https://img.shields.io/github/stars/wyhsirius/LIA.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.02760"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://kuis-ai.github.io/slamp/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2108.04350">VirtualConductor: Music-driven Conducting Video Generation System</a> (ICME 2021)<br><a href="https://github.com/ChenDelong1999/VirtualConductor"><img src="https://img.shields.io/github/stars/ChenDelong1999/VirtualConductor.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.04350"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.08815">Click to Move: Controlling Video Generation with Sparse Motion</a> (ICCV 2021)<br><a href="https://github.com/PierfrancescoArdino/C2M"><img src="https://img.shields.io/github/stars/PierfrancescoArdino/C2M.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.08815"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.10157v2.pdf">VideoGPT: Video Generation using VQ-VAE and Transformers</a><br><a href="https://github.com/wilson1yan/VideoGPT"><img src="https://img.shields.io/github/stars/wilson1yan/VideoGPT.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/pdf/2104.10157v2.pdf"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://wilson1yan.github.io/videogpt/index.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2011.03864v3.pdf">Latent Neural Differential Equations for Video Generation</a><br><a href="https://github.com/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation"><img src="https://img.shields.io/github/stars/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/pdf/2011.03864v3.pdf"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Dorkenwald_Stochastic_Image-to-Video_Synthesis_Using_cINNs_CVPR_2021_paper.pdf">Stochastic Image-to-Video Synthesis Using cINNs</a> (CVPR 2021)<br><a href="https://github.com/CompVis/image2video-synthesis-using-cINNs"><img src="https://img.shields.io/github/stars/CompVis/image2video-synthesis-using-cINNs.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2105.04551"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://compvis.github.io/image2video-synthesis-using-cINNs/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Blattmann_Understanding_Object_Dynamics_for_Interactive_Image-to-Video_Synthesis_CVPR_2021_paper.pdf">Understanding Object Dynamics for Interactive Image-to-Video Synthesis</a> (CVPR 2021)<br><a href="https://github.com/CompVis/interactive-image2video-synthesis"><img src="https://img.shields.io/github/stars/CompVis/interactive-image2video-synthesis.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.11303"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://compvis.github.io/interactive-image2video-synthesis/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_One-Shot_Free-View_Neural_Talking-Head_Synthesis_for_Video_Conferencing_CVPR_2021_paper.pdf">One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing</a> (CVPR 2021)<br><a href="https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis"><img src="https://img.shields.io/github/stars/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2011.15126"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Flow_Guided_Transformable_Bottleneck_Networks_for_Motion_Retargeting_CVPR_2021_paper.pdf">Flow Guided Transformable Bottleneck Networks for Motion Retargeting</a> (CVPR 2021)<br><a href="https://arxiv.org/abs/2106.07771"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Riegler_Stable_View_Synthesis_CVPR_2021_paper.pdf">Stable View Synthesis</a> (CVPR 2021)<br><a href="https://github.com/isl-org/StableViewSynthesis"><img src="https://img.shields.io/github/stars/isl-org/StableViewSynthesis.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2011.07233"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Scene-Aware_Generative_Network_for_Human_Motion_Synthesis_CVPR_2021_paper.pdf">Scene-Aware Generative Network for Human Motion Synthesis</a> (CVPR 2021)<br><a href="https://arxiv.org/abs/2105.14804"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Neural_Scene_Flow_Fields_for_Space-Time_View_Synthesis_of_Dynamic_CVPR_2021_paper.pdf">Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes</a> (CVPR 2021)<br><a href="https://github.com/zhengqili/Neural-Scene-Flow-Fields"><img src="https://img.shields.io/github/stars/zhengqili/Neural-Scene-Flow-Fields.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2011.13084"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Siyao_Deep_Animation_Video_Interpolation_in_the_Wild_CVPR_2021_paper.pdf">Deep Animation Video Interpolation in the Wild</a> (CVPR 2021)<br><a href="https://github.com/lisiyao21/AnimeInterp"><img src="https://img.shields.io/github/stars/lisiyao21/AnimeInterp.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.02495"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Kappel_High-Fidelity_Neural_Human_Motion_Transfer_From_Monocular_Video_CVPR_2021_paper.pdf">High-Fidelity Neural Human Motion Transfer from Monocular Video</a> (CVPR 2021)<br><a href="https://github.com/MoritzKappel/HF-NHMT"><img src="https://img.shields.io/github/stars/MoritzKappel/HF-NHMT.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2012.10974"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Bei_Learning_Semantic-Aware_Dynamics_for_Video_Prediction_CVPR_2021_paper.pdf">Learning Semantic-Aware Dynamics for Video Prediction</a> (CVPR 2021)<br><a href="https://arxiv.org/abs/2104.09762"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Flow-Guided_One-Shot_Talking_Face_Generation_With_a_High-Resolution_Audio-Visual_Dataset_CVPR_2021_paper.pdf">Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset</a> (CVPR 2021)<br><a href="https://github.com/MRzzm/HDTF"><img src="https://img.shields.io/github/stars/MRzzm/HDTF.svg?style=social&label=Star" alt="Star"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Layout-Guided_Novel_View_Synthesis_From_a_Single_Indoor_Panorama_CVPR_2021_paper.pdf">Layout-Guided Novel View Synthesis From a Single Indoor Panorama</a> (CVPR 2021)<br><a href="https://github.com/bluestyle97/PNVS"><img src="https://img.shields.io/github/stars/bluestyle97/PNVS.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2103.17022"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Xian_Space-Time_Neural_Irradiance_Fields_for_Free-Viewpoint_Video_CVPR_2021_paper.pdf">Space-Time Neural Irradiance Fields for Free-Viewpoint Video</a> (CVPR 2021)<br><a href="https://arxiv.org/abs/2011.12950"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_GeoSim_Realistic_Video_Simulation_via_Geometry-Aware_Composition_for_Self-Driving_CVPR_2021_paper.pdf">GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving</a> (CVPR 2021)<br><a href="https://arxiv.org/abs/2101.06543"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://tmux.top/publication/geosim/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Holynski_Animating_Pictures_With_Eulerian_Motion_Fields_CVPR_2021_paper.pdf">Animating Pictures With Eulerian Motion Fields</a> (CVPR 2021)<br><a href="https://arxiv.org/abs/2011.15128"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://eulerian.cs.washington.edu/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2108.02760">SLAMP: Stochastic Latent Appearance and Motion Prediction</a> (ICCV 2021)<br><a href="https://github.com/kaanakan/slamp"><img src="https://img.shields.io/github/stars/wyhsirius/LIA.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.02760"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://kuis-ai.github.io/slamp/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2107.08037v2">CCVS: Context-aware Controllable Video Synthesis</a> (NeurIPS 2021)<br><a href="https://github.com/16lemoing/ccvs"><img src="https://img.shields.io/github/stars/16lemoing/ccvs.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2107.08037v2"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://16lemoing.github.io/ccvs/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2107.04619">Diverse Video Generation using a Gaussian Process Trigger</a> (ICLR 2021)<br><a href="https://github.com/shgaurav1/DVG"><img src="https://img.shields.io/github/stars/shgaurav1/DVG.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2107.04619"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2106.13195">FitVid: Overfitting in Pixel-Level Video Prediction</a><br><a href="https://github.com/google-research/fitvid"><img src="https://img.shields.io/github/stars/google-research/fitvid.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.13195"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2106.04283">NWT: Towards natural audio-to-video generation with representation learning</a><br><a href="https://github.com/lucidrains/NWT-pytorch"><img src="https://img.shields.io/github/stars/lucidrains/NWT-pytorch.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.04283"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://next-week-tonight.github.io/NWT_blog/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.14786">Editable Free-viewpoint Video Using a Layered Neural Representation</a><br><a href="https://github.com/darlinghang/st-nerf"><img src="https://img.shields.io/github/stars/darlinghang/st-nerf.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.14786"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://jiakai-zhang.github.io/st-nerf/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.15069">A Good Image Generator Is What You Need for High-Resolution Video Synthesis</a><br><a href="https://github.com/snap-research/MoCoGAN-HD"><img src="https://img.shields.io/github/stars/snap-research/MoCoGAN-HD.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.15069"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://bluer555.github.io/MoCoGAN-HD/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.14806">GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions</a><br><a href="https://github.com/mehdidc/DALLE_clip_score"><img src="https://img.shields.io/github/stars/mehdidc/DALLE_clip_score.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.14806"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.14631">Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme-Pose Dictionary</a><br><a href="https://github.com/sibozhang/Text2Video"><img src="https://img.shields.io/github/stars/sibozhang/Text2Video.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/pdf/2104.14631"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.11931">Adaptive Appearance Rendering</a><br><a href="https://github.com/wisdomdeng/AdaptiveRendering"><img src="https://img.shields.io/github/stars/wisdomdeng/AdaptiveRendering.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.11931"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.07995">Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation</a><br><a href="https://github.com/FuxiVirtualHuman/Write-a-Speaker"><img src="https://img.shields.io/github/stars/FuxiVirtualHuman/Write-a-Speaker.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.07995"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2103.01950">Predicting Video with VQVAE</a><br><a href="https://github.com/mattiasxu/Video-VQVAE"><img src="https://img.shields.io/github/stars/FuxiVirtualHuman/Write-a-Speaker.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2103.01950"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2101.12195">Playable Video Generation</a> (CVPR 2021)<br><a href="https://github.com/willi-menapace/PlayableVideoGeneration"><img src="https://img.shields.io/github/stars/FuxiVirtualHuman/Write-a-Speaker.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2101.12195"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://willi-menapace.github.io/playable-video-generation-website/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2012.09855">Infinite Nature: Perpetual View Generation of Natural Scenes from a Single Image</a> (ICCV 2021)<br><a href="https://github.com/google-research/google-research/tree/master/infinite_nature"><img src="https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2012.09855"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://infinite-nature.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2010.08188">Vid-ODE: Continuous-Time Video Generation with Neural Ordinary Differential Equation</a> (AAAI 2021)<br><a href="https://github.com/psh01087/Vid-ODE"><img src="https://img.shields.io/github/stars/psh01087/Vid-ODE.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2010.08188"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://psh01087.github.io/Vid-ODE/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2006.15327">Compositional Video Synthesis with Action Graphs</a> (ICML 2021)<br><a href="https://github.com/roeiherz/AG2Video"><img src="https://img.shields.io/github/stars/roeiherz/AG2Video.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2006.15327"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://research.nvidia.com/labs/par/publication/sg2vid.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2004.01823">Temporal Shift GAN for Large Scale Video Generation</a> (WACV 2021)<br><a href="https://github.com/amunozgarza/tsb-gan"><img src="https://img.shields.io/github/stars/amunozgarza/tsb-gan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2004.01823"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2102.06837">Learning Speech-driven 3D Conversational Gestures from Video</a><br><a href="https://arxiv.org/abs/2102.06837"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://vcai.mpi-inf.mpg.de/projects/3d_speech_driven_gesture/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2102.09883">SLPC: a VRNN-based approach for stochastic lidar prediction and completion in autonomous driving</a><br><a href="https://arxiv.org/abs/2102.09883"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2103.05669">Self-Supervision by Prediction for Object Discovery in Videos</a><br><a href="https://arxiv.org/abs/2103.05669"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.03960">Modulated Periodic Activations for Generalizable Local Functional Representations</a> (ICCV 2021)<br><a href="https://github.com/lucidrains/siren-pytorch"><img src="https://img.shields.io/github/stars/lucidrains/siren-pytorch.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.03960"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://ishit.github.io/modsine/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.05940">Dynamic Texture Synthesis by Incorporating Long-range Spatial and Temporal Correlations</a><br><a href="https://arxiv.org/abs/2104.05940"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2106.06561">GANs N&#39; Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)</a><br><a href="https://github.com/mchong6/GANsNRoses"><img src="https://img.shields.io/github/stars/mchong6/GANsNRoses.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.06561"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2106.12423">Alias-Free Generative Adversarial Networks</a> (NeurIPS 2021)<br><a href="https://github.com/NVlabs/stylegan3"><img src="https://img.shields.io/github/stars/NVlabs/stylegan3.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.12423"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://nvlabs.github.io/stylegan3/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2106.14879">Modeling Clothing as a Separate Layer for an Animatable Human Avatar</a><br><a href="https://arxiv.org/abs/2106.14879"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2107.00650">CLIP-It! Language-Guided Video Summarization</a> (NeurIPS 2021)<br><a href="https://github.com/medhini/clip_it"><img src="https://img.shields.io/github/stars/medhini/clip_it.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2107.00650"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://medhini.github.io/clip_it/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2107.07713">Towards an Interpretable Latent Space in Structured Models for Video Prediction</a><br><a href="https://arxiv.org/abs/2107.07713"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2108.04325">AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person</a><br><a href="https://arxiv.org/abs/2108.04325"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://xinshengwang.github.io/project/talking_head/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2108.06180">SPACE: A Simulator for Physical Interactions and Causal Learning in 3D Environments</a><br><a href="https://github.com/jiafei1224/space"><img src="https://img.shields.io/github/stars/jiafei1224/space.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.06180"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2109.04683">PIP: Physical Interaction Prediction via Mental Simulation with Span Selection</a><br><a href="https://github.com/SamsonYuBaiJian/pip"><img src="https://img.shields.io/github/stars/SamsonYuBaiJian/pip.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2109.04683"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2111.10337">Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions</a> (CVPR 2022)<br><a href="https://github.com/microsoft/xpretrain"><img src="https://img.shields.io/github/stars/microsoft/xpretrain.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.10337"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.13548">Responsive Listening Head Generation: A Benchmark Dataset and Baseline</a> (ECCV 2022)<br><a href="https://arxiv.org/abs/2112.13548"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.12761">BANMo: Building Animatable 3D Neural Models from Many Casual Videos</a> (CVPR 2022)<br><a href="https://github.com/facebookresearch/banmo"><img src="https://img.shields.io/github/stars/facebookresearch/banmo.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.12761"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://banmo-www.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.10960">Continuous-Time Video Generation via Learning Motion Dynamics with Neural ODE</a> (BMVC 2021)<br><a href="https://arxiv.org/abs/2112.10960"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://psh01087.github.io/MODE-GAN/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.10457">Image Animation with Keypoint Mask</a><br><a href="https://github.com/or-toledano/animation-with-keypoint-mask"><img src="https://img.shields.io/github/stars/or-toledano/animation-with-keypoint-mask.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.10457"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.10103">SAGA: Stochastic Whole-Body Grasping with Contact</a> (ECCV 2022)<br><a href="https://github.com/JiahaoPlus/SAGA"><img src="https://img.shields.io/github/stars/JiahaoPlus/SAGA.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.10103"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://jiahaoplus.github.io/SAGA/saga.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.09875">Adversarial Memory Networks for Action Prediction</a><br><a href="https://arxiv.org/abs/2112.09875"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.09529v1">End-to-End Rate-Distortion Optimized Learned Hierarchical Bi-Directional Video Compression</a><br><a href="https://github.com/KUIS-AI-Tekalp-Research-Group/video-compression"><img src="https://img.shields.io/github/stars/KUIS-AI-Tekalp-Research-Group/video-compression.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.09529"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.09379">Enhanced Frame and Event-Based Simulator and Event-Based Video Interpolation Network</a><br><a href="https://arxiv.org/abs/2112.09379"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.08913">Discrete neural representations for explainable anomaly detection</a> (AAAI 2022)<br><a href="https://github.com/KT27-A/CSTP"><img src="https://img.shields.io/github/stars/KT27-A/CSTP.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.08913"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.03051">Controllable Animation of Fluid Elements in Still Images</a> (CVPR 2022)<br><a href="https://arxiv.org/abs/2112.03051"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://controllable-cinemagraphs.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.02749">One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning</a> (AAAI 2022)<br><a href="https://github.com/FuxiVirtualHuman/AAAI22-one-shot-talking-face"><img src="https://img.shields.io/github/stars/FuxiVirtualHuman/AAAI22-one-shot-talking-face.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.02749"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2112.01517">Efficient Neural Radiance Fields for Interactive Free-viewpoint Video</a> (SIGGRAPH Asia 2022)<br><a href="https://github.com/zju3dv/enerf"><img src="https://img.shields.io/github/stars/zju3dv/enerf.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.01517"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://zju3dv.github.io/enerf/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2112.01473">Neural Point Light Fields</a> (CVPR 2022)<br><a href="https://github.com/princeton-computational-imaging/neural-point-light-fields"><img src="https://img.shields.io/github/stars/princeton-computational-imaging/neural-point-light-fields.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.01473"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2112.01161">Video Frame Interpolation without Temporal Priors</a> (NeurIPS 2020)<br><a href="https://github.com/yjzhang96/UTI-VFI"><img src="https://img.shields.io/github/stars/yjzhang96/UTI-VFI.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2112.01161"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.15483">ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation</a> (CVPR 2022)<br><a href="https://github.com/danier97/ST-MFNet"><img src="https://img.shields.io/github/stars/danier97/ST-MFNet.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.15483"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://danielism97.github.io/ST-MFNet"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.13817">Video Frame Interpolation Transformer</a> (CVPR 2022)<br><a href="https://github.com/dvlab-research/vfiformer"><img src="https://img.shields.io/github/stars/dvlab-research/vfiformer.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.13817"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.12792">Improving the Perceptual Quality of 2D Animation Interpolation</a> (ECCV 2022)<br><a href="https://github.com/shuhongchen/eisai-anime-interpolator"><img src="https://img.shields.io/github/stars/shuhongchen/eisai-anime-interpolator.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.12792"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.12747">Layered Controllable Video Generation</a> (ECCV 2022)<br><a href="https://github.com/Gabriel-Huang/Layered-Controllable-Video-Generation"><img src="https://img.shields.io/github/stars/Gabriel-Huang/Layered-Controllable-Video-Generation.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.12792"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://gabriel-huang.github.io/layered_controllable_video_generation/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.12731">Human Pose Manipulation and Novel View Synthesis using Differentiable Rendering</a><br><a href="https://github.com/guillaumerochette/humanviewsynthesis"><img src="https://img.shields.io/github/stars/guillaumerochette/humanviewsynthesis.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.12792"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.12301">Two-stage Rule-induction Visual Reasoning on RPMs with an Application to Video Prediction</a><br><a href="https://arxiv.org/abs/2111.12301"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="http://export.arxiv.org/abs/2111.10916">Video Content Swapping Using GAN</a><br><a href="http://export.arxiv.org/abs/2111.10916"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.10533">Temporal-MPI: Enabling Multi-Plane Images for Dynamic Scene Modelling via Temporal Basis Learning</a> (ECCV 2022)<br><a href="https://github.com/HKBU-VSComputing/2022_ECCV_Temporal-MPI"><img src="https://img.shields.io/github/stars/HKBU-VSComputing/2022_ECCV_Temporal-MPI.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.10533"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.10233">Xp-GAN: Unsupervised Multi-object Controllable Video Generation</a><br><a href="https://arxiv.org/abs/2111.10233"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.06925">Action2video: Generating Videos of Human 3D Actions</a> (IJCV 2022)<br><a href="https://arxiv.org/abs/2111.06925"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://vision-and-learning-lab-ualberta.github.io/post/chuan_ijcv_2022/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.05916">Dance In the Wild: Monocular Human Animation with Neural Dynamic Appearance Synthesis</a><br><a href="https://arxiv.org/abs/2111.05916"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.05916">LUMINOUS: Indoor Scene Generation for Embodied AI Challenges</a><br><a href="https://github.com/amazon-science/indoor-scene-generation-eai"><img src="https://img.shields.io/github/stars/amazon-science/indoor-scene-generation-eai.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.05916"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.01105">FREGAN : an application of generative adversarial networks in enhancing the frame rate of videos</a><br><a href="https://arxiv.org/abs/2111.01105"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.01029">Render In-between: Motion Guided Video Synthesis for Action Interpolation</a><br><a href="https://github.com/azuxmioy/Render-In-Between"><img src="https://img.shields.io/github/stars/azuxmioy/Render-In-Between.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.01029"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.00203">Imitating Arbitrary Talking Style for Realistic Audio-DrivenTalking Face Synthesis</a> (MM 2021)<br><a href="https://github.com/wuhaozhe/style_avatar"><img src="https://img.shields.io/github/stars/wuhaozhe/style_avatar.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.00203"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.00203">TaylorSwiftNet: Taylor Driven Temporal Modeling for Swift Future Frame Prediction</a><br><a href="https://arxiv.org/abs/2111.00203"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.14147">Image Comes Dancing with Collaborative Parsing-Flow Video Synthesis</a> (TIP 2021)<br><a href="https://arxiv.org/abs/2110.14147"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.13746">H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion</a><br><a href="https://arxiv.org/abs/2110.13746"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.11795">HDRVideo-GAN: Deep Generative HDR Video Reconstruction</a> (ICVGIP 2021)<br><a href="https://arxiv.org/abs/2110.11795"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.11746">Creating and Reenacting Controllable 3D Humans with Differentiable Rendering</a> (WACV 2022)<br><a href="https://github.com/verlab/CreatingAndReenacting_WACV_2022"><img src="https://img.shields.io/github/stars/wuhaozhe/style_avatar.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2110.11746"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.11586">Wide and Narrow: Video Prediction from Context and Motion</a><br><a href="https://arxiv.org/abs/2110.11586"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2111.12792">MUGL: Large Scale Multi Person Conditional Action Generation with Locomotion</a> (WACV 2022)<br><a href="https://github.com/skelemoa/mugl"><img src="https://img.shields.io/github/stars/skelemoa/mugl.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2111.12792"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://skeleton.iiit.ac.in/mugl"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.11236">LARNet: Latent Action Representation for Human Action Synthesis</a> (ICLR 2022)<br><a href="https://arxiv.org/abs/2110.11236"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://vpr-model.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.09951">Talking Head Generation with Audio and Speech Related Facial Action Units</a> (BMVC 2021)<br><a href="https://arxiv.org/abs/2110.09951"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.09936">NeuralDiff: Segmenting 3D objects that move in egocentric videos</a> (3DV 2021)<br><a href="https://github.com/dichotomies/NeuralDiff"><img src="https://img.shields.io/github/stars/dichotomies/NeuralDiff.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2110.09936"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.robots.ox.ac.uk/~vadim/neuraldiff/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.08580">Intelligent Video Editing: Incorporating Modern Talking Face Generation Algorithms in a Video Editor</a> (ICVGIP 2021)<br><a href="https://arxiv.org/abs/2110.08580"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.07993">Pose-guided Generative Adversarial Net for Novel View Action Synthesis</a> (WACV 2022)<br><a href="https://arxiv.org/abs/2110.07993"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.05881">Fourier-based Video Prediction through Relational Object Motion</a><br><a href="https://arxiv.org/abs/2110.05881"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.04902">Synthetic Data for Multi-Parameter Camera-Based Physiological Sensing</a><br><a href="https://arxiv.org/abs/2110.04902"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.04710">Sketch Me A Video</a><br><a href="https://arxiv.org/abs/2110.04710"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.02951">Video Autoencoder: self-supervised disentanglement of static 3D structure and motion</a> (ICCV 2021)<br><a href="https://github.com/zlai0/VideoAutoencoder"><img src="https://img.shields.io/github/stars/zlai0/VideoAutoencoder.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2110.02951"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://zlai0.github.io/VideoAutoencoder/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.03446">A Hierarchical Variational Neural Uncertainty Model for Stochastic Video Prediction</a><br><a href="https://arxiv.org/abs/2110.03446"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2110.00547">Self-Supervised Decomposition, Disentanglement and Prediction of Video Sequences while Interpreting Dynamics: A Koopman Perspective</a><br><a href="https://arxiv.org/abs/2110.00547"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.12581">A Stacking Ensemble Approach for Supervised Video Summarization</a><br><a href="https://arxiv.org/abs/2109.12581"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.09913">Physics-based Human Motion Estimation and Synthesis from Videos</a> (ICCV 2021)<br><a href="https://arxiv.org/abs/2109.09913"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://nv-tlabs.github.io/physics-pose-estimation-project-page/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.08809">HYouTube: Video Harmonization Dataset</a> (Datasets)<br><a href="https://github.com/bcmi/video-harmonization-dataset-hyoutube"><img src="https://img.shields.io/github/stars/bcmi/video-harmonization-dataset-hyoutube.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2109.08809"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.08591">Diverse Generation from a Single Video Made Possible</a><br><a href="https://github.com/nivha/single_video_generation"><img src="https://img.shields.io/github/stars/nivha/single_video_generation.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2109.08591"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://nivha.github.io/vgpnn/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.07448">Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering</a><br><a href="https://github.com/YoungJoongUNC/Neural_Human_Performer"><img src="https://img.shields.io/github/stars/YoungJoongUNC/Neural_Human_Performer.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2109.08591"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://youngjoongunc.github.io/nhp/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.05864">Conditional MoCoGAN for Zero-Shot Video Generation</a><br><a href="https://arxiv.org/abs/2109.05864"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.04843">Temporally Coherent Person Matting Trained on Fake-Motion Dataset</a><br><a href="https://arxiv.org/abs/2109.04843"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://videoprocessing.github.io/person-matting"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.03292">Simple Video Generation using Neural ODEs</a><br><a href="https://arxiv.org/abs/2109.03292"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://voletiv.github.io/docs/presentations/20191213_Vancouver_NeurIPSW_EncODEDec.pdf"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.02625">ERA: Entity Relationship Aware Video Summarization with Wasserstein GAN</a><br><a href="https://github.com/jnzs1836/era-vsum"><img src="https://img.shields.io/github/stars/bcmi/video-harmonization-dataset-hyoutube.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2109.02625"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.02216">Learning Fine-Grained Motion Embedding for Landscape Animation</a> (ACM Multimedia 2021)<br><a href="https://arxiv.org/abs/2109.02216"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.02081">Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis</a><br><a href="https://arxiv.org/abs/2109.02081"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.00471">Sparse to Dense Motion Transfer for Face Image Animation</a> (ICCV 2021)<br><a href="https://github.com/fangchangma/sparse-to-dense"><img src="https://img.shields.io/github/stars/fangchangma/sparse-to-dense.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2109.00471"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.13408">View Synthesis of Dynamic Scenes based on Deep 3D Mask Volume</a> (ICCV 2021)<br><a href="https://github.com/ken2576/deep-3dmask"><img src="https://img.shields.io/github/stars/ken2576/deep-3dmask.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.13408"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://cseweb.ucsd.edu//~viscomp/projects/ICCV21Deep/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.12845">Flow-Guided Video Inpainting with Scene Templates</a> (ICCV 2021)<br><a href="https://github.com/donglao/videoinpainting"><img src="https://img.shields.io/github/stars/donglao/videoinpainting.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.12845"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.08121">Target Adaptive Context Aggregation for Video Scene Graph Generation</a> (ICCV 2021)<br><a href="https://github.com/mcg-nju/trace"><img src="https://img.shields.io/github/stars/mcg-nju/trace.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.08121"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.07938">FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning</a> (ICCV 2021)<br><a href="https://github.com/zhangchenxu528/FACIAL"><img src="https://img.shields.io/github/stars/zhangchenxu528/FACIAL.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.07938"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.06815">Asymmetric Bilateral Motion Estimation for Video Frame Interpolation</a> (ICCV 2021)<br><a href="https://github.com/junheum/abme"><img src="https://img.shields.io/github/stars/junheum/abme.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.06815"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.06765">Occlusion-Aware Video Object Inpainting</a> (ICCV 2021)<br><a href="https://arxiv.org/abs/2108.06765"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.05658">Conditional Temporal Variational AutoEncoder for Action Video Prediction</a> (ECCV 2018)<br><a href="https://github.com/yccyenchicheng/pytorch-VideoVAE"><img src="https://img.shields.io/github/stars/yccyenchicheng/pytorch-VideoVAE.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.06815"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.05650">UniFaceGAN: A Unified Framework for Temporally Consistent Facial Video Editing</a> (IEEE TIP 2021IEEE TIP 2021)<br><a href="https://arxiv.org/abs/2108.05650"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.05577">iButter: Neural Interactive Bullet Time Generator for Human Free-viewpoint Rendering</a> (ACM MM 2021)<br><a href="https://arxiv.org/abs/2108.05577"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://aoliao12138.github.io/iButter/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.04913">FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face Animation</a><br><a href="https://arxiv.org/abs/2108.04913"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://shahrukhathar.github.io/2021/08/12/FLAMEinNeRF.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.04294">Learning to Cut by Watching Movies</a> (ICCV 2021)<br><a href="https://github.com/PardoAlejo/LearningToCut"><img src="https://img.shields.io/github/stars/PardoAlejo/LearningToCut.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.04294"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://www.alejandropardo.net/publication/learning-to-cut/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.03132">RockGPT: Reconstructing three-dimensional digital rocks from single two-dimensional slice from the perspective of video generation</a><br><a href="https://arxiv.org/abs/2108.03132"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2108.00913">I2V-GAN: Unpaired Infrared-to-Visible Video Translation</a> (ACM MM 2021)<br><a href="https://github.com/BIT-DA/I2V-GAN"><img src="https://img.shields.io/github/stars/BIT-DA/I2V-GAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2108.00913"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.13766">Video Generation from Text Employing Latent Path Construction for Temporal Modeling</a><br><a href="https://arxiv.org/abs/2107.13766"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.09293">Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion</a><br><a href="https://github.com/wangsuzhen/Audio2Head"><img src="https://img.shields.io/github/stars/BIT-DA/I2V-GAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2107.09293"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.09240">Generative Video Transformer: Can Objects be the Words?</a><br><a href="https://arxiv.org/abs/2107.09240"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.07224">StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN</a><br><a href="https://arxiv.org/abs/2107.07224"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.05113">LiveView: Dynamic Target-Centered MPI for View Synthesis</a><br><a href="https://arxiv.org/abs/2107.05113"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.04806">Speech2Video: Cross-Modal Distillation for Speech to Video Generation</a> (ACCV 2020)<br><a href="https://github.com/sibozhang/Speech2Video"><img src="https://img.shields.io/github/stars/sibozhang/Speech2Video.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2107.04806"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sites.google.com/view/sibozhang/speech2video"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.03120">Cross-View Exocentric to Egocentric Video Synthesis</a> (ACM MM 2021)<br><a href="https://arxiv.org/abs/2107.03120"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.03109">Egocentric Videoconferencing</a><br><a href="https://arxiv.org/abs/2107.03109"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="http://gvv.mpi-inf.mpg.de/projects/EgoChat/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.02790">iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis</a> (ICCV 2021)<br><a href="https://github.com/CompVis/ipoke"><img src="https://img.shields.io/github/stars/sibozhang/Speech2Video.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2107.02790"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://compvis.github.io/ipoke/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.14132">Robust Pose Transfer with Dynamic Details using Neural Video Rendering</a> (ICCV 2021)<br><a href="https://github.com/SunYangtian/Neural-Human-Video-Rendering"><img src="https://img.shields.io/github/stars/SunYangtian/Neural-Human-Video-Rendering.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.14132"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.09051">Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure</a><br><a href="https://arxiv.org/abs/2106.09051"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.08318">Gradient Forward-Propagation for Large-Scale Temporal Video Modelling</a> (CVPR 2021)<br><a href="https://arxiv.org/abs/2106.08318"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.05658">Conditional COT-GAN for Video Prediction with Kernel Smoothing</a><br><a href="https://github.com/neuripss2020/kccotgan"><img src="https://img.shields.io/github/stars/neuripss2020/kccotgan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.05658"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.04185">LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization</a> (CVPR 2021)<br><a href="https://arxiv.org/abs/2106.04185"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.04004">Task-Generic Hierarchical Human Motion Prior using VAEs</a><br><a href="https://arxiv.org/abs/2106.04004"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.03956">Novel View Video Prediction Using a Dual Representation</a> (ICIP 2021)<br><a href="https://github.com/google/stereo-magnification"><img src="https://img.shields.io/github/stars/google/stereo-magnification.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.03956"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.03502">Efficient training for future video generation based on hierarchical disentangled representation of latent variables</a><br><a href="https://arxiv.org/abs/2106.03502"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.02719">Hierarchical Video Generation for Complex Data</a><br><a href="https://arxiv.org/abs/2106.02719"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.02328">Temporally coherent video anonymization through GAN inpainting</a> (FG 2021)<br><a href="https://arxiv.org/abs/2106.02328"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.02036">Anticipative Video Transformer</a> (ICCV 2021)<br><a href="https://github.com/facebookresearch/AVT"><img src="https://img.shields.io/github/stars/facebookresearch/AVT.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.02036"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://facebookresearch.github.io/AVT/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.02019">Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control</a> (SIGGRAPH Asia 2021)<br><a href="https://github.com/lingjie0206/Neural_Actor_Main_Code"><img src="https://img.shields.io/github/stars/lingjie0206/Neural_Actor_Main_Code.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2106.02019"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://vcai.mpi-inf.mpg.de/projects/NeuralActor/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.14678">Image-to-Video Generation via 3D Facial Dynamics</a><br><a href="https://arxiv.org/abs/2105.14678"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.13016">Stylizing 3D Scene via Implicit Representation and HyperNetwork</a> (WACV 2022)<br><a href="https://github.com/ztex08010518/Stylizing-3D-Scene"><img src="https://img.shields.io/github/stars/ztex08010518/Stylizing-3D-Scene.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2105.13016"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://ztex08010518.github.io/3dstyletransfer/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.07673">EA-Net: Edge-Aware Network for Flow-based Video Frame Interpolation</a><br><a href="https://arxiv.org/abs/2105.07673"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.04637">Local Frequency Domain Transformer Networks for Video Prediction</a><br><a href="https://github.com/AIS-Bonn/Local_Freq_Transformer_Net"><img src="https://img.shields.io/github/stars/AIS-Bonn/Local_Freq_Transformer_Net.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2105.04637"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.04551">Stochastic Image-to-Video Synthesis using cINNs</a> (CVPR 2021)<br><a href="https://github.com/CompVis/image2video-synthesis-using-cINNs"><img src="https://img.shields.io/github/stars/CompVis/image2video-synthesis-using-cINNs.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2105.04551"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://compvis.github.io/image2video-synthesis-using-cINNs/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.04066">Reconstructive Sequence-Graph Network for Video Summarization</a> (IEEE TPAMI 2021)<br><a href="https://arxiv.org/abs/2105.04066"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.02799">Object-centric Video Prediction without Annotation</a><br><a href="https://github.com/kschmeckpeper/opa"><img src="https://img.shields.io/github/stars/kschmeckpeper/opa.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2105.02799"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.02742">Pose-Guided Sign Language Video GAN with Dynamic Lambda</a><br><a href="https://arxiv.org/abs/2105.02742"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.02195">Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid Scenes</a><br><a href="https://arxiv.org/abs/2105.02195"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.12357">VCGAN: Video Colorization with Hybrid Generative Adversarial Network</a> (IEEE (TMM)2021)<br><a href="https://github.com/zhaoyuzhi/VCGAN"><img src="https://img.shields.io/github/stars/zhaoyuzhi/VCGAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.12357"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.12051">3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head</a><br><a href="https://arxiv.org/abs/2104.12051"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.11216">Hierarchical Motion Understanding via Motion Programs</a> (CVPR 2021)<br><a href="https://github.com/Sumith1896/motion2prog_release"><img src="https://img.shields.io/github/stars/Sumith1896/motion2prog_release.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.11216"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://sumith1896.github.io/motion2prog/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.11116">Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation</a> (CVPR 2021)<br><a href="https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS"><img src="https://img.shields.io/github/stars/Hangz-nju-cuhk/Talking-Face_PC-AVS.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.11116"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://hangz-nju-cuhk.github.io/projects/PC-AVS"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.09762">Learning Semantic-Aware Dynamics for Video Prediction</a> (CVPR 2021)<br><a href="https://arxiv.org/abs/2104.09762"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.09762">MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement</a> (CVPR 2021)<br><a href="https://github.com/facebookresearch/meshtalk"><img src="https://img.shields.io/github/stars/facebookresearch/meshtalk.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.09762"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.07473">Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video Super-Resolution</a> (CVPR 2020)<br><a href="https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020"><img src="https://img.shields.io/github/stars/Mukosame/Zooming-Slow-Mo-CVPR-2020.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.07473"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.07452">Audio-Driven Emotional Video Portraits</a> (CVPR 2021)<br><a href="https://github.com/jixinya/EVP"><img src="https://img.shields.io/github/stars/jixinya/EVP.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.07452"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://jixinya.github.io/projects/evp/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.06697">Revisiting Hierarchical Approach for Persistent Long-Term Video Prediction</a> (ICLR 2021)<br><a href="https://github.com/1Konny/HVP"><img src="https://img.shields.io/github/stars/1Konny/HVP.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.06697"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.02687">Strumming to the Beat: Audio-Conditioned Contrastive Video Textures</a> (WACV 2022)<br><a href="https://github.com/medhini/audio-video-textures"><img src="https://img.shields.io/github/stars/medhini/audio-video-textures.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.02687"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://medhini.github.io/audio_video_textures/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.01517">PDWN: Pyramid Deformable Warping Network for Video Interpolation</a><br><a href="https://github.com/zhiqiiiiiii/PDWN_for_Video_Interp"><img src="https://img.shields.io/github/stars/zhiqiiiiiii/PDWN_for_Video_Interp.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.01517"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.01122">M3L: Language-based Video Editing via Multi-Modal Multi-Level Transformers</a> (CVPR 2022)<br><a href="https://arxiv.org/abs/2104.01122"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.00924">Video Prediction Recalling Long-term Motion Context via Memory Alignment Learning</a> (CVPR 2021)<br><a href="https://github.com/sangmin-git/LMC-Memory"><img src="https://img.shields.io/github/stars/zhiqiiiiiii/PDWN_for_Video_Interp.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.00924"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.02656">Collaborative Learning to Generate Audio-Video Jointly</a> (ICASSP 2021)<br><a href="https://github.com/DelTA-Lab-IITK/AVG"><img src="https://img.shields.io/github/stars/DelTA-Lab-IITK/AVG.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2104.02656"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://delta-lab-iitk.github.io/AVG/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.17204">Long-Term Temporally Consistent Unpaired Video Translation from Simulated Surgical 3D Data</a> (IJCV 2021)<br><a href="https://github.com/verlab/ShapeAwareHumanRetargeting_IJCV_2021"><img src="https://img.shields.io/github/stars/verlab/ShapeAwareHumanRetargeting_IJCV_2021.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2103.17204"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://verlab.github.io/ShapeAwareHumanRetargeting_IJCV_2021/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.15596">A Shape-Aware Retargeting Approach to Transfer Human Motion and Appearance in Monocular Videos</a> (IJCV 2021)<br><a href="https://arxiv.org/abs/2103.15596"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.11078">AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</a> (ICCV 2021)<br><a href="https://github.com/YudongGuo/AD-NeRF"><img src="https://img.shields.io/github/stars/YudongGuo/AD-NeRF.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2103.11078"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://yudongguo.github.io/ADNeRF/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.10308">Future Frame Prediction for Robot-assisted Surgery</a> (IPMI 2021)<br><a href="https://arxiv.org/abs/2103.10308"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.05842">Learning to compose 6-DoF omnidirectional videos using multi-sphere images</a><br><a href="https://arxiv.org/abs/2103.05842"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.04677">Behavior-Driven Synthesis of Human Dynamics</a> (CVPR 2021)<br><a href="https://github.com/CompVis/behavior-driven-video-synthesis"><img src="https://img.shields.io/github/stars/CompVis/behavior-driven-video-synthesis.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2103.04677"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://compvis.github.io/behavior-driven-video-synthesis/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.04174">Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction</a><br><a href="https://arxiv.org/abs/2103.04174"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.02984">Motion-blurred Video Interpolation and Extrapolation</a> (AAAI 2021)<br><a href="https://arxiv.org/abs/2103.02984"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.02597">Neural 3D Video Synthesis from Multi-view Video</a> (CVPR 2022)<br><a href="https://github.com/facebookresearch/neural_3d_video"><img src="https://img.shields.io/github/stars/facebookresearch/neural_3d_video.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2103.02597"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a><br><a href="https://neural-3d-video.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.02243">MotionRNN: A Flexible Model for Video Prediction with Spacetime-Varying Motions</a> (CVPR 2021)<br><a href="https://github.com/thuml/MotionRNN"><img src="https://img.shields.io/github/stars/facebookresearch/neural_3d_video.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2103.02243"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2102.13329">Dual-MTGAN: Stochastic and Deterministic Motion Transfer for Image-to-Video Synthesis</a> (ICPR 2020)<br><a href="https://arxiv.org/abs/2102.13329"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2102.09737">One Shot Audio to Animated Video Generation</a><br><a href="https://arxiv.org/abs/2102.09737"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2102.09532">Clockwork Variational Autoencoders</a><br><a href="https://github.com/vaibhavsaxena11/cwvae"><img src="https://img.shields.io/github/stars/vaibhavsaxena11/cwvae.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2102.09532"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2102.05822">Frame Difference-Based Temporal Loss for Video Stylization</a><br><a href="https://github.com/AtlantixJJ/frame-difference-loss"><img src="https://img.shields.io/github/stars/AtlantixJJ/frame-difference-loss.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2102.05822"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a><br><a href="https://atlantixjj.github.io/FDB/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2102.04680">Tr√§umerAI: Dreaming Music with StyleGAN</a> (NeurIPS Workshop 2020)<br><a href="https://github.com/jdasam/traeumerAI"><img src="https://img.shields.io/github/stars/jdasam/traeumerAI.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2102.04680"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a><br><a href="https://jdasam.github.io/traeumerAI_demo/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2102.00863">Self-Supervised Equivariant Scene Synthesis from Video</a><br><a href="https://arxiv.org/abs/2102.00863"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2101.12050">VAE^2: Preventing Posterior Collapse of Variational Video Predictions in the Wild</a><br><a href="https://arxiv.org/abs/2101.12050"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2101.08779">AI Choreographer: Music Conditioned 3D Dance Generation with AIST++</a><br><a href="https://github.com/google-research/mint"><img src="https://img.shields.io/github/stars/google-research/mint.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2102.04680"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a><br><a href="https://google.github.io/aichoreographer/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2101.07496">Disentangled Recurrent Wasserstein Autoencoder</a><br><a href="https://arxiv.org/abs/2101.07496"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2101.03710">ArrowGAN : Learning to Generate Videos by Learning Arrow of Time</a><br><a href="https://github.com/Kibeom-Hong/ArrowGAN-pytorch"><img src="https://img.shields.io/github/stars/jdasam/traeumerAI.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2101.03710"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2101.03049">InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation</a><br><a href="https://github.com/c1a1o1/InMoDeGAN-project"><img src="https://img.shields.io/github/stars/c1a1o1/InMoDeGAN-project.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2101.03049"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://wyhsirius.github.io/InMoDeGAN/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1903.10836">Personal Privacy Protection via Irrelevant Faces Tracking and Pixelation in Video Live Streaming</a><br><a href="https://arxiv.org/abs/1903.10836"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2105.06468">Dynamic View Synthesis from Dynamic Monocular Video</a> (ICCV 2021)<br><a href="https://github.com/gaochen315/DynamicNeRF"><img src="https://img.shields.io/github/stars/gaochen315/DynamicNeRF.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2105.06468"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://free-view-video.github.io/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2012.12247">Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video</a> (CVPR 2021)<br><a href="https://github.com/facebookresearch/nonrigid_nerf"><img src="https://img.shields.io/github/stars/facebookresearch/nonrigid_nerf.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2012.12247"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://vcai.mpi-inf.mpg.de/projects/nonrigid_nerf/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
</ul>
<h2>2020</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/2011.10727">Stochastic Talking Face Generation Using Latent Distribution Matching</a><br><a href="https://github.com/ry85/Stochastic-Talking-Face-Generation-Using-Latent-Distribution-Matching"><img src="https://img.shields.io/github/stars/ry85/Stochastic-Talking-Face-Generation-Using-Latent-Distribution-Matching.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2011.10727"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2011.03864">Latent Neural Differential Equations for Video Generation</a><br><a href="https://github.com/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation"><img src="https://img.shields.io/github/stars/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2011.03864"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2010.16078">LIFI: Towards Linguistically Informed Frame Interpolation</a><br><a href="https://github.com/midas-research/linguistically-informed-frame-interpolation"><img src="https://img.shields.io/github/stars/midas-research/linguistically-informed-frame-interpolation.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2010.16078"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2007.06705">Unsupervised object-centric video generation and decomposition in 3D</a> (NeurIPS 2020)<br><a href="https://github.com/pmh47/o3v"><img src="https://img.shields.io/github/stars/pmh47/o3v.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2007.06705"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2007.02808">Novel-View Human Action Synthesis</a> (ACCV 2020)<br><a href="https://github.com/mlakhal/gtnet"><img src="https://img.shields.io/github/stars/mlakhal/gtnet.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2007.02808"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://mlakhal.github.io/novel-view_action_synthesis.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2007.01971">Structure-Aware Human-Action Generation</a> (ECCV 2020)<br><a href="https://github.com/PingYu-iris/SA-GCN"><img src="https://img.shields.io/github/stars/PingYu-iris/SA-GCN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2007.01971"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2006.12226">Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single Sample</a> (NeurIPS 2020)<br><a href="https://github.com/shirgur/hp-vae-gan"><img src="https://img.shields.io/github/stars/shirgur/hp-vae-gan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2006.12226"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2006.10704">Latent Video Transformer</a><br><a href="https://github.com/rakhimovv/lvt"><img src="https://img.shields.io/github/stars/rakhimovv/lvt.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2006.10704"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2002.10137">Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose</a><br><a href="https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose"><img src="https://img.shields.io/github/stars/yiranran/Audio-driven-TalkingFace-HeadPose.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2002.10137"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2002.09905">Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and Temporal-Consistency Video Prediction</a> (CVPR 2020)<br><a href="https://github.com/Bei-Jin/STMFANet"><img src="https://img.shields.io/github/stars/Bei-Jin/STMFANet.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2002.09905"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2002.09219">Stochastic Latent Residual Video Prediction</a> (ICML 2020)<br><a href="https://github.com/edouardelasalles/srvp"><img src="https://img.shields.io/github/stars/edouardelasalles/srvp.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2002.09219"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1912.05523">G3AN: Disentangling Appearance and Motion for Video Generation</a> (CVPR 2020)<br><a href="https://github.com/wyhsirius/g3an-project"><img src="https://img.shields.io/github/stars/wyhsirius/g3an-project.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1912.05523"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://wyhsirius.github.io/G3AN/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1906.02634">Scaling Autoregressive Video Models</a> (ICLR 2020)<br><a href="https://github.com/rakhimovv/lvt"><img src="https://img.shields.io/github/stars/rakhimovv/lvt.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1906.02634"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1903.04480">VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation</a> (ICLR 2020)<br><a href="https://github.com/tensorflow/tensor2tensor"><img src="https://img.shields.io/github/stars/tensorflow/tensor2tensor.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1903.04480"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://lucassheng.github.io/publication/pan-video-2019/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
</ul>
<h2>2019</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/2002.09219">Music-oriented Dance Video Synthesis with Pose Perceptual Loss</a><br><a href="https://github.com/xrenaa/Music-Dance-Video-Synthesis"><img src="https://img.shields.io/github/stars/xrenaa/Music-Dance-Video-Synthesis.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/2002.09219"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1910.09139">DwNet: Dense warp-based network for pose-guided human video generation</a><br><a href="https://github.com/ubc-vision/DwNet"><img src="https://img.shields.io/github/stars/ubc-vision/DwNet.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1910.09139"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1907.08845">Order Matters: Shuffling Sequence Generation for Video Prediction</a><br><a href="https://github.com/andrewjywang/SEENet"><img src="https://img.shields.io/github/stars/andrewjywang/SEENet.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1907.08845"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1907.06571">Adversarial Video Generation on Complex Datasets</a><br><a href="https://github.com/Harrypotterrrr/DVD-GAN"><img src="https://img.shields.io/github/stars/Harrypotterrrr/DVD-GAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1907.06571"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1905.10240">From Here to There: Video Inbetweening Using Direct 3D Convolutions</a><br><a href="https://github.com/xih108/Video_Completion"><img src="https://img.shields.io/github/stars/xih108/Video_Completion.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1905.10240"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1904.12165">Improved Conditional VRNNs for Video Prediction</a> (ICCV 2019)<br><a href="https://github.com/facebookresearch/improved_vrnn"><img src="https://img.shields.io/github/stars/facebookresearch/improved_vrnn.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1904.12165"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1706.02631">Sliced Wasserstein Generative Models</a> (CVPR 2019)<br><a href="https://github.com/musikisomorphie/swd"><img src="https://img.shields.io/github/stars/musikisomorphie/swd.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1706.02631"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1904.02912">Point-to-Point Video Generation</a> (ICCV 2019)<br><a href="https://github.com/yccyenchicheng/p2pvg"><img src="https://img.shields.io/github/stars/yccyenchicheng/p2pvg.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1904.02912"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://zswang666.github.io/P2PVG-Project-Page/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1903.06531">High Frame Rate Video Reconstruction based on an Event Camera</a><br><a href="https://github.com/panpanfei/Bringing-a-Blurry-Frame-Alive-at-High-Frame-Rate-with-an-Event-Camera"><img src="https://img.shields.io/github/stars/panpanfei/Bringing-a-Blurry-Frame-Alive-at-High-Frame-Rate-with-an-Event-Camera.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1903.06531"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1903.04480">Video Generation from Single Semantic Label Map</a> (CVPR 2019)<br><a href="https://github.com/junting/seg2vid"><img src="https://img.shields.io/github/stars/junting/seg2vid.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1903.04480"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1901.11384">Learning to navigate image manifolds induced by generative adversarial networks for unsupervised video generation</a><br><a href="https://github.com/belaalb/frameGAN"><img src="https://img.shields.io/github/stars/junting/seg2vid.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1901.11384"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1812.08861">Animating Arbitrary Objects via Deep Motion Transfer</a> (CVPR 2019)<br><a href="https://github.com/AliaksandrSiarohin/monkey-net"><img src="https://img.shields.io/github/stars/AliaksandrSiarohin/monkey-net.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1812.08861"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1812.02784">StoryGAN: A Sequential Conditional GAN for Story Visualization</a> (CVPR 2019)<br><a href="https://github.com/yitong91/StoryGAN"><img src="https://img.shields.io/github/stars/yitong91/StoryGAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1812.02784"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1804.01523">Stochastic Adversarial Video Prediction</a> (ICLR 2019)<br><a href="https://github.com/alexlee-gk/video_prediction"><img src="https://img.shields.io/github/stars/alexlee-gk/video_prediction.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1804.01523"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://video-prediction.github.io/video_prediction/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
</ul>
<h2>2018</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/1812.01037">TwoStreamVAN: Improving Motion Modeling in Video Generation</a><br><a href="https://github.com/sunxm2357/TwoStreamVAN"><img src="https://img.shields.io/github/stars/sunxm2357/TwoStreamVAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1812.01037"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1811.09393">Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation</a><br><a href="https://github.com/thunil/TecoGAN"><img src="https://img.shields.io/github/stars/thunil/TecoGAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1811.09393"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://ge.in.tum.de/wp-content/uploads/2020/05/ClickMe.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1810.02419">Towards High Resolution Video Generation with Progressive Growing of Sliced Wasserstein GANs</a><br><a href="https://github.com/musikisomorphie/swd"><img src="https://img.shields.io/github/stars/musikisomorphie/swd.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1810.02419"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1808.07371">Everybody Dance Now</a> (ICCV 2019)<br><a href="https://github.com/carolineec/EverybodyDanceNow"><img src="https://img.shields.io/github/stars/carolineec/EverybodyDanceNow.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1808.07371"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1807.09951">Learning to Forecast and Refine Residual Motion for Image-to-Video Generation</a> (ECCV 2018)<br><a href="https://github.com/garyzhao/FRGAN"><img src="https://img.shields.io/github/stars/garyzhao/FRGAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1807.09951"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://garyzhao.github.io/archives/eccv18_frgan_poster.pdf"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1804.04786">Talking Face Generation by Conditional Recurrent Adversarial Network</a><br><a href="https://github.com/susanqq/Talking_Face_Generation"><img src="https://img.shields.io/github/stars/susanqq/Talking_Face_Generation.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1804.04786"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1803.08085">Probabilistic Video Generation using Holistic Attribute Control</a> (ECCV 2018)<br><a href="https://github.com/yccyenchicheng/pytorch-VideoVAE"><img src="https://img.shields.io/github/stars/yccyenchicheng/pytorch-VideoVAE.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1803.08085"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1802.07687">Stochastic Video Generation with a Learned Prior</a> (ICML 2018)<br><a href="https://github.com/edenton/svg"><img src="https://img.shields.io/github/stars/edenton/svg.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1802.07687"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://holmdk.github.io/2020/01/22/stochastic_vid.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1802.07687">Stochastic Video Generation with a Learned Prior</a> (ICML 2018)<br><a href="https://github.com/edenton/svg"><img src="https://img.shields.io/github/stars/edenton/svg.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1802.07687"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://holmdk.github.io/2020/01/22/stochastic_vid.html"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1710.11252">Stochastic Variational Video Prediction</a> (ICLR 2018)<br><a href="https://github.com/RoboTurk-Platform/roboturk_real_dataset"><img src="https://img.shields.io/github/stars/RoboTurk-Platform/roboturk_real_dataset.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1710.11252"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1711.09618">Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture</a> (AAAI 2018)<br><a href="https://github.com/mil-tokyo/FTGAN"><img src="https://img.shields.io/github/stars/mil-tokyo/FTGAN.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1711.09618"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1707.04993">MoCoGAN: Decomposing Motion and Content for Video Generation</a> (CVPR 2018)<br><a href="https://github.com/sergeytulyakov/mocogan"><img src="https://img.shields.io/github/stars/sergeytulyakov/mocogan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1707.04993"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
</ul>
<h2>2017</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/1711.11453">Improving Video Generation for Multi-functional Applications</a><br><a href="https://github.com/bernhard2202/improved-video-gan"><img src="https://img.shields.io/github/stars/bernhard2202/improved-video-gan.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1711.11453"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1708.05980">Attentive Semantic Video Generation using Captions</a> (ICCV 2017)<br><a href="https://github.com/Singularity42/cap2vid"><img src="https://img.shields.io/github/stars/Singularity42/cap2vid.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1708.05980"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1611.06624">Temporal Generative Adversarial Nets with Singular Value Clipping</a> (ICCV 2017)<br><a href="https://github.com/universome/stylegan-v"><img src="https://img.shields.io/github/stars/universome/stylegan-v.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1611.06624"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a>
<a href="https://pfnet-research.github.io/tgan/"><img src="https://img.shields.io/badge/Website-9cf" alt="Website"></a></p>
</li>
</ul>
<h2>2016</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/1611.10314">Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive Architectures</a><br><a href="https://github.com/Singularity42/Sync-DRAW"><img src="https://img.shields.io/github/stars/Singularity42/Sync-DRAW.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1611.10314"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
<li><p><a href="https://arxiv.org/pdf/1605.07157">Unsupervised Learning for Physical Interaction through Video Prediction</a><br><a href="https://github.com/Xiaohui9607/physical_interaction_video_prediction_pytorch"><img src="https://img.shields.io/github/stars/Xiaohui9607/physical_interaction_video_prediction_pytorch.svg?style=social&label=Star" alt="Star"></a>
<a href="https://arxiv.org/abs/1605.07157"><img src="https://img.shields.io/badge/arXiv-b31b1b.svg" alt="arXiv"></a></p>
</li>
</ul>
<!--
  // Entry template

+ [TITLE](LINK) (CONFERENCE)  
  [![Star](https://img.shields.io/github/stars/XXX/YYY.svg?style=social&label=Star)](GITHUB)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](ARXIV)
  [![Website](https://img.shields.io/badge/Website-9cf)](WEBSITE)

-->
